<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Something Witty]]></title>
  <link href="http://ElPiloto.github.io/atom.xml" rel="self"/>
  <link href="http://ElPiloto.github.io/"/>
  <updated>2014-11-10T14:56:08-05:00</updated>
  <id>http://ElPiloto.github.io/</id>
  <author>
    <name><![CDATA[Luis R. Piloto]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Sleep EEG: Pre-SfN Push - the sequel]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/11/07/sleep-eeg-pre-sfn-push/"/>
    <updated>2014-11-07T14:46:00-05:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/11/07/sleep-eeg-pre-sfn-push</id>
    <content type="html"><![CDATA[<h2 id="sleep-untransformed-loso-erp-slow-oscillation-bins">Sleep Untransformed LOSO ERP Slow Oscillation Bins</h2>

<p><strong>1st Batch Subjects</strong>   <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/BIN_SO_ERP_LOSO/1st_batch_ERP4_lambda10_FACESCENE.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/BIN_SO_ERP_LOSO/1st_batch_ERP4_lambda10_FACESCENE.png" width="700" height="350" /></a>     <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/BIN_SO_ERP_LOSO/1st_batch_ERP4face_sceneAUC_lambda10PVAL.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/BIN_SO_ERP_LOSO/1st_batch_ERP4face_sceneAUC_lambda10PVAL.png" width="700" height="350" /></a>       </p>

<p><strong>2nd Batch Subjects - all trials</strong>
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/BIN_SO_ERP_LOSO/2nd_batch_ERP4_lambda10_FACESCENE.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/BIN_SO_ERP_LOSO/2nd_batch_ERP4_lambda10_FACESCENE.png" width="700" height="350" /></a>     <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/BIN_SO_ERP_LOSO/2nd_batch_ERP4face_sceneAUC_lambda10PVAL.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/BIN_SO_ERP_LOSO/2nd_batch_ERP4face_sceneAUC_lambda10PVAL.png" width="700" height="350" /></a>       </p>

<p><strong>2nd Batch Subjects - spindleboost trials only</strong>
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/BIN_SO_ERP_LOSO/SPINDLEONLY_2nd_batch_ERP4_lambda10_FACESCENE.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/BIN_SO_ERP_LOSO/SPINDLEONLY_2nd_batch_ERP4_lambda10_FACESCENE.png" width="700" height="350" /></a>     <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/BIN_SO_ERP_LOSO/SPINDLEONLY_2nd_batch_ERP4face_sceneAUC_lambda10PVAL.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/BIN_SO_ERP_LOSO/SPINDLEONLY_2nd_batch_ERP4face_sceneAUC_lambda10PVAL.png" width="700" height="350" /></a>       </p>

<p><strong>2nd Batch Subjects - NO spindleboost trials</strong>
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/BIN_SO_ERP_LOSO/NOSPINDLEBOOST_2nd_batch_ERP4_lambda10_FACESCENE.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/BIN_SO_ERP_LOSO/NOSPINDLEBOOST_2nd_batch_ERP4_lambda10_FACESCENE.png" width="700" height="350" /></a>     <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/BIN_SO_ERP_LOSO/NOSPINDLEBOOST_2nd_batch_ERP4face_sceneAUC_lambda10PVAL.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/BIN_SO_ERP_LOSO/NOSPINDLEBOOST_2nd_batch_ERP4face_sceneAUC_lambda10PVAL.png" width="700" height="350" /></a>       </p>

<h2 id="sleep-untransformed-loso-erp-locked-to-slow-oscillation-trough">Sleep Untransformed LOSO ERP LOCKED to Slow Oscillation Trough</h2>

<p><strong>1st Batch Subjects</strong>   <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_ERP/1st_batch_ERP4_lambda10_FACESCENE.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_ERP/1st_batch_ERP4_lambda10_FACESCENE.png" width="700" height="350" /></a>     <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_ERP/1st_batch_ERP4face_sceneAUC_lambda10PVAL.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_ERP/1st_batch_ERP4face_sceneAUC_lambda10PVAL.png" width="700" height="350" /></a>       </p>

<p><strong>2nd Batch Subjects - all trials</strong>
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_ERP/2nd_batch_ERP4_lambda10_FACESCENE.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_ERP/2nd_batch_ERP4_lambda10_FACESCENE.png" width="700" height="350" /></a>     <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_ERP/2nd_batch_ERP4face_sceneAUC_lambda10PVAL.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_ERP/2nd_batch_ERP4face_sceneAUC_lambda10PVAL.png" width="700" height="350" /></a>       </p>

<p><strong>2nd Batch Subjects - spindleboost trials only</strong>
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_ERP/SPINDLEONLY_2nd_batch_ERP4_lambda10_FACESCENE.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_ERP/SPINDLEONLY_2nd_batch_ERP4_lambda10_FACESCENE.png" width="700" height="350" /></a>     <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_ERP/SPINDLEONLY_2nd_batch_ERP4face_sceneAUC_lambda10PVAL.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_ERP/SPINDLEONLY_2nd_batch_ERP4face_sceneAUC_lambda10PVAL.png" width="700" height="350" /></a>       </p>

<p><strong>2nd Batch Subjects - NO spindleboost trials</strong>
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_ERP/NOSPINDLEBOOST_2nd_batch_ERP4_lambda10_FACESCENE.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_ERP/NOSPINDLEBOOST_2nd_batch_ERP4_lambda10_FACESCENE.png" width="700" height="350" /></a>     <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_ERP/NOSPINDLEBOOST_2nd_batch_ERP4face_sceneAUC_lambda10PVAL.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_ERP/NOSPINDLEBOOST_2nd_batch_ERP4face_sceneAUC_lambda10PVAL.png" width="700" height="350" /></a>       </p>

<h2 id="sleep-untransformed-loso-erp-locked-to-cue">Sleep Untransformed LOSO ERP LOCKED to CUE</h2>

<p><strong>1st Batch Subjects</strong>   <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/CUE_LOCKED_ERP/1st_batch_ERP4_lambda10_FACESCENE.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/CUE_LOCKED_ERP/1st_batch_ERP4_lambda10_FACESCENE.png" width="700" height="350" /></a>     <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/CUE_LOCKED_ERP/1st_batch_ERP4face_sceneAUC_lambda10PVAL.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/CUE_LOCKED_ERP/1st_batch_ERP4face_sceneAUC_lambda10PVAL.png" width="700" height="350" /></a>       </p>

<p><strong>2nd Batch Subjects - all trials</strong>
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/CUE_LOCKED_ERP/2nd_batch_ERP4_lambda10_FACESCENE.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/CUE_LOCKED_ERP/2nd_batch_ERP4_lambda10_FACESCENE.png" width="700" height="350" /></a>     <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/CUE_LOCKED_ERP/2nd_batch_ERP4face_sceneAUC_lambda10PVAL.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/CUE_LOCKED_ERP/2nd_batch_ERP4face_sceneAUC_lambda10PVAL.png" width="700" height="350" /></a>       </p>

<p><strong>2nd Batch Subjects - spindleboost trials only</strong>
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/CUE_LOCKED_ERP/SPINDLEONLY_2nd_batch_ERP4_lambda10_FACESCENE.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/CUE_LOCKED_ERP/SPINDLEONLY_2nd_batch_ERP4_lambda10_FACESCENE.png" width="700" height="350" /></a>     <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/CUE_LOCKED_ERP/SPINDLEONLY_2nd_batch_ERP4face_sceneAUC_lambda10PVAL.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/CUE_LOCKED_ERP/SPINDLEONLY_2nd_batch_ERP4face_sceneAUC_lambda10PVAL.png" width="700" height="350" /></a>       </p>

<p><strong>2nd Batch Subjects - NO spindleboost trials</strong>
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/CUE_LOCKED_ERP/NOSPINDLEBOOST_2nd_batch_ERP4_lambda10_FACESCENE.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/CUE_LOCKED_ERP/NOSPINDLEBOOST_2nd_batch_ERP4_lambda10_FACESCENE.png" width="700" height="350" /></a>     <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/CUE_LOCKED_ERP/NOSPINDLEBOOST_2nd_batch_ERP4face_sceneAUC_lambda10PVAL.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/CUE_LOCKED_ERP/NOSPINDLEBOOST_2nd_batch_ERP4face_sceneAUC_lambda10PVAL.png" width="700" height="350" /></a>       </p>

<h2 id="sleep-untransformed-loso-hilbert-locked-to-slow-oscillation-trough-classify-all-frequencies">Sleep Untransformed LOSO <em>HILBERT</em> LOCKED to Slow Oscillation trough: Classify All Frequencies</h2>

<p><strong>1st Batch Subjects</strong>   <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_HILB/1st_batch_HILB_ALL_FREQS_lambda10_FACESCENE.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_HILB/1st_batch_HILB_ALL_FREQS_lambda10_FACESCENE.png" width="700" height="350" /></a>     <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_HILB/1st_batch_HILB_ALL_FREQSface_sceneAUC_lambda10PVAL.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_HILB/1st_batch_HILB_ALL_FREQSface_sceneAUC_lambda10PVAL.png" width="700" height="350" /></a>       </p>

<p><strong>2nd Batch Subjects - all trials</strong>
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_HILB/2nd_batch_HILB_ALL_FREQS_lambda10_FACESCENE.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_HILB/2nd_batch_HILB_ALL_FREQS_lambda10_FACESCENE.png" width="700" height="350" /></a>     <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_HILB/2nd_batch_HILB_ALL_FREQSface_sceneAUC_lambda10PVAL.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_HILB/2nd_batch_HILB_ALL_FREQSface_sceneAUC_lambda10PVAL.png" width="700" height="350" /></a>       </p>

<p><strong>2nd Batch Subjects - spindleboost trials only</strong>
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_HILB/SPINDLEONLY_2nd_batch_HILB_ALL_FREQS_lambda10_FACESCENE.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_HILB/SPINDLEONLY_2nd_batch_HILB_ALL_FREQS_lambda10_FACESCENE.png" width="700" height="350" /></a>     <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_HILB/SPINDLEONLY_2nd_batch_HILB_ALL_FREQSface_sceneAUC_lambda10PVAL.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_HILB/SPINDLEONLY_2nd_batch_HILB_ALL_FREQSface_sceneAUC_lambda10PVAL.png" width="700" height="350" /></a>       </p>

<p><strong>2nd Batch Subjects - NO spindleboost trials</strong>
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_HILB/NOSPINDLEBOOST_2nd_batch_HILB_ALL_FREQS_lambda10_FACESCENE.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_HILB/NOSPINDLEBOOST_2nd_batch_HILB_ALL_FREQS_lambda10_FACESCENE.png" width="700" height="350" /></a>     <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_HILB/NOSPINDLEBOOST_2nd_batch_HILB_ALL_FREQSface_sceneAUC_lambda10PVAL.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_HILB/NOSPINDLEBOOST_2nd_batch_HILB_ALL_FREQSface_sceneAUC_lambda10PVAL.png" width="700" height="350" /></a>       </p>

<h2 id="sleep-untransformed-loso-hilbert-locked-to-slow-oscillation-trough-classify-frequencies-separately">Sleep Untransformed LOSO <em>HILBERT</em> LOCKED to Slow Oscillation trough: Classify Frequencies Separately</h2>

<p><strong>1st Batch Subjects</strong>   <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_HILB/1st_batch_HILB_lambda10_FACESCENE.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_HILB/1st_batch_HILB_lambda10_FACESCENE.png" width="700" height="350" /></a>     <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_HILB/1st_batch_HILBface_sceneAUC_lambda10PVAL.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_HILB/1st_batch_HILBface_sceneAUC_lambda10PVAL.png" width="700" height="350" /></a>       </p>

<p><strong>2nd Batch Subjects - all trials</strong>
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_HILB/2nd_batch_HILB_lambda10_FACESCENE.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_HILB/2nd_batch_HILB_lambda10_FACESCENE.png" width="700" height="350" /></a>     <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_HILB/2nd_batch_HILBface_sceneAUC_lambda10PVAL.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_HILB/2nd_batch_HILBface_sceneAUC_lambda10PVAL.png" width="700" height="350" /></a>       </p>

<p><strong>2nd Batch Subjects - spindleboost trials only</strong>
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_HILB/SPINDLEONLY_2nd_batch_HILB_lambda10_FACESCENE.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_HILB/SPINDLEONLY_2nd_batch_HILB_lambda10_FACESCENE.png" width="700" height="350" /></a>     <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_HILB/SPINDLEONLY_2nd_batch_HILBface_sceneAUC_lambda10PVAL.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_HILB/SPINDLEONLY_2nd_batch_HILBface_sceneAUC_lambda10PVAL.png" width="700" height="350" /></a>       </p>

<p><strong>2nd Batch Subjects - NO spindleboost trials</strong>
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_HILB/NOSPINDLEBOOST_2nd_batch_HILB_lambda10_FACESCENE.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_HILB/NOSPINDLEBOOST_2nd_batch_HILB_lambda10_FACESCENE.png" width="700" height="350" /></a>     <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_HILB/NOSPINDLEBOOST_2nd_batch_HILBface_sceneAUC_lambda10PVAL.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_LOCKED_HILB/NOSPINDLEBOOST_2nd_batch_HILBface_sceneAUC_lambda10PVAL.png" width="700" height="350" /></a>       </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[State Representation in OFC]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/10/31/PDP-nico/"/>
    <updated>2014-10-31T12:08:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/10/31/PDP-nico</id>
    <content type="html"><![CDATA[<h1 id="where-and-how-is-state-represented">Where and how is state represented?</h1>

<p>Perhaps OFC is a cognitive map of task space? as suggested in paper of similar title by Bob Wilson, … Yael Niv   </p>

<p>Represents map of states in current task   </p>

<h3 id="predicitons-of-current-study">Predicitons of current study:</h3>

<p>activitiy patterns in OFC should allow us to discriminate between hidden states   </p>

<p>similarity of OFC state representations should represent distance in task space    </p>

<h3 id="stimuli">Stimuli</h3>

<p>Young/old faces overlaid on old/modern houses   </p>

<h3 id="task">Task</h3>
<p>Trial 1:  What is the age of the currently attended image?  Need to attend to faces or houses (this is latent state during task, because instructed at start of trial) <br />
Trial 2:  If age of currently attended image changes, next trial will have to switch attention to opposite stimulus type (e.g. from faces to houses)   </p>

<h3 id="what-constitutes-a-state">What constitutes a state?</h3>
<p><strong>Dimensions of State:</strong> <br />
1. category of previous trial  <br />
2. age of prev. trial  <br />
3. category current trial  <br />
4. age of current trial   </p>

<h3 id="behavioral-data">Behavioral Data</h3>
<p>Rewarded if stay below error   </p>

<h3 id="classification-analsysis">Classification Analsysis</h3>
<p>estimate activation pattern associated with each state <br />
- train classifier to discriminate between states <br />
- assess which dimensions classifier can distinguish   </p>

<h4 id="ofc-encodes-most-dimensions-of-state-spate">OFC encodes most dimensions of state spate</h4>
<p>prev category *
prev age (p = 0.06) <br />
current category * <br />
current age  - doesn’t get encoded, for some reason   </p>

<p>DLPFC and Hippo: Less information than OFC   </p>

<h3 id="relations-of-states">Relations of States:</h3>
<p>Basically look at state transition matrix, cluster according to successor representation    </p>

<p>See reliable difference in correlation between states with same successor representation minus correlation between states with different successor representation in OFC    </p>

<p>Size of effect in OFC has <strong>strong</strong> correlation with amount of errors   </p>

<p>Also see: hippocampus, DLPFC, auditory cortex, and visual cortex   </p>

<p>So then what does this mean? Perhaps multiple areas involved in this task?  That seems likely, but what is the role of OFC in this?     </p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ZNN: Cpu-based Conv Nets]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/10/20/znn-cpu-based-conv-nets/"/>
    <updated>2014-10-20T12:08:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/10/20/znn-cpu-based-conv-nets</id>
    <content type="html"><![CDATA[<h1 id="znn">ZNN</h1>

<h3 id="motivation">Motivation</h3>
<ul>
  <li>not that many neural network implementations utilize multi-cpu</li>
  <li>o</li>
</ul>

<h2 id="convoluational-neural-net-training">Convoluational Neural Net Training</h2>

<h4 id="forward-pass">Forward Pass:</h4>
<p><em>Direct:</em>
- each edge is a filter
- convolve f &amp; w
<em>FFT Method:</em>
- FFT of image
- FFT of filter
- Multiple
- inverse FFT</p>

<h4 id="backward-pass">Backward Pass:</h4>
<p><em>Direct:</em>   <br />
calculate G   <br />
convolve g and w   <br />
<em>FFT Method:</em>   <br />
- calculate FFT of G   <br />
- FFT of W (already calculcated during forward pass)   <br />
- inverse FFT     </p>

<h4 id="updating-weights">Updating Weights</h4>
<p><em>Direct:</em>   <br />
- calculate $\frac{dE}{dB}$ and update the biases  <br />
- convolve F(ilter) &amp; G’ and update W   <br />
<em>FFT:</em>   <br />
- calculate $\frac{dE}{dB}$ and update the biases   <br />
- IFFT(FFT(F),FFT(G)), update W     </p>

<h2 id="parallel-models--data-vs-thread-vs-task-model">Parallel Models:  Data vs. Thread vs. Task Model</h2>

<p><strong>Data Model:</strong>
- parallelize a single operation (convolution)  <br />
- example: parallel loops   <br />
<strong>Thread Model:</strong>  <br />
- each thread has its own duties    <br />
- thread communicate with signals/messages   <br />
<strong>Task Model:</strong>  <br />
- taks is an abstract concept - a function <br />
- CPUs pick up tasks that are ready to be executed  <br />
- global queue - can be prioritized  <br />
- tasks can be stolen   </p>

<h2 id="znns-task-model">ZNN’s Task Model</h2>
<p><strong>Prioritized Task Model</strong>
<strong>Scheduling strategy</strong>
- priority based on the numbder of tasks depending on the current task’s completion
<strong>Stealing tasks</strong>     </p>

<h2 id="general-idea-for-parallelizing-forward-pass">General Idea for parallelizing forward pass:</h2>
<p>Let’s look at dependencies (for instance, need to load a training example before you’ll need to FFT it, maybe even before you FFT individual filters, etc) and to grab job’s as needed.  <br />
- Ultimately this leads to the only bottleneck being waiting for the output peceptron    </p>

<h2 id="general-idea-for-parallelizing-backward-pass">General Idea for parallelizing backward pass:</h2>
<ul>
  <li>can update weights as you do backward pass   </li>
</ul>

<h1 id="how-to-znn">How to Znn?</h1>

<p>Three input files:  <br />
1.  Network spec  <br />
2. Data spec  <br />
3. Training Options     </p>

<h4 id="network-spec">Network Spec:</h4>
<p>Can be specified as a graph:  <br />
	- node groups and edge groups  <br />
[C1]    % unique name of node group
size=5   % number of nodes
activation=tanh
act_params=1.712,0.666
% mass pooling operation(only support max pooling, atm)
filter=max
filter_size=2,2,1
filter_stride=2,2,1
% weight/bias initialization
init_type=Uniform
init_params=0.05
bias=0
% learning
eta=0.01
mom=0.9
wc=0
% fft switch - unnecessary
fft=0</p>

<p>Edgegroup spec:
[SOURCENODE_TARGETNODEGROUP] 
size=4,4,1
% more initialization
init_type=Uniform
init_params=0.05</p>

<h4 id="data-spec">Data Spec:</h4>
<p>[INPUT1]
path=
ext=image
size=512,512,30
pptype=standard2D %preprocessing -</p>

<p>[LABEL1]</p>

<h4 id="training-options">Training Options:</h4>
<p>[PATH]
config=./networks/N3.spec
load= (can load previously saved network instances)
data= DATA_SPEC_FILE
save = SAVE_FILE
[OPTIMIZE]
n_threads=64
force_fft=0 % force all edge groups to use FFT
optimize_fft=1
[TRAIN]
train_range=1
test_range=2
outsz=100,100,1
dp_type=volume % data provider types - only allows volume currently   <br />
cost_fn=cross_entropy   <br />
cost_fn_param=0    <br />
data_aug=1 % randomly transform input data (random rotation, random flip - each data provider is responsible for implementing its own data augmentation logic)   <br />
clas_thresh=0.5 % classification threshold   <br />
softmax=1 % apply softmax at output layer  <br />
[UPDATE]
force_eta=0.01
momentum=0.9
wc=0
anneal_factor=0.997
anneal_freq
[MONITOR] % saves current network instance
n_iters=100000
check_freq=10
test_freq=100
test_samples=10</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG: Pre-SFN Push]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/10/13/sleep-eeg-pre-sfn-push/"/>
    <updated>2014-10-13T17:17:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/10/13/sleep-eeg-pre-sfn-push</id>
    <content type="html"><![CDATA[<p><a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/WAKE_ERP_TRANSFORM_SLEEP_ERP_AVG.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/WAKE_ERP_TRANSFORM_SLEEP_ERP_AVG.jpg" width="700" height="350" /></a>    </p>

<h3 id="with-rescaling">WITH rescaling</h3>
<p><a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/RESCALING_WAKE_LOSO_ERP_ZSCORE_175ms.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/RESCALING_WAKE_LOSO_ERP_ZSCORE_175ms.png" width="700" height="350" /></a>  <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/ERP_WAKE_LOSO_RESCALING_SPATIAL_TEMPLATES_175ms.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/ERP_WAKE_LOSO_RESCALING_SPATIAL_TEMPLATES_175ms.jpg" width="700" height="350" /></a>    </p>

<h3 id="without-rescaling">WITHOUT rescaling</h3>
<p><a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/WITHOUT_RESCALING_WAKE_LOSO_ERP_ZSCORE_175ms.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/WITHOUT_RESCALING_WAKE_LOSO_ERP_ZSCORE_175ms.png" width="700" height="350" /></a>  <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/ERP_WAKE_LOSO_NORESCALING_SPATIAL_TEMPLATES_175ms.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/ERP_WAKE_LOSO_NORESCALING_SPATIAL_TEMPLATES_175ms.jpg" width="700" height="350" /></a>    </p>

<h1 id="second-batch-of-subjects">Second Batch Of Subjects:</h1>

<h2 id="things-that-are-the-same">Things that are the same:</h2>

<h4 id="wake-loso-timebin-freq-sweep">Wake LOSO Timebin-Freq Sweep</h4>

<p>These two guys seem pretty similar (the top is the 2nd batch of subjects) </p>

<p><a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/LOSO_WAKE_LOGREG_POST_BUG_2nd_subjects_face_sceneAUC_lambda10.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/LOSO_WAKE_LOGREG_POST_BUG_2nd_subjects_face_sceneAUC_lambda10.png" width="700" height="350" /></a>    </p>

<p><a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/LOSO_WAKE_LOGREG_POST_BUG_face_sceneAUC_lambda10.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/LOSO_WAKE_LOGREG_POST_BUG_face_sceneAUC_lambda10.png" width="700" height="350" /></a>    </p>

<h4 id="wake-loso-erp-classification">Wake LOSO ERP Classification</h4>

<p><strong>2nd Batch Subjects</strong>
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/2nd_batch_LOSO_WAKE_LOGREG_ERP25_face_sceneAUC_lambda10.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/2nd_batch_LOSO_WAKE_LOGREG_ERP25_face_sceneAUC_lambda10.png" width="700" height="350" /></a>     <br />
<strong>1st Batch Subjects</strong>   <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/LOSO_WAKE_LOGREG_ERP25_face_sceneAUC_lambda10.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/LOSO_WAKE_LOGREG_ERP25_face_sceneAUC_lambda10.png" width="700" height="350" /></a>       </p>

<h2 id="things-that-are-not-the-same">Things that are NOT the same:</h2>

<h4 id="sleep-loso-timebin-freq-sweep">Sleep LOSO Timebin-Freq Sweep</h4>
<p><strong>2nd Batch Subjects</strong>
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SLEEP_LOSO_LOGREG_2nd_batch_subjects_face_sceneAUC_lambda10.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SLEEP_LOSO_LOGREG_2nd_batch_subjects_face_sceneAUC_lambda10.png" width="700" height="350" /></a>     <br />
<strong>1st Batch Subjects</strong>   <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SLEEP_LOSO_LOGREG_face_sceneAUC_lambda10.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SLEEP_LOSO_LOGREG_face_sceneAUC_lambda10.png" width="700" height="350" /></a>       </p>

<h4 id="sleep-loso-erp-classification">Sleep LOSO ERP Classification</h4>
<p><strong>2nd Batch Subjects</strong>
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/2nd_batch_LOSO_SLEEP_LOGREG_ERP25_face_sceneAUC_lambda10.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/2nd_batch_LOSO_SLEEP_LOGREG_ERP25_face_sceneAUC_lambda10.png" width="700" height="350" /></a>     <br />
<strong>1st Batch Subjects</strong>   <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/1st_batch_LOSO_SLEEP_LOGREG_ERP25_face_sceneAUC_lambda10.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/1st_batch_LOSO_SLEEP_LOGREG_ERP25_face_sceneAUC_lambda10.png" width="700" height="350" /></a>       </p>

<h2 id="sleep-classify-spindleboost-trials-only">Sleep Classify SpindleBoost Trials Only</h2>
<p><strong>2nd Batch Subjects</strong>
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SPINDLE_TRIALS_SLEEP_LOSO_LOGREG_2nd_batch_subjects_face_sceneAUC_lambda10.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SPINDLE_TRIALS_SLEEP_LOSO_LOGREG_2nd_batch_subjects_face_sceneAUC_lambda10.png" width="700" height="350" /></a>       </p>

<h2 id="classify-erp-4-ms">Classify ERP 4 ms</h2>

<h4 id="wake">WAKE</h4>
<p><strong>2nd Batch Subjects</strong>
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/2nd_batch_LOSO_WAKE_LOGREG_ERP4_2seconds_face_sceneAUC_lambda10.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/2nd_batch_LOSO_WAKE_LOGREG_ERP4_2seconds_face_sceneAUC_lambda10.png" width="700" height="350" /></a>     <br />
<strong>1st Batch Subjects</strong>   <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/1st_batch_LOSO_WAKE_LOGREG_ERP4_2seconds_face_sceneAUC_lambda10.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/1st_batch_LOSO_WAKE_LOGREG_ERP4_2seconds_face_sceneAUC_lambda10.png" width="700" height="350" /></a>       </p>

<h4 id="sleep">SLEEP</h4>
<p><strong>2nd Batch Subjects</strong>
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/2nd_batch_LOSO_SLEEP_LOGREG_ERP4_2seconds_face_sceneAUC_lambda10.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/2nd_batch_LOSO_SLEEP_LOGREG_ERP4_2seconds_face_sceneAUC_lambda10.png" width="700" height="350" /></a>     <br />
<strong>1st Batch Subjects</strong>   <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/1st_batch_LOSO_SLEEP_LOGREG_ERP4_2seconds_face_sceneAUC_lambda10.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/1st_batch_LOSO_SLEEP_LOGREG_ERP4_2seconds_face_sceneAUC_lambda10.png" width="700" height="350" /></a>       </p>

<h2 id="sleep-untransformed-loso-erp-slow-oscillation-locked---fixed">Sleep Untransformed LOSO ERP Slow Oscillation Locked - FIXED</h2>
<p><strong>2nd Batch Subjects - all trials</strong>
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_locked/2nd_batch_subjs_slow_oscillation_ERP4_SOlocked_face_sceneAUC_lambda10.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_locked/2nd_batch_subjs_slow_oscillation_ERP4_SOlocked_face_sceneAUC_lambda10.png" width="700" height="350" /></a>     <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_locked/2nd_batch_subjs_slow_oscillation_ERP4_SOlockedPVAL_face_sceneAUC_lambda10.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_locked/2nd_batch_subjs_slow_oscillation_ERP4_SOlockedPVAL_face_sceneAUC_lambda10.png" width="700" height="350" /></a>     <br />
<strong>2nd Batch Subjects - no spindleboost trials</strong>
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_locked/2nd_batch_subjs_slow_oscillation_ERP4_SOlocked_NOSPINDLEBOOST_face_sceneAUC_lambda10.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_locked/2nd_batch_subjs_slow_oscillation_ERP4_SOlocked_NOSPINDLEBOOST_face_sceneAUC_lambda10.png" width="700" height="350" /></a>     <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_locked/2nd_batch_subjs_slow_oscillation_ERP4_SOlocked_NOSPINDLEBOOSTPVAL_face_sceneAUC_lambda10.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_locked/2nd_batch_subjs_slow_oscillation_ERP4_SOlocked_NOSPINDLEBOOSTPVAL_face_sceneAUC_lambda10.png" width="700" height="350" /></a>     <br />
<strong>1st Batch Subjects</strong>   <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_locked/1st_batch_subjs_slow_oscillation_ERP4_SOlocked_face_sceneAUC_lambda10.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_locked/1st_batch_subjs_slow_oscillation_ERP4_SOlocked_face_sceneAUC_lambda10.png" width="700" height="350" /></a>      <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_locked/1st_batch_subjs_slow_oscillation_ERP4_SOlockedPVAL_face_sceneAUC_lambda10.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_locked/1st_batch_subjs_slow_oscillation_ERP4_SOlockedPVAL_face_sceneAUC_lambda10.png" width="700" height="350" /></a>       </p>

<h2 id="sleep-untransformed-loso-erp-slow-oscillation-bins---fixed">Sleep Untransformed LOSO ERP Slow Oscillation Bins - FIXED</h2>
<p><strong>2nd Batch Subjects - all trials</strong>
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_BINS/2nd_batch_ERP4_all_trials_face_sceneAUC_lambda10.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_BINS/2nd_batch_ERP4_all_trials_face_sceneAUC_lambda10.png" width="700" height="350" /></a>     <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_BINS/2nd_batch_ERP4_all_trialsPVAL_face_sceneAUC_lambda10.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_BINS/2nd_batch_ERP4_all_trialsPVAL_face_sceneAUC_lambda10.png" width="700" height="350" /></a>     <br />
<strong>2nd Batch Subjects - no spindleboost trials</strong>
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_BINS/2nd_batch_slow_oscillation_ERP4_by_bins_NOSPINDLES_face_sceneAUC_lambda10.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_BINS/2nd_batch_slow_oscillation_ERP4_by_bins_NOSPINDLES_face_sceneAUC_lambda10.png" width="700" height="350" /></a>     <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_BINS/2nd_batch_slow_oscillation_ERP4_by_bins_NOSPINDLESPVAL_face_sceneAUC_lambda10.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_BINS/2nd_batch_slow_oscillation_ERP4_by_bins_NOSPINDLESPVAL_face_sceneAUC_lambda10.png" width="700" height="350" /></a>     <br />
<strong>1st Batch Subjects</strong>   <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_BINS/1st_batch_slow_oscillation_ERP4_by_bins_faceAUC_lambda10.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_BINS/1st_batch_slow_oscillation_ERP4_by_bins_faceAUC_lambda10.png" width="700" height="350" /></a>     <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_BINS/1st_batch_slow_oscillation_ERP4_by_binsPVAL_face_sceneAUC_lambda10.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SO_BINS/1st_batch_slow_oscillation_ERP4_by_binsPVAL_face_sceneAUC_lambda10.png" width="700" height="350" /></a>       </p>

<h2 id="sleep-untransformed-loso-erp-spindle-bins">Sleep Untransformed LOSO ERP Spindle Bins</h2>
<p><strong>2nd Batch Subjects - all trials</strong>
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SPINDLE_BINS/2nd_batch_subjs_ERP4_by_SPINDLE_bins_face_sceneAUC_lambda10.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SPINDLE_BINS/2nd_batch_subjs_ERP4_by_SPINDLE_bins_face_sceneAUC_lambda10.png" width="700" height="350" /></a>     <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SPINDLE_BINS/2nd_batch_subjs_ERP4_by_SPINDLE_binsPVAL_face_sceneAUC_lambda10.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SPINDLE_BINS/2nd_batch_subjs_ERP4_by_SPINDLE_binsPVAL_face_sceneAUC_lambda10.png" width="700" height="350" /></a>     <br />
<strong>1st Batch Subjects</strong>   <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SPINDLE_BINS/1st_batch_subjs_ERP4_by_SPINDLE_bins_face_sceneAUC_lambda10.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SPINDLE_BINS/1st_batch_subjs_ERP4_by_SPINDLE_bins_face_sceneAUC_lambda10.png" width="700" height="350" /></a>     <br />
<a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SPINDLE_BINS/1st_batch_subjs_ERP4_by_SPINDLE_binsPVAL_face_sceneAUC_lambda10.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/SPINDLE_BINS/1st_batch_subjs_ERP4_by_SPINDLE_binsPVAL_face_sceneAUC_lambda10.png" width="700" height="350" /></a>       </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Howard Eichenbaum]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/10/09/howard-eichenbaum/"/>
    <updated>2014-10-09T16:34:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/10/09/howard-eichenbaum</id>
    <content type="html"><![CDATA[<h1 id="the-hippocampus-in-space-and-time">The hippocampus in space and time</h1>

<h2 id="lets-start-with-space">Let’s start with space</h2>
<p>How the crap do we reconcile memory and location functions of hippocampus?   </p>

<p>Two relevant pieces of evidence:  <br />
1. hippocampus essential for episodic memory (Vargha-Khadem , Science 1997)    </p>

<ol>
  <li>episodic memories are organized in space and time  (Tulving, Organization of Memory, 1972)    </li>
</ol>

<p><strong>Hypothesis</strong> hippocampus organizes memories in space and time   </p>

<p>context-guided object association:
key point: forces animal to organize memories spatially   </p>

<p>two connected rooms: <br />
- same pair of compounds “A and B” in each room; one compound is positive in one room, negative in the other and vice versa      </p>

<p>hippocampus neurons highly dimensional:   <br />
- have firing selective to item (compound) AND location     </p>

<h2 id="how-are-these-dimensions-organized-within-hippocampus">How are these dimensions organized within hippocampus?</h2>

<p>to do so, upscaled task:
some trials have same rooms, but different compounds “C and D” (with same positive/negative versions in each room)</p>

<p>can look at:
item identity (A v B v C v D) vs. valence (AC v BD) vs. pair (AB v CD)</p>

<p>Also, recording from ensembles of neurons (instead of looking for individual cells that are responsive) and perform multivariate representational similarity analysis    </p>

<p>position: same item, different item, change valence, change pair</p>

<h4 id="punchline">Punchline</h4>
<p>Look at RSA across different manipulations and find that representation is most sensitive in decreasing order to :context, position, valence, item     </p>

<p>Also looked at this hierarchy in orbitofrontal cortex and found that the hierarchy is swapped (see that it cares more about valence which is consistent with what we expect from OFC)   </p>

<h2 id="how-is-memory-space-managed">How is “memory space” managed?</h2>
<p>Must be some activity between PFC and (dorsal) hippocampus:</p>

<p><strong>Idea:</strong> record from hippocampus, inactivate PFC with muscimol, re-record from hippocampus  </p>

<p><strong>Results:</strong> found that cell that fire to item A and context 2, would subsequently fire to both item A and context 2     – although it seems like maybe they’re talking baout recording from mPFC?   </p>

<h3 id="what-is-the-origin-of-contextual-information-to-prefrontal-cortex">What is the origin of contextual information to prefrontal cortex?</h3>

<p>Perhaps this is the flow of communication
1. details –&gt; context generated from dorsal hippocampus –&gt; ventral
2. ventral hippocampus –&gt; PFC carrying contextual information
3. PFC –&gt; dorsal hippocampus to select relevant information</p>

<p>look at J. Neuro 2010 phase lag at theta between areas to see if we can determine flow of information <br />
found -103ms DHipp, -27ms VHipp, and mPFC 64ms    </p>

<h2 id="what-about-time">What about time?</h2>

<p>Do you need a hippocampus for memory for temporal order? (Fortin et al. Nature Neuro 2002)  <br />
Experiment: learn sequence of 5 stimuli, test: which of two stimuli came earlier? <br />
Look at behavior as a function of normal vs. hippocampal lesions
Also tested for recognition and found lesion animals not deficient - seems like hippocampus must play an important role in memory for temporal order    </p>

<h4 id="how-does-the-hippocampus-tell-time">How does the hippocampus tell time</h4>
<p>Kraus et al. Neuron, 2013 - figure 8 task, but confounded with position, so how do we isolate time?   <br />
Keep rat on treadmill, then find “time cells” which fire at a particular time point     </p>

<p>but could potentially confound time elapsed and distance, so vary treadmill rate and found in fact both time cells and distance cells   <br />
but actually found that there’s a distribution of cells which actually respond to some combination of time and distance, but the time-only and distance-only cells are at the tails of that distribution    </p>

<h2 id="general-knowledge">General Knowledge:</h2>
<p>ventral hippocampus: bigger place fields, generally take up whole context; develop a bit slower than dorsal hippocampus  <br />
dorsal hippocampus: more specific, smaller place fields; develop place field selectivity perfectly correlated with learning    <br />
monosynapse from ventral hippocampus to PFC   </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG: Sleep Transform Details And McDuff Trouble]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/10/08/sleep-eeg-sleep-transform-details-and-mcduff-trouble/"/>
    <updated>2014-10-08T17:28:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/10/08/sleep-eeg-sleep-transform-details-and-mcduff-trouble</id>
    <content type="html"><![CDATA[<h2 id="importance-maps">Importance Maps:</h2>

<p>It turns out that we have been re-scaling (the eeg_ana_toolbox traintest function) features when performing classification.  This is arguably an okay thing to do in general, BUT DEFINITELY NOT OKAY to do when calculating McDuff importance maps which rely on looking at the average pattern activation signs and the signs of the weights because the weights were generated for a rescaled (always between 0 and 1) pattern, so that it doesn’t make sense to look at the sign agreement between the two for calculating the McDuff importance maps.  </p>

<h2 id="generating-sleep-transformed-plots">Generating Sleep Transformed Plots:</h2>
<p>Since it has been working, for the purposes of generating a wake template, we’re going to use the RFE results generated by performing feature selection on the raw feature weights (instead of McDuff importance)</p>

<h4 id="additionally-there-was-some-concern-that-we-might-be-z-scoring-across-electrodes-after-each-rfe-iteration-but-really-thats-not-what-we-think-we-should-be-doing-and-luckily-not-what-were-doing-in-the-code">Additionally, there was some concern that we might be z-scoring across electrodes after each RFE iteration, but really that’s not what we think we should be doing and <em>luckily</em> <strong>not</strong> what we’re doing in the code!</h4>

<h4 id="we-have-a-preference-for-more-textured-templates-and-using-the-correlation-instead-of-the-dot-product-because-the-correlation-is-scale-invariant">We have a preference for more “textured” templates and using the correlation instead of the dot product because the correlation is scale-invariant</h4>

<h4 id="stucture">Stucture:</h4>
<p>We want to generate a “flipbook” for every pair of wake time-freq bin we choose (roughly 6) and each corresponding sleep frequency.  Given 5 frequency bands and 6 wake templates, we’ll end up with roughly 30 flipbooks.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG: LOSO (post-bug) and ERP]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/10/06/sleep-eeg-loso-post-bug-and-erp/"/>
    <updated>2014-10-06T10:29:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/10/06/sleep-eeg-loso-post-bug-and-erp</id>
    <content type="html"><![CDATA[<h3 id="as-described-in-a-hrefblog20140925sleep-eeg-replicating-best-p-val-resultsthis-posta-there-was-a-bug-that-messed-things-up-for-the-results-posted-a-hrefblog20140904sleep-eeg-finding-optimal-wake-pattern-for-sleep-transformherea">As described in <a href="http://ElPiloto.github.io/blog/2014/09/25/sleep-eeg-replicating-best-p-val-results/">this post</a>, there was a bug that messed things up for the results posted <a href="http://ElPiloto.github.io/blog/2014/09/04/sleep-eeg-finding-optimal-wake-pattern-for-sleep-transform/">here</a></h3>

<p>In general, we were producing a non-random, though erroneous, transformation to our data so now that that bug is fixed we should be getting pretty similar results just along different times and frequencies.   </p>

<h4 id="nothing-has-changed-for-the-optimal-lambda-choice">Nothing has changed for the optimal lambda choice:</h4>
<p>Still seems pretty close between the two, but let’s stick with <a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/LOSO_WAKE_LOGREG_POST_BUG_face_sceneAUC_lambda10.png">$\lambda = 10$</a> over <a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/LOSO_WAKE_LOGREG_POST_BUG_face_sceneAUC_lambda50.png">$\lambda = 50$
</a></p>

<h4 id="looking-for-best-timebin-and-frequency">Looking for best timebin and frequency:</h4>
<p>previously we liked <strong>300 ms</strong> time bin and were slightly frequency agnostic based on <a href="http://ElPiloto.github.io/images/research/LOSO_sleep_transform/LOSO_WAKE_LOGREG_SWEEP_PVAL_face_sceneAUC_lambda10.png">this</a>, but now we get totally different looking results:</p>

<p><a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/LOSO_WAKE_LOGREG_POST_BUGPVAL_face_sceneAUC_lambda10.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/LOSO_WAKE_LOGREG_POST_BUGPVAL_face_sceneAUC_lambda10.png" width="700" height="350" /></a></p>

<p><strong>Thoughts:</strong> Cool things: we’re getting something around when you’d expect to see the N170 for faces!  We were getting hints of this when we were doing individual subject classification, but it seems that this is the only thing from the <a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/wake_hilb_mean_across_subjects.png">subject-specific classification</a> that comes through in the LOSO analysis (although we also get other strongly classifiable time bins in the LOSO analsis, it’s just that those didn’t pop out in the subject-specific classification results).  <strong>SO</strong>, I say let’s generate transformed sleep data plotaccording to four or five different patterns: 
	- something in the 4 Hz, 125-275 ms timebin range
	- something in the 4 Hz, 450-600 ms timebin range
	- something in the 4 Hz, 725-925 ms timebin range
	- something in the 8 Hz, 100-175 ms timebin range 
	- something in the 8 Hz, 275-325 ms timebin range   </p>

<p>This will be A LOT of plots to look at, but if we see something we can always check on James’ second batch of spindle-cued subjects.</p>

<h4 id="lets-look-at-the-recursive-feature-analysis-results-for-lambda--10">Let’s look at the recursive feature analysis results for $\lambda = 10$</h4>
<p>And see what’s the best we get across the different time bins for the 4 Hz and 8 Hz frequencies at some points that we cared about: <del>- I’m working on presenting these in a better fashion but for now these are the plots that were most easily generated:</del>   </p>

<p><a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/LOSO_RFE_BEST.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/LOSO_RFE_BEST.png" width="700" height="350" /></a>   </p>

<h4 id="additionally-lets-look-at-wake-loso-classification-using-just-erps">Additionally, let’s look at wake LOSO classification using just ERPs:</h4>
<p>Well, this is awkward: our best classification results are for using the ERPs: 68% accuracy.</p>

<p><a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/LOSO_WAKE_LOGREG_ERP25_face_sceneAUC_lambda10.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/LOSO_WAKE_LOGREG_ERP25_face_sceneAUC_lambda10.png" width="700" height="350" /></a></p>

<h4 id="lastly-lets-not-forget-that-we-wanted-to-look-at-16-hz-175-ms-in-the-sleep-data-because-we-did-well-with-it-on-an-individual-subject-level-as-seen-here">Lastly, let’s not forget that we wanted to look at 16 Hz, 175 ms in the sleep data because we did well with it on an individual subject level as seen here:</h4>

<p><a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/sleep_hilb_mean_across_subjects_lambda1.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/sleep_hilb_mean_across_subjects_lambda1.png" width="700" height="350" /></a></p>

<p><strong>TODO:</strong></p>

<ul>
  <li>when picking number of electrodes to keep, err on the side of too many so that we get a varied distribution of importance weights (so that we can do correlation)</li>
  <li>generate sleep similarity plots BY wednesday!</li>
  <li>let’s look at RFE and ERP results, too and generate a template based on that as well!</li>
  <li>let’s do whatever we can to help James preprocess the validation subjects</li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG: Replicating best p-val results]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/09/25/sleep-eeg-replicating-best-p-val-results/"/>
    <updated>2014-09-25T14:51:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/09/25/sleep-eeg-replicating-best-p-val-results</id>
    <content type="html"><![CDATA[<h3 id="we-are-trying-to-replicate-these-results-below">We are trying to replicate these results below:</h3>

<h4 id="so-far-ive-tried">So far I’ve tried:</h4>
<ul>
  <li>Running the same exact analysis (driver_LOSO_replicate_PVAL.m)   </li>
  <li>Running the same exact analysis but only classifying between faces and scenes via:
<code>preprocessing.type_specifics_args.classes = [1 2];</code>    <br />
    <ul>
      <li>this did not help   </li>
    </ul>
  </li>
  <li>Try loading up results from Sept 09, and regenerating plots - will indicate whether or not something changed in plots or results. <br />
    <ul>
      <li>Doing this, I’ve found that the results of interest are specifically in this file:  <br />
<code> 09-Sep-2014_14_04_44LOSO_WAKE_logreg_sweep.mat </code>   </li>
      <li>I’ve found that it is in fact for $ \lambda = 10 $ (I was worried I might have accidentally documented the wrong value of $\lambda$)       </li>
      <li>We were classifying against all 5 classes (validating our findings above that classifying only faces vs scenes did not reproduce the results)   </li>
    </ul>
  </li>
  <li>Try duplicating exact time range used originally, perhaps there is some bug in my code <br />
    <ul>
      <li><strong>BAD NEWS:</strong> this reproduced the results, meaning there has to be some BUG in my code where I reshape the data, append subject data together, OR iterate through individual timebins and frequencies. This is why it would be nice to have the time to create some simulated data.</li>
    </ul>
  </li>
</ul>

<h3 id="fixing-the-damn-bug">FIXING THE DAMN BUG</h3>
<p>Relevant files:  </p>

<p>So far, I’ve ensured that when I reshape and permute the subject’s data after loading it (<code>get_preprocessed_subj_data.m, Line: 128 </code>), that I have not messed up the ordering of the data.</p>

<p>Next: when I filter timebins out, do I mess that process up?  This also checks out(<code>get_preprocessed_subj_data.m, Line: 60 </code>)   </p>

<p>Next: what about when I call <code>iterate_over_data</code>?  The data was misordered here, so somewhere between <code>get_preprocessed_subj_data</code> and <code>iterate_over_data</code> we messed things up.</p>

<p>Next: let’s check that the anonymous function that is used for reshaping the data (after it’s been flattened) is correct, this gets used in a few places.  YEP, this is the problem: someone please kindly shoot me in the face.   </p>

<h3 id="bug-the-sequel-discrepancy-between-the-recursive-feature-elimination-results-and-logistic-regression-results">BUG, The Sequel: DISCREPANCY BETWEEN THE RECURSIVE FEATURE ELIMINATION RESULTS AND LOGISTIC REGRESSION RESULTS</h3>
<p>I’ve verified that there is no discrepancy between the accumulated values and the plotted values, so it does not seem to be the case that the plotting function is messing things up.   </p>

<p>Recursive feature elimination (RFE) analysis has a setting for the minimum number of features i.e. remove features until we hit some minimum number of features.  In general, this is set to 1, so that we can see the effect of removing 0 to all but one features (electrodes).  I have found that if I set that value to 60 (only removing 4 features), then the RFE results align with the logistic regression results, however, if I set that to 1 or even just 50, the results stop lining up.  Very, very perplexing.    </p>

<p>Next, let’s look at the features used for the first and second iteration with different settings (60 and 50) of the minimum number of features.  If they differ in any way, then the code for removing features must be incorrect and we can delve into that.<br />
   - So for the second iteration, the fourth frequency, the RFE results have different features they’ve removed.  WTF, mate?   A reasonable next step would be to look at the values passed into the <code>remove_unused_features</code> for both parameter settings and see if they are the same.</p>

<p>Looking at the classifier weights for the first iteration of the fourth frequency for both parameter settings, we see that they have different values, so naturally removing features would produce different removed features for the second iteration.  Additionally, this implies there is something different being fed into the classifier for the first iteration that is producing different feature weights.</p>

<p><strong>HERE IT IS:</strong>  Currently, we’ve been doing an RFE iteration (going from 64 features down to minimum num_features) for each frequency bin (2 Hz all the way up to 32 Hz).  To maintain the correct level of regularization, we take the max number of features (64) and look at the ratio of regularization (usually 10) to the number of features and keep this proportional as we remove more and more features.  The bug was that between RFE iterations (i.e. starting anew at 4 Hz, after doing 2 Hz), the regularization ratio was calculating using the previous iteration’s last number of features (not the max number of features) - so as we moved from 2 Hz to 32 Hz, our results got more and more skewed from the regular logistic regression results because we were using much smaller regularization.  This is fixed now!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG: James Visit Recap and Brainstorm]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/09/18/sleep-eeg-james-visit-recap-and-brainstorm/"/>
    <updated>2014-09-18T14:49:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/09/18/sleep-eeg-james-visit-recap-and-brainstorm</id>
    <content type="html"><![CDATA[<h4 id="regarding-this-our-best-result-during-sleep-is-058---but-were-messed-up-by-multiple-comparisons">Regarding this, our best result, during sleep, is 0.58 - but we’re messed up by multiple comparisons</h4>
<p>TODO: we can use our a priori best itmebin and frequency to analyze James’ new dataset (timebin 175ms, freq 16)</p>

<p>Cephalapod L1 vs L2</p>

<p>http://elpiloto.github.io/images/research/sleep_eeg_hilbert/sleep_hilb_mean_across_subjects_lambda1.png</p>

<ul>
  <li>
    <p>negative peak of a slow-wave, phase of 0.6 - 1.2 Hz</p>
  </li>
  <li>
    <p>how similar are frequency band patterns to each other?</p>
  </li>
</ul>

<p>N170 is really sharp, maybe at 8 Hz and 225ms, large over Cz sometimes (doesn’t make a lot of sense anatomically - you’d expect it to be over occipital)</p>

<p>once we get the template, we want to inspect them, although it’s hard to know what to expect</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Install Lightspeed Toolbox For Matlab on Os X Maverick]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/09/17/install-lightspeed-toolbox-for-matlab-on-os-x-maverick/"/>
    <updated>2014-09-17T10:35:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/09/17/install-lightspeed-toolbox-for-matlab-on-os-x-maverick</id>
    <content type="html"><![CDATA[<p>Thomas Minka’s lightspeed toolbox</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep Transform ALL the Subjects]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/09/10/sleep-transform-all-the-subjects/"/>
    <updated>2014-09-10T11:35:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/09/10/sleep-transform-all-the-subjects</id>
    <content type="html"><![CDATA[<h2 id="transformation-outline">Transformation Outline</h2>

<p>In this <a href="http://ElPiloto.github.io/blog/2014/09/04/sleep-eeg-finding-optimal-wake-pattern-for-sleep-transform/">post</a> we decided we’re going to try to generate a wake template for all of the different frequency bands at the <strong>300ms time bin</strong>.  Recall that those results were for L2-regularization of LOSO data ( with $\lambda = 10$ giving us the best results )  Below we outline how we plan to transform the sleep data.</p>

<pre>
# for now, we're going to do a sleep transformation classification for each frequency
# for later: concatenate transformed data at each frequency into a single dataset
foreach frequency band wake_F:
	foreach electrode E:
		if pval( mean_power_faces[E] - mean_power_scenes[E] ) &lt; 0.05
			informative_electrodes.add_electrode(E)
	
	wake_data = append_subject_data(freq = wake_F, timebin = 300ms, electrodes = informative_electrodes,
				classes_to_use = [faces scenes objects scrambled_faces scrambled_scenes])
	
	wake_logreg = train_logistic_regression_classifier(wake_data, lambda = 10)

	# we could also take the mean pattern as our wake template
	wake_template = get_importance_map(wake_logreg, class = faces)
	
	# Option 1: old-school sleep transform analysis
	foreach subject S:
		foreach pattern P in sleep_data[S]:

			filtered_pattern = remove_noninformative_electrodes(P, informative_electrodes)

			foreach timebin T:
				foreach frequency sleep_F:

					transformed_sleep_data[S,P,T,sleep_F] = dot_product( wake_template, filtered_pattern[T,sleep_F] )
				
				# note below we are only grabbing data for THIS subject and THIS timebin - meaning a classifier
				# score for each timebin
				sleep_logreg[T] = train_logistic_regression_classifier(transformed_sleep_data[S,all_patterns,T, all_frequencies],
											lambda = ?, cross_validation = leave-one-trial-out)
	
	# Option 2: LOSO sleep transformed
	LOSO_sleep_logreg = train_logistic_regression_classifier(transformed_sleep_data, lambda = ?, cross_validation = per_subject)

	# Option 3: Use all sleep times as a single pattern (nice b/c get single classification score per trial)
</pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG Finding Optimal Wake Pattern For Sleep Transform]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/09/04/sleep-eeg-finding-optimal-wake-pattern-for-sleep-transform/"/>
    <updated>2014-09-04T10:06:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/09/04/sleep-eeg-finding-optimal-wake-pattern-for-sleep-transform</id>
    <content type="html"><![CDATA[<h2 id="wake-loso-log-reg-150ms---550ms-time-freq-sweep-on-hilbert-transformed-data">WAKE LOSO LOG REG 150ms - 550ms Time Freq. Sweep On Hilbert Transformed data</h2>

<p>Find those results below.  It seems that across the board, looking at diffAUC, we get that the 300ms timebin is the best.  However, one of the goals of this analysis was to see if there was any variability in when faces are classifiable compared to when scenes are classifiable.  Towards this end, we plot the AUC on a per class basis.  That is, we look at the AUC calculating the face classifier output for faces as positive instances, and the face classifier output for scenes as negative instances.  Similarly, we do this for scenes.  When we look at this across all subjects, we find that the 300ms timebin strong diffAUC is actually driven by strong faceAUC(~0.58) for that timebin and not at all by the sceneAUC(~0.43).  Thus, if we do decide to proceed with sleep transforming, we should only face transform at 300ms.   When we look at the faceAUC per subject, we find that 300ms at the 30Hz band is the good for all but two subjects (08 and 22).  This is encouraging!  Feel encouraged.  Did it work? Are you encouraged?  Fine.  Nevermind.  Whatever.  </p>

<p>I ran this analysis with a shortened time window, I’m re-running the analysis with the full time window to make sure that scenes aren’t doing well during the later periods.  Additionally, we have always been z-scoring across electrodes, I’m not doing that for this new analysis just to make sure our z-scoring decision (supported via our first pilot) are still valid.  I can look at the difference between the new results and the results below for the timebins they overlap to see the effect of z-scoring.   </p>

<p><strong>TLDR;</strong><br />
- diffAUC points to timebin 300ms as best time across all subjects with average diffAUC = 0.58 <br />
- looking into faceAUC vs. sceneAUC, it’s clear that the diffAUC is entirely driven by faceAUC (i.e. scenes aren’t above chance at 300ms), we should sleep transform sleep data using the 300ms timebin importance maps.   </p>

<p><strong>QUESTIONS TO ANSWER</strong> <br />
1. does one-sample, one-tailed t-test reveal better bins? <br />
2. are there better times? <br />
3. is there a difference between z-scoring and not z-scoring across channels? <br />
4. do we get better results for lower lambdas?   </p>

<p><strong>ANSWERS TO QUESTIONS</strong> <br />
<strong>ONE.</strong> We still get the best time bin to be 300 ms <a href="http://ElPiloto.github.io/images/research/LOSO_sleep_transform/LOSO_WAKE_LOGREG_SWEEP_PVAL_face_sceneAUC_lambda100.png">if you look at the p-values</a> instead of the <a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__face_sceneAUC_lambda100.png">mean across subjects</a>  <br /></p>

<p><strong>TWO.</strong> When we do the same analysis looking at time bins 550ms - 1000ms, we don’t get anything better.  There’s a random blip around 950ms for scenes    <a href="http://ElPiloto.github.io/images/research/LOSO_sleep_transform/LOSO_WAKE_LOGREG_SWEEP_Q2_ZSCOREPVAL_face_sceneAUC_lambda50.png">here</a>, but it’s not any better than what we saw for the 150ms - 550ms case.     <br /></p>

<p><strong>THREE.</strong> YES IT DOES.  Compare these two plots WITHOUT z-scoring to their respective plots under question 4.  <br /></p>
<center>
<a href="http://ElPiloto.github.io/images/research/LOSO_sleep_transform/LOSO_WAKE_LOGREG_SWEEP_Q3_ZSCOREPVAL_face_sceneAUC_lambda50.png">$\lambda = 50$</a><br />
<a href="http://ElPiloto.github.io/images/research/LOSO_sleep_transform/LOSO_WAKE_LOGREG_SWEEP_Q3_ZSCOREPVAL_face_sceneAUC_lambda100.png">$\lambda = 100$</a><br />
</center>

<p><strong>FOUR.</strong>  We get better results with lambda = 10 for the p-val AUC plots, but not by too much if we just look at the mean AUC across subjects.  <strong>Let’s go with 10 for a wake transform</strong>    </p>
<center>
<a href="http://ElPiloto.github.io/images/research/LOSO_sleep_transform/LOSO_WAKE_LOGREG_SWEEP_PVAL_face_sceneAUC_lambda1.png">$ \lambda = 1$</a><br />   
<a href="http://ElPiloto.github.io/images/research/LOSO_sleep_transform/LOSO_WAKE_LOGREG_SWEEP_PVAL_face_sceneAUC_lambda10.png">$ \lambda = 10$</a><br />   
<a href="http://ElPiloto.github.io/images/research/LOSO_sleep_transform/LOSO_WAKE_LOGREG_SWEEP_PVAL_face_sceneAUC_lambda50.png">$ \lambda = 50$</a><br />   
<a href="http://ElPiloto.github.io/images/research/LOSO_sleep_transform/LOSO_WAKE_LOGREG_SWEEP_PVAL_face_sceneAUC_lambda100.png">$ \lambda = 100$</a><br />   
</center>

<p><strong>Ken Reply</strong></p>

<p>i like the idea of finding the best time/freq for faces and then using
that for a sleep transform. i agree that 300ms / 30hz looks like the
best candidate for the “face champion”.    </p>

<p>before you do the sleep transform, it would be great if you could run
the same analysis that you just ran, but for lower lambda vals (say 1,
10, 50). it looks like things were definitely getting worse as you
increased lambda in the analyses that you just ran, so maybe the best
lambda vals for this analysis are &lt; 100.    </p>

<p>if 300ms / 30hz still looks good with other lambda vals, then let’s
sleep transform using that.    </p>

<p>if we go ahead with 300ms / 30hz for the transformation, then i
recommend first filtering out uninformative electrodes, and then
re-running the classifier on the leftover electrodes and either 1)
using the importance map values for faces as the template, or 2) using
the mean pattern values (for faces) as the template. i also recommend
sticking with our original plan of grabbing the spatial pattern from
the BEST frequency band (e.g., 30hz) at the best time point (e.g.,
300ms) as opposed to grabbing ALL of the frequency-specific spatial
patterns from the best time point. we can use this pattern to
transform all of the frequences in the sleep data.     </p>

<p>also, after you do the sleep transform, i am interested in looking at
timecourses for reactivation as opposed to jumping right into crossval
accuracy.    </p>

<h2 id="lambda--100">Lambda = 100</h2>

<h4 id="diffauc-face-minus-scene">diffAUC (face minus scene)</h4>

<p><a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__diffAUC_lambda100.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__diffAUC_lambda100.png" width="700" height="350" /></a>
<a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__face_sceneAUC_lambda100.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__face_sceneAUC_lambda100.png" width="700" height="350" /></a>
<a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__faceAUC_per_subj_lambda100.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__faceAUC_per_subj_lambda100.png" width="700" height="350" /></a>
<a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__sceneAUC_per_subj_lambda100.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__sceneAUC_per_subj_lambda100.png" width="700" height="350" /></a></p>

<h2 id="lambda--500">Lambda = 500</h2>

<h4 id="diffauc-face-minus-scene-1">diffAUC (face minus scene)</h4>

<p><a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__diffAUC_lambda500.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__diffAUC_lambda500.png" width="700" height="350" /></a>
<a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__face_sceneAUC_lambda500.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__face_sceneAUC_lambda500.png" width="700" height="350" /></a>
<a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__faceAUC_per_subj_lambda500.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__faceAUC_per_subj_lambda500.png" width="700" height="350" /></a>
<a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__sceneAUC_per_subj_lambda500.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__sceneAUC_per_subj_lambda500.png" width="700" height="350" /></a></p>

<h2 id="lambda--1000">Lambda = 1000</h2>

<h4 id="diffauc-face-minus-scene-2">diffAUC (face minus scene)</h4>

<p><a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__diffAUC_lambda1000.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__diffAUC_lambda1000.png" width="700" height="350" /></a>
<a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__face_sceneAUC_lambda1000.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__face_sceneAUC_lambda1000.png" width="700" height="350" /></a>
<a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__faceAUC_per_subj_lambda1000.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__faceAUC_per_subj_lambda1000.png" width="700" height="350" /></a>
<a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__sceneAUC_per_subj_lambda1000.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__sceneAUC_per_subj_lambda1000.png" width="700" height="350" /></a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG: Past TODO items and next directions]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/09/02/sleep-eeg-past-to-do-items-and-next-directions/"/>
    <updated>2014-09-02T14:15:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/09/02/sleep-eeg-past-to-do-items-and-next-directions</id>
    <content type="html"><![CDATA[<p>Before determining where we should go next now that we have a little bit of breathing room, I did a big review of the notes (and also some emails) I left myself from the last two months of updates in order to make sure we didn’t leave any ideas behind.  <strong>Bold asterisks indicate the items I intend to do next</strong> either because I think they’re important, are easy to do, or are a requisite for something important.   </p>

<h4 id="below-is-a-consolidated-list-of-to-do-items-random-things-weve-bookmarked-and-some-of-my-own-ideas">Below is a consolidated list of to-do items, random things we’ve bookmarked, and some of my own ideas</h4>

<ol>
  <li>
    <p>L1-regularization (instead of univariate feature selection + feature regularization via L2)<strong>******(important)</strong>    </p>
  </li>
  <li>
    <p>why was subject 9’s classification accuracy so low? (didn’t do poorly with other classification methods, so maybe it’s not so important to look at)   </p>
  </li>
  <li>coming up with a good wake template <strong>******(important)</strong> 
    <ul>
      <li>look at mcduff importance maps for wake LOSO logistic regression: does something pop out as the timebin/frequency to use for wake transformation?      </li>
    </ul>
  </li>
  <li>
    <p>different AUC metrics (diffAUC pooled over trials, meanAUC, etc)   </p>
  </li>
  <li>
    <p>boosting untransformed wake LOSO did poorly (about chance ) - was this a bug/not enough regularization or was it due to using the relatively quicker, unfamiliar totalboost algorithm? we can test this by running logitboost or gentleboost (which worked previously) now that we don’t have the pressure of the kenP deadline <strong>******(easy to do)</strong>   </p>
  </li>
  <li>
    <p>when looking at classification scores that are significantly better than chance, let’s also look at scores below chance   </p>
  </li>
  <li>
    <p>do we want to look out longer than 1 second after a tone is played? if first 500ms of sleep trial data happens to fall on down phase, we wouldn’t expect classification to work for that trial   </p>
  </li>
  <li>sleep transform: <br />
    <ul>
      <li>LOSO version   </li>
      <li>use face AND scene transformed data   </li>
      <li>exclude electrodes individually after finding good timebin/freq for wake   </li>
      <li>explicitly disentangle effects of dot-product vs. correlation on sleep transform   </li>
    </ul>
  </li>
  <li>
    <p>separate face vs. scene classifiers (face vs. objects/scrambled faces/scrambled scenes AND scene vs. objects/scrambled faces/ scrambled scenes): are we doing better at one of these than the other?  do they have different optimal classification times? (ties into previous thoughts about AUC: looking at face and scene classifier difference, diffAUC, vs. meanAUC)   <strong>******(important)</strong>   </p>
  </li>
  <li>
    <p>classification of data using ERPs   </p>
  </li>
  <li>visualize data: <br />
    <ul>
      <li>interface with EEGLAB topoplots   </li>
      <li>compare face/scene templates per subject to those generated on an individual-subject level   </li>
      <li>similarity of sleep data across time bins to wake pattern via simple correlation: are these different for the different classes?   <br />
        <ul>
          <li>we can look at the average similarity to a face or scene pattern against the average similarity for all other classes across sleep time  bins for each frequency<strong>******(important)</strong>     </li>
        </ul>
      </li>
      <li>autocorrelation of sleep data for various electrodes, could we use this as an indicator of down phases for the purpose of excluding time bins or trials?  is there another proxy for determining a down phase?      </li>
    </ul>
  </li>
  <li>
    <p>LOSO using some form of non-linear classification? previously we didn’t think this was a viable option given the ratio of training examples to features, but the combination of Hilbert-transformed data and LOSO makes me think this is now possible   </p>
  </li>
  <li>
    <p>exclude forgotten items from the sleep analysis, does this improve our sleep cross-validation?   </p>
  </li>
  <li>
    <p>test wake reactivation during learning of associated locations (this is different than the current wake data we use to train on which we take from the wake localizer)   </p>
  </li>
  <li>
    <p>generate simulated data to test idea that we could even find a wake pattern embedded randomly in a sleep data timebin-freq bin. also, would be good for general code validation (importance map transformations, mvpa code, etc)   </p>
  </li>
  <li>
    <p>look at classifier output for scrambled conditions and use to sleep transform: do these results look different than what we get when we transform via face and scene conditions?   </p>
  </li>
  <li>
    <p>“feature shuffle noise” wake classification analysis: do we get better generalization when we extend training data with patterns that are hybrids of two patterns from the same class? analogous to dropout techniques for neural networks    </p>
  </li>
  <li>
    <p>What if we transform the sleep data according to the timebin-freq pair that performs the best for each subject?  This is obviously less parsimonious than we’d like, but a signature of reactivation is a signature of reactivation.    </p>
  </li>
  <li>For results with <em>really</em> low wake classification accuracy, what do we get if we sleep transform by flipping the labels on the importance maps generated?    </li>
</ol>

<h4 id="below-is-a-working-list-identical-to-the-above-list-showing-whats-been-completed-so-far">Below is a working list identical to the above list showing what’s been completed so far.</h4>

<ol>
 <li> <del>L1-regularization (instead of univariate feature selection + feature regularization via L2)</del> Results: <br /> <br />

	NOTE: For lambda too high, the classifier simply assigns 0 to all the weights which makes sense given the number of training examples i.e. the $\lambda\ = 100$ case was just to test that this actually worked.  Totally intentional, I swear.   

<a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/LOSO_wake_L1.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/LOSO_wake_L1.jpg" width="700" height="350" /></a>

<a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/LOSO_wake_L1_per_subject.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/LOSO_wake_L1_per_subject.jpg" width="700" height="350" /></a>

   
 <li> <del>why was subject 9&#8217;s classification accuracy so low? (didn&#8217;t do poorly with other classification methods, so maybe it&#8217;s not so important to look at)</del> Look at <a href="http://localhost:4000/images/research/sleep_eeg_hilbert/LOSO_wake_L1_per_subject.jpg">this L1-regularized plot</a>, clearly it was just a pathological case for feature selection plus L2-regularization.  Officially, NOT gunna worry about this.  <br />  <br />
    
<li> coming up with a good wake template __******(important)__<br /> 
    - look at mcduff importance maps for wake LOSO logistic regression: does something pop out as the timebin/frequency to use for wake transformation?     <br />  <br />
   
<li> different AUC metrics (diffAUC pooled over trials, meanAUC, etc)  <br />  <br />
   
<li> <del>boosting untransformed wake LOSO did poorly (about chance ) - was this a bug/not enough regularization or was it due to using the relatively quicker, unfamiliar totalboost algorithm? we can test this by running logitboost or gentleboost (which worked previously) now that we don&#8217;t have the pressure of the kenP deadline </del> Results:    <br /><br /> 
   
	Additional: Running logitboost, also looking at methods for generating feature importance from boosting ensembles (using matlab toolbox so it&#8217;s a bit of a blackbox to get information from) <br /> <br />
<a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/LOSO_wake_gentleboost_2_classes.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/LOSO_wake_gentleboost_2_classes.jpg" width="700" height="350" /></a>


<li> when looking at classification scores that are significantly better than chance, let&#8217;s also look at scores below chance    <br /> <br />
   
<li> do we want to look out longer than 1 second after a tone is played? if first 500ms of sleep trial data happens to fall on down phase, we wouldn&#8217;t expect classification to work for that trial    <br /> <br /> <br />
   
<li> sleep transform:    <br />
    - LOSO version    <br />
    - use face AND scene transformed data    <br />
    - exclude electrodes individually after finding good timebin/freq for wake    <br />
    - explicitly disentangle effects of dot-product vs. correlation on sleep transform    <br /> <br />
   
<li> separate face vs. scene classifiers (face vs. objects/scrambled faces/scrambled scenes AND scene vs. objects/scrambled faces/ scrambled scenes): are we doing better at one of these than the other?  do they have different optimal classification times? (ties into previous thoughts about AUC: looking at face and scene classifier difference, diffAUC, vs. meanAUC)   br&gt;<br />
   
<li> classification of data using ERPs   <br /><br />
   
<li> visualize data:   <br />
    - interface with EEGLAB topoplots   <br />
    - compare face/scene templates per subject to those generated on an individual-subject level  <br /> 
    - similarity of sleep data across time bins to wake pattern via simple correlation: are these different for the different classes?    <br /> 
		- we can look at the average similarity to a face or scene pattern against the average similarity for all other classes across sleep time  bins for each frequency__******(important)__    <br /> 
    - autocorrelation of sleep data for various electrodes, could we use this as an indicator of down phases for the purpose of excluding time bins or trials?  is there another proxy for determining a down phase?      <br /><br />
      
    
<li> LOSO using some form of non-linear classification? previously we didn&#8217;t think this was a viable option given the ratio of training examples to features, but the combination of Hilbert-transformed data and LOSO makes me think this is now possible   <br /><br />
   
<li> exclude forgotten items from the sleep analysis, does this improve our sleep cross-validation?   <br /><br />
   
<li> test wake reactivation during learning of associated locations (this is different than the current wake data we use to train on which we take from the wake localizer)   <br /><br />
   

<li> generate simulated data to test idea that we could even find a wake pattern embedded randomly in a sleep data timebin-freq bin. also, would be good for general code validation (importance map transformations, mvpa code, etc)  <br /><br /> 


<li> look at classifier output for scrambled conditions and use to sleep transform: do these results look different than what we get when we transform via face and scene conditions?  <br /><br /> 
   
<li> &#8220;feature shuffle noise&#8221; wake classification analysis: do we get better generalization when we extend training data with patterns that are hybrids of two patterns from the same class? analogous to dropout techniques for neural networks   <br /><br /> 

<li> What if we transform the sleep data according to the timebin-freq pair that performs the best for each subject?  This is obviously less parsimonious than we&#8217;d like, but a signature of reactivation is a signature of reactivation.    

<li> For results with _really_ low wake classification accuracy, what do we get if we sleep transform by flipping the labels on the importance maps generated?    

</li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></ol>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG: Leave-one-subject-out]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/08/26/sleep-eeg-leave-one-subject-out/"/>
    <updated>2014-08-26T14:45:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/08/26/sleep-eeg-leave-one-subject-out</id>
    <content type="html"><![CDATA[<h2 id="wake-leave-one-subject-out-loso-logistic-regression-use-all-freqs-all-times">Wake Leave-One-Subject-Out (LOSO) Logistic Regression, Use All freqs, All Times</h2>

<p><a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/log_reg_LOSO_all_features_class_AUC_vary_lambda.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/log_reg_LOSO_all_features_class_AUC_vary_lambda.jpg" width="700" height="350" /></a></p>

<h4 id="this-should-contain-the-mcduff-importance-maps">This should contain the McDuff Importance Maps</h4>

<h2 id="wake-loso-logistic-regression-all-freqs-restrict-time-range-to-150ms---550ms">Wake LOSO Logistic Regression, All Freqs, Restrict Time Range to 150ms - 550ms</h2>

<p><a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/log_reg_LOSO_all_features_btwn_150ms550ms_lambda100_persubj.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/log_reg_LOSO_all_features_btwn_150ms550ms_lambda100_persubj.jpg" width="700" height="350" /></a></p>

<h2 id="wake-loso-logistic-regression-time-freq-sweeps">Wake LOSO Logistic Regression, Time-Freq Sweeps</h2>

<h2 id="wake-loso-boosting-totalboost-all-freqs-all-times">Wake LOSO Boosting (TotalBoost), All Freqs, All Times</h2>

<h2 id="loso-sleep-logistic-regression-all-freqs-all-times">LOSO Sleep Logistic Regression, All Freqs, All Times</h2>

<h2 id="loso-sleep-boosting-all-freqs-all-times">LOSO Sleep Boosting, All Freqs, All Times</h2>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Relational RL]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/08/10/relational-rl/"/>
    <updated>2014-08-10T16:12:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/08/10/relational-rl</id>
    <content type="html"><![CDATA[<h2 id="motivation">Motivation</h2>

<p>Why do we even care about relational reinformcent learning?  </p>

<ol>
  <li>
    <p><em>Rule Learning by Seven-Month-Old Infants.</em> Marcus, Vijayan, Rao, &amp; Vishton, 1999:<br />
<strong>Methods:</strong>  <br />
7-month old infants were exposed to sentences of the form: ABB or ABA during a training phase (e.g. “ga ti ti” or “ga ti ga”), testing phase entailed consistent or inconsistent conditions where presented with entirely new words either in ABB or ABA format (e.g. “wo fe fe” or “wo fe wo”). <br />
<strong>Results:</strong> Infants looked longer at flashing light during inconsistent sentences in test period ( e.g. train “ABB”, test “ABA”) compared to consistent.<br />
<strong>Discussion:</strong> This indicates rule learning in infants, cannot be explained by statistical learning of transition probabilities because test set consisted entirely of new words (no estimate of transition probability).  Requires extraction of relationships: are these two entities the same?  </p>
  </li>
  <li>
    <p>Can generalize relationships over identities of objects in the world - variable abstraction.</p>
  </li>
  <li>
    <p>Introspection: our own capacities for abstraction and transfer suggest the importance of symbolic processing (Buchheit, 1999).</p>
  </li>
  <li>
    <p>Allows rich, intuitive specification of background knowledge: facilitating learning to new tasks (via bootstrapping from previously learned experiences): using a key on a lock is not dependent on the particular task at hand: if we learn through trial and error the abstract transition model $ T(isLocked(lock) = true, unlocks(lock,key) = true, useKeyOnLock(lock,key)) \rightarrow  isLocked(lock) = false $, then we can utilize that information in any domain that uses keys and locks.  While this allows for humans to specify background knowledge to an artificial agent by inserting that definite clause into the initial transition structure for a new RL problem, the more satisying use case is one where an agent builds up its own set of abstract relations and takes a library of relations into each new task.   </p>
  </li>
  <li>
    <p>Once rules have been learned, acting with them is a well-studied research problem (Pasula, Zettlemoyer, and Kaelbling, 2004) allowing for one-shot policy adjustment given novel domain configurations (e.g. a previously blocked path is now open).</p>
  </li>
  <li>
    <p>Propositional logic isn’t sufficient, need first-order a.k.a. relational representations: chess can be propositionalized as follows via propositions as follows: <br />
<code>NumberOfBlackPawnsIsNotFive</code> and <code>whiteKingOnSameLineAsBlackKing</code>, but requires fixed number of objects, proposition to be constructed for all possible relations between all objects.  Compare to the relational representation: <code>sameLine(blackKing,whiteKing)</code>.  The effect of this being that we cannot easily generalize over objects or similar situations (Van Otterlo 2005). See Bongard problem for another example where relational representation is much more natural than propositional.  <strong>Take home:</strong> the allure of first-order logic is not simply that there exist theorem provers and goal regression (which is also a property of propositional logic), but the fact that it explicitly provides a mechanism for relationships between entities.   </p>
  </li>
  <li>
    <p>Lookup tables or propositional representations aren’t able to represent structural aspects of states and actions in relational domains such as Block World.   </p>
  </li>
  <li>
    <p>A representation should enable representing and reasoning about <em>objects</em> (Kaelbling et al., 2001); one has to be be able to represent objects and relations in our language if an intensional stance is taken (Dennett, 1987).</p>
  </li>
</ol>

<h2 id="relational-learning">Relational Learning</h2>

<p>Learning in first-order logic can be broken into two categories: <br />
1. <strong>parameter learning:</strong> assuming that we’re given a set of definite clauses, let’s learn the parameter values (i.e. probabilities).<br />
2. __ structure learning:__ let’s learn both the definite clauses and the associated parameters (using refinement and generalization operators to maximize).   </p>

<h2 id="what-is-relational-reinforcement-learning">What is relational reinforcement learning?</h2>

<p>Relational reinforcement learning hinges on upgrading representations of the individual components of an MDP e.g. states, transition function, etc.  There are various relational representations with which the components of an MDP can be upgraded: first-order logic (instantiated as stochastic context-free grammars or probabilistic relational models), graph-based relational representations.  Here we’ll stick with a probabilistic first-order logic:</p>

<p>Formally, given a first-order probabilistic logic $ \Lambda $ (e.g. <em>inductive logic programming</em>), a hypothesis $ \Upsilon \in \Lambda $, predicates $p$, constants $c$, a special set of predicates $A$, then we can define the MDP as:<br />
  $ S: {s \in HB^{P \cap C} | s \models \Upsilon } $ <br />
  $ A: {a \in HB^{A \cap C} | a \models \Upsilon } $<br />
  $ T: S x A \longmapsto S $ <br />
  $ R: S x A \longmapsto R $ <br />
  $ HB = \text{Herbrand Base} $ <br />
  $ \Upsilon $ defines which states are possible in the current domain<br />
  $ \models $ means is an interpretation of   </p>

<h4 id="example-relational-mdp-definition-of-states-and-actions">Example relational MDP definition of states and actions</h4>
<p>$ P = {on/2, clear/1} $<br />
$ C = a,b,c,d,e, floor $<br />
$ A = {move/2} $</p>

<p>This defines the set of all possible states by filling in each possible constant into each possible slot of the predicates such that $ \mid S\mid = 501 $ legal states</p>

<p>Holy Moley Batman, that’s a lot of states!  If $\mid C\mid = k$ and the arity (number of arguments accepted by a predicate) is $ \alpha_p $, then the total number of possible interpretations is $ \prod_p 2^{k^{\alpha_p}} \implies $ <strong>intractable</strong>.  </p>

<p>Clearly, like many other state representations, we are in dire need of abstraction.  This is done by using <em>ungrounded</em> predicates for learning a transition structure or policy, that is, predicates where the arguments are populated entirely or partially by variables.  </p>

<h4 id="example-abstract-policy">Example abstract policy</h4>
<p>Assuming the current goal is to place object a on object b, here is an abstract policy utilizing the decision list construction where we follow the rules sequentially and apply the first rule that matches: <br />
  $ r<em>0: on(a,b) $ $\rightarrow \text{no move} $  <br />
  $ r</em>1: onTop(X,b) \rightarrow move(X,floor) $  <br />
  $ r<em>2: onTop(X,a) \rightarrow move(X, floor) $  <br />
  $ r</em>3: clear(a), clear(b) \rightarrow move(a,b) $   </p>

<p><strong>Note:</strong> It is important that any ungrounded terms (i.e. <em>variables</em>) that appear on the lefthand side of a rule, also appear on the righthand side.   </p>

<h2 id="model-free-relational-rl">Model-free relational RL:</h2>

<h4 id="q-value">Q-Value</h4>
<p>The general idea is to use relational regression in order to approximate q-value function.  This has yielded things like:<br />
    k-nearest neighbor: store examples, predict new q-values using distance-weighted average (requires defining distance over relational states) - also called relational instance-based regression (I think)   <br />
    kernel-based regression using Gaussian processes: pretty similar to above, requires defining kernel over graphs or using convolution kernel, two benefits: can control generalization via covariance function smoothing parameter and since it’s embedded in a statistical framework, can give confidence of q-values which can be used to guide exploration using any of a bunch of algorithms such as upper-confidence bound, etc. <br />
    learning decision list: use relational regression to learn <em>grounded</em> Q-value function, calculate optimal <em>grounded</em> policy, feed optimal ground policy to decision list learner to produce <em>abstract</em> policy, keep both grounded and abstract policy around, use abstract policy to guide learning via generalization <br />
    TG relational regression:  incrementally build first-order trees; relational MDP specification defines which predicates are posssible, TG algorithm incrementally adds them to a regression tree as needed  </p>

<h4 id="approximate-policy-iteration">Approximate Policy Iteration</h4>

<h4 id="relational-naive-bayes">Relational Naive Bayes</h4>

<h4 id="q-larc">Q-LARC:</h4>
<p>feature construction based on predictive features   </p>

<h2 id="model-based-relational-rl">Model-based relational RL:</h2>

<p>Idea: Translate Bellman Backup Operator into relational domain.  Requires finding “preconditions” i.e. what states could lead to this state (this is called <em>regression), then a _combination</em> operator, and a <em>maximization</em> operator to define relational Bellman operator.   Defines new abstract policy from old abstract policy.  This is called <strong>decision-theoretic regression</strong>.  See explanation-based reinforcement learning.</p>

<h2 id="applications-of-relational-reinforcement-learning">Applications of relational reinforcement learning</h2>

<h4 id="learning-grounded-relational-symbols-from-continuous-data-for-abstract-reasoning">Learning Grounded Relational Symbols from Continuous Data for Abstract Reasoning</h4>

<h4 id="learning-probabilistic-relational-planning-rules">Learning Probabilistic Relational Planning Rules</h4>

<p><strong>learnRules:</strong> performs a search through a rule set using ILP operators (i.e. refinement and generalization operators)<br />
<strong>induceOutcomes:</strong>  finds a best set of outcomes given a context and an action <br />
<strong>learnParameters:</strong> learns a probability distribution over a set of outcomes  </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG: Hilbert Transformed Data]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/08/06/sleep-eeg-hilbert-transformed-data/"/>
    <updated>2014-08-06T10:36:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/08/06/sleep-eeg-hilbert-transformed-data</id>
    <content type="html"><![CDATA[<h2 id="wake-hilbert-transformed-classification">Wake Hilbert Transformed Classification</h2>

<p><a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/wake_hilb_mean_across_subjects.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/wake_hilb_mean_across_subjects.png" width="700" height="350" /></a></p>

<p><strong>Figure 1:</strong> Average AUC on Hilbert-transformed wake data across all subjects for each time bin and frequency band combination.</p>

<p><strong>Thoughts:</strong> We see “hotspots” where we were hoping to see them: at the theta frequency around 200-230ms. Moreover, we see that the 200-230ms theta bin forms a local peak: as you move earlier or later in time, classification accuracy decreases.  Importantly, the accuracy using these bands (as opposed to individual frequencies like we had before) yields comparable average wake classification across subjects if we look at this <a href="http://ElPiloto.github.io/images/research/sleep_eeg_9_subjects_06_23_2014/wake_auc_avgd.png">old plot</a>.  If this result holds for all subjects, then we would feel pretty good about using the 200ms (or 230ms) theta time-freq bin to transform the sleep data.  Therefore, let’s look at the individual subjects to see if these results are driven by some subjects or if they seem to be pretty evenly distributed (spoiler: doesn’t seem to be a good time-freq bin, or even classifiable, for all subjects, but it does pretty well)</p>

<p><a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/wake_hilb_all_subjects_all_freqs_all_times.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/wake_hilb_all_subjects_all_freqs_all_times.png" width="700" height="350" /></a></p>

<p><strong>Figure 2:</strong> AUC heatmaps of classification AUC per subject.  Subject IDs are along the top of each subplot, timebins run along the y-axis and frequencies along the x-axis (only explicitly marked for the last subject, but the axes are exactly the same as Figure 1).</p>

<p><strong>Thoughts:</strong> These plots, though informative, suck for the purpose of evaluating how good the 200-250ms, theta bins do across all subjects.  Let’s zoom in on the areas of interest below.  One additional point, is that we’re getting <strong>much</strong> better max AUC across timebins than previously before (using individual frequencies) suggesting that the Hilbert transform is a better preprocessing technique for classification.</p>

<p><a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/wake_across_subjs_times7__8__9_freq2.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/wake_across_subjs_times7__8__9_freq2.png" width="700" height="350" /></a></p>

<p><strong>Figure 3:</strong>  Above we show the AUC for each subject for 3 different times (200, 225, and 250ms respectively) for the 4 Hz (theta) frequency band.</p>

<p><strong>Thoughts:</strong> This isn’t the greatest, especially considering some subjects have time-freq bins with much higher AUC.  “Irregardless,” these results are sufficiently positive to justify transforming sleep data.  Everything that we said about the non-band wake analyses holds here (e.g. we can look at other bins that look good for subjects, we can take each subject’s best bin to transform the data, etc).</p>

<h2 id="sleep-hilbert-untransformed-classification---sweep-across-time-and-frequency-combinations">Sleep Hilbert Untransformed Classification - Sweep Across Time And Frequency Combinations</h2>

<p>Below we look at how discriminable the sleep data is at each particular timebin and frequency combination of the sleep data using logistic regression.  Additionally, we test the regularization parameter for two separate values $ \lambda = 1, 50 $</p>

<p><a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/sleep_hilb_mean_across_subjects_lambda1.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/sleep_hilb_mean_across_subjects_lambda1.png" width="700" height="350" /></a></p>

<p><strong>Figure 4:</strong> Above we show the across-subjects average AUC for performing logistic regression on the sleep data at each time and frequency band combination with regularization parameter $ \lambda = 1 $.</p>

<p><a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/sleep_hilb_lambda50_mean_across_subjects.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/sleep_hilb_lambda50_mean_across_subjects.png" width="700" height="350" /></a></p>

<p><strong>Figure 5:</strong> Same as Figure 4, except this time we set the regularization parameter $ \lambda = 50 $.</p>

<p><strong>Thoughts:</strong> The band around 175ms, 16 Hz looks promising - we had mentioned that we’d be pretty happy if we could get something like 7-8 points above chance average classification accuracy across subjects.  This is much higher than the average classification accuracy ($ mean AUC = 0.52 $) we were getting for our “best 8” untransformed, logistic regression average AUC using all times and frequencies (recall that those results were not using frequency bands like we are now, but instead look at the power spectrum at individual frequencies).  Obviously, we can’t directly compare the individual time and frequency band results to using all time and frequency features, but it’s the closest benchmark we have.  Those old results can be found <a href="http://ElPiloto.github.io/blog/2014/07/27/sleep-eeg-post-boosting-results/"> here </a> under the section “To-Do Item #2”.</p>

<p>These results are encouraging for two reasons.  First, the best time and frequency combination falls within a plausible time range.  We could talk to James to see if he has any a priori evidence to support getting results in the BETA frequency range, but at least the time isn’t too early.  Second, the AUC is pretty good on the individual subject level as we see below: most subjects are either at or above chance.</p>

<p><a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/best_time_freq_untransformed_sleep_lambda1.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/best_time_freq_untransformed_sleep_lambda1.jpg" width="700" height="350" /></a></p>

<p><strong>Figure 6:</strong> Above we show the AUC for each subjects generated by performing logistic regression on the sleep data at the best time and frequency band combination with regularization parameter $ \lambda = 1 $.</p>

<p><a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/best_time_freq_untransformed_sleep_lambda50.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/best_time_freq_untransformed_sleep_lambda50.jpg" width="700" height="350" /></a></p>

<p><strong>Figure 7:</strong> Same as Figure 6, except this time we set the regularization parameter $ \lambda = 50 $.</p>

<p><strong>Thoughts:</strong> Okay, so we have <strong>a</strong> time and frequency combination that gives us good classification accuracy, but this could just arise from multiple comparisons.  Sure, we can take comfort that the classification accuracy seems to peak at 175ms and degrades smoothly as we move away, but it’s also the case that the EEG power spectrum is pretty similar at those times.  <strong>SO</strong>, what would it take for me to believe these results?  Running classification on <strong>ALL</strong> of the <em>untransformed</em> sleep data features with heavy regularization and yielding good classification accuracy with weights that consistently preferred that time and frequency combination.  This is next on my to-do list, it shouldn’t take too long to get up and running.  I’d also like to add a reminder to myself that it may be worth subsampling the time dimension.</p>

<h2 id="sleep-hilbert-untransformed-log-reg-classification---use-all-time-and-frequencies">Sleep Hilbert Untransformed Log Reg Classification - Use All Time and Frequencies</h2>

<p><a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/untransformed_logreg_all_features.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/untransformed_logreg_all_features.png" width="700" height="350" /></a></p>

<p><strong>Figure 8:</strong> Above we show the AUC per subject (along the y-axis) for various logistic regression regularization penalty values (along the x-axis) for classification using ALL of the features for the untransformed (i.e. not transformed according to wake pattern) sleep data using frequency bands (using Hilbert transform).  The <em>NaN</em> value is because that particular job failed and I didn’t think it was that important to justify putting off these results.</p>

<p><a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/untransformed_logreg_all_features_avg_across_subjects.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/untransformed_logreg_all_features_avg_across_subjects.png" width="700" height="350" /></a></p>

<p><strong>Figure 9:</strong> Same as Figure 8, except this time we average across subjects for each regularization setting.</p>

<p><strong>Thoughts:</strong> These results definitely aren’t what we were hoping for, and subject09’s  results are worrisome.  We’ve seen pretty consistently classifiable results out of subject 15 which is a good sanity check that there isn’t something wrong with the code or with the range of regularization values we’ve tried.  The best case scenario is that subject 15’s data is really easily classifiable, whereas the other subjects require more careful feature selection.  Towards this end, I’m going to try three things.  First, I’m running this same exact analysis except I’m including feature selection as a preprocessing step (note: I’ll have to run it for just a single regularization value since the feature selection process is pretty time-consuming).  Second, I’m going to look at the importance maps generated by this current analysis to see which features are being loaded upon.  Lastly, I’m going to try boosting using all the features.  This would also be a great place to have simulated data, but that’ll have to take a back seat.</p>

<h2 id="sleep-hilbert-untransformed-gentleboost-ensemble-classification---use-all-time-and-frequencies">Sleep Hilbert Untransformed GentleBoost Ensemble Classification - Use All Time And Frequencies</h2>

<p><a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/untransformed_gentleboost_all_features_ACC.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/untransformed_gentleboost_all_features_ACC.png" width="700" height="350" /></a></p>

<p><strong>Figure 10:</strong> Above we show the AUC per subject (along the y-axis) for various logistic regression regularization penalty values (along the x-axis) for classification using ALL of the features for the untransformed (i.e. not transformed according to wake pattern) sleep data using frequency bands (using Hilbert transform).  The <em>NaN</em> value is because that particular job failed and I didn’t think it was that important to justify putting off these results.</p>

<p><a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/untransformed_gentleboost_all_features_avg_across_subjects_ACC.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/untransformed_gentleboost_all_features_avg_across_subjects_ACC.png" width="700" height="350" /></a></p>

<p><strong>Figure 11:</strong> Same as Figure 10, except this time we average across subjects for each value of the <code>num_learners</code> parameter.</p>

<h2 id="the-continuing-story-of-delbungalow-billdel-subject-09">The Continuing Story of <del>Bungalow Bill</del> Subject 09</h2>

<p><strong>Sleep Statistics:</strong> 35 faces, 35 scenes.  Cross-validation was performed using leave-one-out-cross-validation (a.k.a. 35-fold).</p>

<p>Achieves reasonable classification accuracy using boosting.  Additionally, classification accuracy (as opposed to AUC) isn’t SO horrendous for logistic regression (0.31 for feature selection using $\lambda = 100$ and setting <code>stat_thresh</code> = 0.1)  When we look at the AUC per cross-validation iteration, we find that the AUC is always 0 or 1 essentially (see figure 11 below).  It seems like we’re severly overfitting.  This is supported by the fact that the boosting method actually does well for subject 09 ( boosting much less susceptible to overfitting).  However, if this really was the case, you’d expect that doing feature selection would help this subject’s results.  Currently, that is not the case, so I’m gunna try even stricter feature selection to see if that’s what’s really at the bottom of this.</p>

<p><a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/auc_per_iteration_feature_select_subj9.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/auc_per_iteration_feature_select_subj9.jpg" width="700" height="350" /></a></p>

<p><strong>Figure 12:</strong> Above we show the AUC per cross-validation iteration for subject 09 using logistic regression on all of the untransformed sleep features</p>

<p><a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/per_fold_correctminusincorrect_class_outputs.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/per_fold_correctminusincorrect_class_outputs.jpg" width="700" height="350" /></a></p>

<p><strong>Figure 13:</strong> Above we show the difference between the classifier output for the correct class minus the classifier output for the incorrect class for each test item over all cross-validation iterations for various parametrizations of wake logistic regression classification for subject 9.  Note that for p = 0.01, we get something like 30-50 features selected.</p>

<p><strong>Random thought:</strong> What would we get if we tried to use this for sleep transforming data by flipping the classes?</p>

<p><strong>Extra thoughts:</strong> I was feeling uneasy about the fact that AUC and classification accuracy give completely different answers for subject 09.  However, I went through and verified by hand that the code for calculating the AUC is in fact correct (given the type of AUC metric we want to use).  Currently, we’re looking at the distribution of (face - scene) classifier outputs to calculate the AUC, but we get vastly different results for this subject (and I assume other subjects as well), if we calculate a separate AUC for the face classifier output, a separate AUC for the scene classifier output, and then take the average (possibly weighted by number of examples from each class) of these two AUCs to yield a single number (let’s call this method <em>meanAUC</em>).  Although we’ve been evaluating with respect to the former definition of AUC ( let’s dub this <em>diffAUC</em>), the best metric to use actually depends on how we intend to relate the sleep classifier output to the behavioral data.  One method for relating classifier output uses a binary measure to predict subsequent performance: each reactivation event is labelled as a 0 or a 1 indicating whether or not the classifier predicted the correct class.  Subsequent memory for an item should increase if the classifier correctly classified reactivation events.  In this case, the appropriate AUC method to use is the <em>diffAUC</em> because we’re interested in the difference between the classifier outputs for each class.    An alternative method for relating classifier output to behavioral data would be to only look at, for each reactivation event, the classifier output from the classifier corresponding to the item’s category (i.e. if we’re looking at a face item, we only look at the face classifier output; if we’re looking at a scene item, we take the scene classifier output).  We would predict that items with a high classifier output, despite their category, would yield better subsequent memory.  This method lends itself to evaluation via <em>meanAUC</em>.   <strong>They are fundamentally different metrics</strong> and so I shouldn’t take this as a sign that my code base is messed up.  To further illustrate how different these metrics can be, below I provide a sample of the different metrics for a particular subject:   </p>
<pre>
                 diffAUC: .1714
                 meanAUC: 0.5286
              % correct: 0.2857
</pre>

<p>Here’s a case that’s also a bit counterintuitive, but checks out.  I’m leaving this here just as a note for future Luis.</p>
<pre>
true classes = {face, scene}
---------------------------
face = 0.435, 0.4635
scene = 0.5650, 0.5365
diff = -0.13, -0.07

Max Class Output Prediction: scene (incorrect), scene (correct)
% correct = 0.5
diffAUC = 0
</pre>

<h2 id="ken-meeting-points">Ken Meeting Points</h2>

<h4 id="auc">AUC</h4>
<p>The space of <em>diffAUC</em>, <em>meanAUC</em>, and individual AUCs (e.g. <em>faceAUC</em> or <em>sceneAUC</em>) are all justifiable classification metrics.  Let’s not open this can of worms just yet, <em>however</em>, let’s <strong>definitely</strong> switch to pooling the classification outputs from all cross-validation folds and <em>then</em> calculating AUC, instead of calculating AUC for each fold, and averaging across folds.  That being said, subject 09 per-fold diffAUC was 0.1714 (see above), and their pooled diffAUC = 0.18 - so it doesn’t seem to have too much of an impact as I suspected.</p>

<h4 id="feature-selection">Feature Selection</h4>
<p>Ken thinks it’s unconventional to use feature selection and L2 regularization.  Let’s try L1 regularization instead, which has one less parameter and performs multivariate feature selection.</p>

<h4 id="subject-9-crappiness">Subject 9 Crappiness</h4>
<p>I haven’t justified why overfitting should lead to systematically incorrect predictions, it should lead to chance performance.   It may be the case that feature selection and L2 regularization is just susceptible to pathological results given that our classifier does okay when we try boosting.  If we try L1-regularization and that works okay, then we’ll know the problem is feature selection + L2 regularization.</p>

<h4 id="next-steps">Next Steps:</h4>
<p>Let’s try our leave-one-subject-out analysis for individual time-frequency pairs.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Probabilistic Logic Learning - Luc De Raedt and Kristian Kersting]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/08/05/probabilistic-logic-learning-luc-de-raedt-and-kristian-kersting/"/>
    <updated>2014-08-05T12:51:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/08/05/probabilistic-logic-learning-luc-de-raedt-and-kristian-kersting</id>
    <content type="html"><![CDATA[<h4 id="what-does-probabilistic-logic-learning-mean">What does probabilistic logic learning mean?</h4>
<p><strong>probabilistic</strong> - probabilistic representations and reasoning mechanisms <br />
<strong>logic</strong> - first order logical and relational representations (compared to propositional logic affords reasoning about objects)</p>

<h3 id="logic-glossary">Logic Glossary</h3>
<p>first-order logic $\leftrightarrow$ relational representation <br />
allows for:<br />
<strong>extensional</strong>: <code>ram(r1,c1)</code> – </p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG: Varying regularization and simulated results]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/08/01/sleep-eeg-varying-regularization-and-simulated-results/"/>
    <updated>2014-08-01T10:27:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/08/01/sleep-eeg-varying-regularization-and-simulated-results</id>
    <content type="html"><![CDATA[<h3 id="logistic-regression-untransformed-sleep-data-varying-regularization-term">Logistic Regression Untransformed Sleep Data: Varying regularization term</h3>
<p>The motivation for this line of inquiry is contained in <a href="http://ElPiloto.github.io/blog/2014/07/27/sleep-eeg-post-boosting-results/">this post</a> under the discussion section for figure 4.</p>

<p><a href="http://ElPiloto.github.io/images/research/untransformed_log_reg_lambda250and750.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/untransformed_log_reg_lambda250and750.jpg" width="700" height="350" /></a></p>

<p><strong>Figure 1:</strong> This shows the sleep classification accuracy for logistic regression using untransformed sleep data with <code>statmap_anova</code> feature selection (stat_thresh = 0.1).</p>

<p><strong>Thoughts</strong> Well this is interesting.  I had previously been using a regularization term of 1 for all the logistic regression analyses and to see this level of classification accuracy using a regularization penalty two orders of magnitude higher than previously suggests that I didn’t have much of a grasp on the role of the regularization term in this analysis.  The $\lambda = 750$ case is encouraging because it’s up there in terms of the highest average classification accuracy we’ve seen AND it includes that largest regularization term we’ve used thus far.  It’s worth investigating how far we can push regularization before it collapses, thus I’ve launched an additional analysis trying results with regularization = 1250 and 1750.  Will update this post when those results are in.  ALSO, this begs the question: should we re-run our sleep transformed analyses using stronger regularization for the wake classes?  <strong>YES</strong></p>

<p><a href="http://ElPiloto.github.io/images/research/untransformed_log_reg_lambda1250and1750.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/untransformed_log_reg_lambda1250and1750.jpg" width="700" height="350" /></a></p>

<p><strong>Figure 2:</strong> This shows the sleep classification accuracy for logistic regression using untransformed sleep data with <code>statmap_anova</code> feature selection (stat_thresh = 0.1).</p>

<p><strong>Thoughts:</strong> This still doesn’t make it seem like we’ve hit the upper bound on the regularization penalty. I’m not sure what to make of the fact that 750 = good regularzation, 1750 = good regularization, but 1250 gives pretty crappy results.  I guess I don’t have any strong theoretical justification to think classification accuracy would vary smoothly as we modify the regularization parameter.  I’m now launching sleep untransformed classification on <em>all</em> the subjects with a high regularization term.</p>

<p><a href="http://ElPiloto.github.io/images/research/untransformed_log_reg_lambda12000_ALLSUBJS.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/untransformed_log_reg_lambda12000_ALLSUBJS.jpg" width="700" height="350" /></a></p>

<p><strong>Figure 3:</strong> This shows the sleep classification accuracy for logistic regression using untransformed sleep data with <code>statmap_anova</code> feature selection (stat_thresh = 0.1).</p>

<p><strong>Thoughts:</strong> This is clearly too high for most subjects. Although <strong>some</strong> of the subjects that have been doing well in our “best 8” analysis get pretty good classification with $\lambda = 2000$, which is unsurprising given their results for $\lambda = 1750$, some other subjects (e.g. subject 7) that did well are now getting below-chance accuracy.  The flop of some of the good “best 8” subjects from above to below chance accuracy makes a good case for reducing the regularization penalty.  Thus, I’m gunna try running all subjects with a smaller value for $\lambda$ because the infrastructure is in place to do that.</p>

<h3 id="simulated-results">Simulated Results</h3>

<p><del><strong>STILL CODING</strong></del> <strong>PUT ON HOLD, GOING TO TRY HILBERT TRANSFORMED DATA ANALYSIS FIRST</strong></p>

<!--Ken is most interested in the hilbert-transformed data in the hopes that the Hilbert stuff will clean up (induce consistency across subjects) for wake.  Hopefully see good classification across subjects theta, 200-ish, also which electrodes are informative.-->
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG: Group Meeting]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/07/28/sleep-eeg-group-meeting/"/>
    <updated>2014-07-28T17:10:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/07/28/sleep-eeg-group-meeting</id>
    <content type="html"><![CDATA[<h3 id="ken-succinctly-described-two-levels-of-complexity-in-our-sleep-analysis">Ken succinctly described two levels of complexity in our sleep analysis:</h3>
<ol>
  <li>How do we transform the sleep?  Recall this can be face or scene transformed, we can use correlation or the dot product, we can use data from different frequencies and different time bins in the wake data, etc.<br />
    <ul>
      <li>Perhaps we don’t need to sleep transform?</li>
      <li>Perhaps we need to exclude electrodes explicitly? We thought the McDuff importance maps method implemented this, but correlation doesn’t care about the magnitude so we need to re-evaluate this (either use dot product OR filter out electrodes by some other measure)  <strong>(TODO ITEM: dot product)</strong></li>
    </ul>
  </li>
  <li>When does reinstatement happen?  How should we score reinstatement?  If we do classification, do we just look at one time bin for the sleep?  Do we sum up classification across all time bins, etc.<br />
    <ul>
      <li>Maybe we want to use a simpler method than classification a la Staresina paper: sum correlation between template and sleep pattern and threshold to indicate replay event.   </li>
      <li>We could and should look at trial-by-trial plots of correlation across time for both the incorrect and correct pattern - eyeball the crap out of this.  <strong>(TODO ITEM)</strong></li>
    </ul>
  </li>
  <li>James also mentioned trial-by-trial variability: if cue during down phase, won’t expect reactivation in next 500 ms when all neurons are turned off.</li>
</ol>

<h3 id="behavioral-results-look-great">Behavioral Results Look great</h3>
<ol>
  <li>Spindles too rare to use to limit the sleep data   </li>
</ol>

<h3 id="additional-thoughts">Additional thoughts:</h3>
<ol>
  <li>We should leave open the option to look at ERPs for classification, although Ehren’s thoughts were that ERP winds up showing in theta band of power spectrum, plus see things in power spectrum that you don’t see in the ERPs.</li>
  <li>We can theoretically get more classification juice if we exclude forgotten items from the sleep analysis.  </li>
  <li>Waiting on James to get power spectrum for bands using Hilbert transform  </li>
  <li>We have the data to look at wake classification during wake reactivation i.e. can we classify when they’re learning the associated locations?   </li>
</ol>

<h3 id="todo-items">TODO ITEMS:</h3>
<p><strong>dot product:</strong><br />
<del>currently running sleep log reg mcduff using both faces and scenes, results will be in <code>sleep_xform_mcduff_scene_face_8subjs_dotprod.mat</code></del>  Results in post: sleep-eeg-boosting-results
<del>currently running boosting gentle log reg using both faces and scenes, results will be in <code>boosting_mcduff_best8_gentle_scene_face_dotprod.mat</code></del>Results in post: sleep-eeg-boosting-results, <strong>EXCEPT</strong> for some reason</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG: Post Boosting Results]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/07/27/sleep-eeg-post-boosting-results/"/>
    <updated>2014-07-27T14:46:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/07/27/sleep-eeg-post-boosting-results</id>
    <content type="html"><![CDATA[<h3 id="our-to-do-list-consists-of-the-following">Our to-do list consists of the following:</h3>
<ol>
  <li>logistic regression on sleep data using data from all time bins of the sleep data and compare this to existing results from boosting  </li>
  <li>try boosting and logistic regression using <strong>untransformed</strong> sleep data (large volume of data)</li>
  <li>look at various other parameterizations of boosting classification result (just because it’s easy to do)</li>
  <li>Let’s look at what happens when the classifier gets fed in both face and scene transformed data   </li>
  <li>Let’s look at when the classifier gets transformed using both face and scene transformed data using the DOT product (see post )  </li>
</ol>

<h3 id="as-a-refresher-here-is-the-last-boosting-result">As a refresher, here is the last boosting result:</h3>

<p><a href="http://ElPiloto.github.io/images/research/ensemble_best_8_wake_subj_202ms_7hz_gentle.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/ensemble_best_8_wake_subj_202ms_7hz_gentle.jpg" width="700" height="350" /></a></p>

<p><strong>Figure 1:</strong> The y-axis shows classification accuracy.  Subjects and parameters are shown along the x-axis (g = gentleboost algorithm, f = face mcduff importance sleep transform).</p>

<h2 id="logistic-regression-sleep-importance-map-transformed-results-to-do-item-1">Logistic Regression Sleep Importance Map Transformed Results (To-Do Item #1)</h2>
<p><a href="http://ElPiloto.github.io/images/research/log_reg_best_8_subjects_mcduff_xform.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/log_reg_best_8_subjects_mcduff_xform.jpg" width="700" height="350" /></a></p>

<p><strong>Figure 2a:</strong> Displayed above is the classifier accuracy for all subjects(x-axis).  The classification accuracy was generated for various sleep importance map transformations (face, scene, or face minus scene), which is indicated along the y-axis.  The classifier features consisted of transforming the sleep data at each timebin and concatenating the sleep transformed data at each timebin into a single data matrix.  The entries along the x and y axis labelled “Mean” simply show the mean along the x and y axis respectively.</p>

<p><strong>Thoughts</strong>: These results lend further credence to the notion that it may be beneficial to sleep transform data according to both face and scene classifiers and feed both of those into a single classifier.  This idea that perhaps there is information in the scene pattern that isn’t in the face pattern (and vice versa) can be further scrutinized by looking at MDS plots of the untransformed data, the face transformed data, and scene transformed data - this may be worth doing depending on how much free time I have.  If I don’t end up looking at that, I’m not too upset since item #2 on the to-do list will also give us information about the role of the sleep transformation.  Additionally: note that the average classification accuracy, across both subjects and sleep transformation types, is pretty similar to that achieved with boosting, but none of the individual sleep transformations really give the same classification accuracy profile across subjects as the boosting results (this is easier to see in the plot below where I plot the rows of Figure 2 as separate horizontal bar graphs to match the format of the boosting results.  Why does it matter that accuracy across subjects in the logistic regression case doesn’t look like the accuracy achieved with boosting?  If our sleep transformation method yielded enough signal, then it should yield similar classification accuracy across subjects for the different classifier types.</p>

<p><a href="http://ElPiloto.github.io/images/research/log_reg_best_8_subjects_mcduff_xform_supplement.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/log_reg_best_8_subjects_mcduff_xform_supplement.jpg" width="700" height="350" /></a>
<strong>Figure 2b:</strong> A different visualization of the logistic regression plots useful for direct visual comparison against the ensemble results (Figure 1).</p>

<h2 id="to-do-item-3-boosting-classification-results-using-different-sleep-transforms-best-8-subjects">To-Do Item #3: Boosting Classification Results Using Different Sleep Transforms, Best 8 Subjects</h2>
<p><a href="http://ElPiloto.github.io/images/research/ensemble_best_8_wake_subj_202ms_7hz_gentle_ALL_TRANSFORMS.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/ensemble_best_8_wake_subj_202ms_7hz_gentle_ALL_TRANSFORMS.jpg" width="700" height="350" /></a></p>

<p><strong>Figure 3:</strong> This shows boosting classification accuracy using different sleep transforms.  The x-axis indicates which boosting algorithm (g for gentleboost) was used, the subject id (e.g. ‘15’ or ‘07’), and the wake pattern that was used for the sleep transformation (f = face, s = scene, M = face minus scene).</p>

<p><strong>Thoughts:</strong> This is a clear example where the classification accuracy results would look totally different based on which analysis parameters we use.  I suspect that if we get around to doing boosting classification using both the scene and face transformation, that will look entirely different, too.</p>

<h2 id="to-do-item-2-logistic-regression-classification-results-sleep---untransformed-best-8-subjects">To-Do Item #2: Logistic Regression Classification Results Sleep - UNTRANSFORMED, Best 8 Subjects</h2>
<p><del><strong>3/23 JOBS STILL RUNNING ON CLUSTER</strong>  This takes a relatively long time because the feature selection has to operate over ~100,000 features and that needs to run for each cross-validation fold for each subject.</del>  The results are displayed below</p>

<p><a href="http://ElPiloto.github.io/images/research/sleep_log_reg_raw_best8.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_log_reg_raw_best8.jpg" width="700" height="350" /></a></p>

<p><strong>Figure 4:</strong> This shows the result of performing logistic regression on the raw sleep data using all frequencies, all time bins, and all electrodes on raw data.  This amounts to about 100,00 features (64 electrodes x ~47 frequencies x ~38 time bins ) so obviously I need to run this analysis with feature selection.  I really wanted to include as many features as I could (without exceeding my RAM limits), so I used a loose stat threshold for statmap_anova feature selection (p = 0.1) which yielded about 10,000 features.</p>

<p><strong>Thoughts:</strong> This tells us that this data is damn noisy, we need to be smarter about what features we’re putting into the data.  Straightforward logistic regression isn’t working particularly well on this - it might be worth explicitly using LASSO instead of ridge regression as we’re currently using.  Additionally, I found something in the documentation of the <code>logRegFun</code> that suggests I should be trying much higher regularization values than I have been.  I’m going to try this out since it’s easy to run and I want to explicitly rule out the penalty term as why we’re not getting poor results. I will also look into whether or not it’s easy to use LASSO.  <strong>Update</strong> There does seem to be L1 regression in the MVPA toolbox via the <code>SMLR</code> function, although it implements multinomial regression (as compared to the current method of training a single logistic regression classifier for each class), however, it seems like it’ll bw</p>

<p>I think the fact that these results are comparable to the sleep transformed data in terms of classification accuracy tells us something about the sleep data, not necessarily about the sleep transformation.  I have to double-check the linear algebra behind this interpretation of the sleep data, but I think that when we transform the sleep data, we’re essentially projecting each dimension of the sleep data onto a single vector defined by the average pattern.  Now: if there is some best subspace of the untransformed data where the classes are just barely separable (i.e. you could get 52% accuracy), then whether or not we transform the sleep data won’t really matter.  If we don’t project, we can build a classifier that goes along the subspace I mentioned.  If we do project, then that same subspace will manifest in a different basis, but with the same separability.  Obviously you won’t get this barely classifiable subspace if the vector onto which you project is orthogonal to the subspace, but if there are multiple, weakly classifiable subspaces in the data - which is not crazy to think if the data are in general weakly separable, then one of these subspaces will not be orthogonal to the vector you’re choosing yielding once again a weakly separable subspace.  In fact, I suspect that we should be able to project onto random vectors and get the same classification accuracy on average - this comes from some light reading into random subspace projection theory where given a random projection you can give a probabilistic bound on the dot product between two vectors in a random subspace as a function of their dot product in the original space i.e. with some probability the dot product in the transformed space will be within some range of the dot product in the original space.  This is all pretty loosey goosey linear algebra logic, but I think it’s a start in the right direction for understanding what’s up with the transform vs untransformed data.</p>

<h2 id="to-do-item-4--feed-classifiers-face-and-scene-transformed-data-using-mcduff-importance-maps">To-Do Item #4:  Feed classifiers face AND scene transformed data using McDuff Importance Maps</h2>

<p><a href="http://ElPiloto.github.io/images/research/ensemble_best_8_wake_subj_202ms_7hz_gentle_face_AND_SCENE.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/ensemble_best_8_wake_subj_202ms_7hz_gentle_face_AND_SCENE.jpg" width="700" height="350" /></a></p>

<p><strong>Figure 5a:</strong> Boosting classification exactly like the Figure 1) except that these incorporate a feature vector that consists of transforming all frequencies with the importance map for scene and concatenating those features onto the feature vector generated by transforming all frequencies in the sleep data with the importance map for faces.</p>

<p><a href="http://ElPiloto.github.io/images/research/log_reg_sleep_mcduff_best8_faceSCENEtogether.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/log_reg_sleep_mcduff_best8_faceSCENEtogether.jpg" width="700" height="350" /></a></p>

<p><strong>Figure 5b:</strong> Exactly the same as figure 2a, recall we’re feeding in all the timepoints to logistic regression (no explicit feature selection in this version, although we still use regularization), except this time we used both face and scene transformed features in the data.</p>

<p><strong>Thoughts:</strong> Well, not uniformly super awesome classification results for both methods, BUT I’m pretty happy to see that they have the same general profile: across both analyses subjects 11,13,15 and 5,7,8 both pop out whereas classification is poor for both subject 9 and subject 1 in both analyses.  Consistency across analyses is the only proxy we currently have for assessing whether our results are due to random chance so this is a relatively nice result in that context.  That’s a score for the veracity of our results, but the other key aspect of evaluating our results, how good the results actually are, is still lacking.  Barring success with modifying the regularization term, we’ll have to look into more explicit electrode filtering and using Hilbert transformed data to try to improve our results.  <strong>Importantly</strong> I’d like to run future analyses this same way: modifying features, but running them through both classifiers to make sure we’re getting consistent results.</p>

<h2 id="to-do-item-5--feed-classifiers-face-and-scene-transformed-data-using-mcduff-importance-maps-with-dot-product-instead-of-correlation-as-pattern-similarity-measure">To-Do Item #5:  Feed classifiers face AND scene transformed data using McDuff Importance Maps with DOT PRODUCT instead of correlation as pattern similarity measure</h2>
<p><del><strong>IMPORTANT:</strong> For some reason, the boosting results look EXACTLY the same as the correlation-based boosting results.  This is strange because I explicitly verified the dot product and correlation calculations actually give different values. I’m investigating whether or not this is a bug as we speak, just gotta manually step through both the correlation and the dot product code which is a pain in the, as they say in the medical field, patootie.</del></p>

<p><strong>UPDATE:</strong> Comparing the preprocessed data using the dot product and correlation, it looks like the data being processed is in fact different for the boosting classifier i.e. I don’t think that this is a bug.  Looking into it more, given that the correlation is the normalized dot product, it makes sense that we get different results for logistic regression but not the boosting ensemble.  Why? Well, logistic regression regularization term penalizes the magnitude of the weights, thus keeping the same regularization penalty but scaling the weights is effectively the same as keeping the same weights but modifying the regularization term.  However, boosting just creates decision stumps which will split the data regardless of whether or not the data are normalized.  Thus these results make sense: all is right with the world.  Kind of.</p>

<p><a href="http://ElPiloto.github.io/images/research/8_wake_subj_202ms_7hz_gentle_faceSCENE_DOTPROD.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/8_wake_subj_202ms_7hz_gentle_faceSCENE_DOTPROD.jpg" width="700" height="350" /></a></p>

<p><a href="http://ElPiloto.github.io/images/research/log_reg_sleep_mcduff_best8_faceSCENEtogether_DOTPROD.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/log_reg_sleep_mcduff_best8_faceSCENEtogether_DOTPROD.jpg" width="700" height="350" /></a></p>

]]></content>
  </entry>
  
</feed>
