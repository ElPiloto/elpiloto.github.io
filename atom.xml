<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Something Witty]]></title>
  <link href="http://ElPiloto.github.io/atom.xml" rel="self"/>
  <link href="http://ElPiloto.github.io/"/>
  <updated>2014-08-10T16:53:45-04:00</updated>
  <id>http://ElPiloto.github.io/</id>
  <author>
    <name><![CDATA[Luis R. Piloto]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Relational RL]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/08/10/relational-rl/"/>
    <updated>2014-08-10T16:12:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/08/10/relational-rl</id>
    <content type="html"><![CDATA[<h2 id="motivation">Motivation</h2>

<p>Why do we even care about relational reinformcent learning?  </p>

<ol>
  <li>
    <p><em>Rule Learning by Seven-Month-Old Infants.</em> Marcus, Vijayan, Rao, &amp; Vishton, 1999: <strong>Methods:</strong>  <br />
7-month old infants were exposed to sentences of the form: ABB or ABA during a training phase (e.g. “ga ti ti” or “ga ti ga”), testing phase entailed consistent or inconsistent conditions where presented with entirely new words either in ABB or ABA format (e.g. “wo fe fe” or “wo fe wo”). <br />
<strong>Results:</strong> Infants looked longer at flashing light during inconsistent sentences in test period ( e.g. train “ABB”, test “ABA”) compared to consistent.<br />
<strong>Discussion:</strong> This indicates rule learning in infants, cannot be explained by statistical learning of transition probabilities because test set consisted entirely of new words (no estimate of transition probability).  Requires extraction of relationships: are these two entities the same?  </p>
  </li>
  <li>
    <p>Can generalize relationships over identities of objects in the world - variable abstraction.</p>
  </li>
  <li>
    <p>Introspection: our own capacities for abstraction and transfer suggest the importance of symbolic processing (Buchheit, 1999).</p>
  </li>
  <li>
    <p>Allows rich, intuitive specification of background knowledge: facilitating learning to new tasks (via bootstrapping from previously learned experiences): using a key on a lock is not dependent on the particular task at hand: if we learn through trial and error the abstract transition model: $ T(isLocked(lock) = true, unlocks(lock,key) = true, useKeyOnLock(lock,key)) \rightarrow  isLocked(lock) = false$</p>
  </li>
</ol>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG: Hilbert Transformed Data]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/08/06/sleep-eeg-hilbert-transformed-data/"/>
    <updated>2014-08-06T10:36:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/08/06/sleep-eeg-hilbert-transformed-data</id>
    <content type="html"><![CDATA[<h2 id="wake-hilbert-transformed-classification">Wake Hilbert Transformed Classification</h2>

<p><a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/wake_hilb_mean_across_subjects.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/wake_hilb_mean_across_subjects.png" width="700" height="350" /></a></p>

<p><strong>Figure 1:</strong> Average AUC on Hilbert-transformed wake data across all subjects for each time bin and frequency band combination.</p>

<p><strong>Thoughts:</strong> We see “hotspots” where we were hoping to see them: at the theta frequency around 200-230ms. Moreover, we see that the 200-230ms theta bin forms a local peak: as you move earlier or later in time, classification accuracy decreases.  Importantly, the accuracy using these bands (as opposed to individual frequencies like we had before) yields comparable average wake classification across subjects if we look at this <a href="http://ElPiloto.github.io/images/research/sleep_eeg_9_subjects_06_23_2014/wake_auc_avgd.png">old plot</a>.  If this result holds for all subjects, then we would feel pretty good about using the 200ms (or 230ms) theta time-freq bin to transform the sleep data.  Therefore, let’s look at the individual subjects to see if these results are driven by some subjects or if they seem to be pretty evenly distributed (spoiler: doesn’t seem to be a good time-freq bin, or even classifiable, for all subjects, but it does pretty well)</p>

<p><a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/wake_hilb_all_subjects_all_freqs_all_times.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/wake_hilb_all_subjects_all_freqs_all_times.png" width="700" height="350" /></a></p>

<p><strong>Figure 2:</strong> AUC heatmaps of classification AUC per subject.  Subject IDs are along the top of each subplot, timebins run along the y-axis and frequencies along the x-axis (only explicitly marked for the last subject, but the axes are exactly the same as Figure 1).</p>

<p><strong>Thoughts:</strong> These plots, though informative, suck for the purpose of evaluating how good the 200-250ms, theta bins do across all subjects.  Let’s zoom in on the areas of interest below.  One additional point, is that we’re getting <strong>much</strong> better max AUC across timebins than previously before (using individual frequencies) suggesting that the Hilbert transform is a better preprocessing technique for classification.</p>

<p><a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/wake_across_subjs_times7__8__9_freq2.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/wake_across_subjs_times7__8__9_freq2.png" width="700" height="350" /></a></p>

<p><strong>Figure 3:</strong>  Above we show the AUC for each subject for 3 different times (200, 225, and 250ms respectively) for the 4 Hz (theta) frequency band.</p>

<p><strong>Thoughts:</strong> This isn’t the greatest, especially considering some subjects have time-freq bins with much higher AUC.  “Irregardless,” these results are sufficiently positive to justify transforming sleep data.  Everything that we said about the non-band wake analyses holds here (e.g. we can look at other bins that look good for subjects, we can take each subject’s best bin to transform the data, etc).</p>

<h2 id="sleep-hilbert-untransformed-classification---sweep-across-time-and-frequency-combinations">Sleep Hilbert Untransformed Classification - Sweep Across Time And Frequency Combinations</h2>

<p>Below we look at how discriminable the sleep data is at each particular timebin and frequency combination of the sleep data using logistic regression.  Additionally, we test the regularization parameter for two separate values $ \lambda = 1, 50 $</p>

<p><a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/sleep_hilb_mean_across_subjects_lambda1.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/sleep_hilb_mean_across_subjects_lambda1.png" width="700" height="350" /></a></p>

<p><strong>Figure 4:</strong> Above we show the across-subjects average AUC for performing logistic regression on the sleep data at each time and frequency band combination with regularization parameter $ \lambda = 1 $.</p>

<p><a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/sleep_hilb_lambda50_mean_across_subjects.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/sleep_hilb_lambda50_mean_across_subjects.png" width="700" height="350" /></a></p>

<p><strong>Figure 5:</strong> Same as Figure 4, except this time we set the regularization parameter $ \lambda = 50 $.</p>

<p><strong>Thoughts:</strong> The band around 175ms, 16 Hz looks promising - we had mentioned that we’d be pretty happy if we could get something like 7-8 points above chance average classification accuracy across subjects.  This is much higher than the average classification accuracy ($ mean AUC = 0.52 $) we were getting for our “best 8” untransformed, logistic regression average AUC using all times and frequencies (recall that those results were not using frequency bands like we are now, but instead look at the power spectrum at individual frequencies).  Obviously, we can’t directly compare the individual time and frequency band results to using all time and frequency features, but it’s the closest benchmark we have.  Those old results can be found <a href="http://ElPiloto.github.io/blog/2014/07/27/sleep-eeg-post-boosting-results/"> here </a> under the section “To-Do Item #2”.</p>

<p>These results are encouraging for two reasons.  First, the best time and frequency combination falls within a plausible time range.  We could talk to James to see if he has any a priori evidence to support getting results in the BETA frequency range, but at least the time isn’t too early.  Second, the AUC is pretty good on the individual subject level as we see below: most subjects are either at or above chance.</p>

<p><a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/best_time_freq_untransformed_sleep_lambda1.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/best_time_freq_untransformed_sleep_lambda1.jpg" width="700" height="350" /></a></p>

<p><strong>Figure 6:</strong> Above we show the AUC for each subjects generated by performing logistic regression on the sleep data at the best time and frequency band combination with regularization parameter $ \lambda = 1 $.</p>

<p><a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/best_time_freq_untransformed_sleep_lambda50.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/best_time_freq_untransformed_sleep_lambda50.jpg" width="700" height="350" /></a></p>

<p><strong>Figure 7:</strong> Same as Figure 6, except this time we set the regularization parameter $ \lambda = 50 $.</p>

<p><strong>Thoughts:</strong> Okay, so we have <strong>a</strong> time and frequency combination that gives us good classification accuracy, but this could just arise from multiple comparisons.  Sure, we can take comfort that the classification accuracy seems to peak at 175ms and degrades smoothly as we move away, but it’s also the case that the EEG power spectrum is pretty similar at those times.  <strong>SO</strong>, what would it take for me to believe these results?  Running classification on <strong>ALL</strong> of the <em>untransformed</em> sleep data features with heavy regularization and yielding good classification accuracy with weights that consistently preferred that time and frequency combination.  This is next on my to-do list, it shouldn’t take too long to get up and running.  I’d also like to add a reminder to myself that it may be worth subsampling the time dimension.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Probabilistic Logic Learning - Luc De Raedt and Kristian Kersting]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/08/05/probabilistic-logic-learning-luc-de-raedt-and-kristian-kersting/"/>
    <updated>2014-08-05T12:51:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/08/05/probabilistic-logic-learning-luc-de-raedt-and-kristian-kersting</id>
    <content type="html"><![CDATA[<h4 id="what-does-probabilistic-logic-learning-mean">What does probabilistic logic learning mean?</h4>
<p><strong>probabilistic</strong> - probabilistic representations and reasoning mechanisms <br />
<strong>logic</strong> - first order logical and relational representations (compared to propositional logic affords reasoning about objects)</p>

<h3 id="logic-glossary">Logic Glossary</h3>
<p>first-order logic $\leftrightarrow$ relational representation <br />
allows for:<br />
<strong>extensional</strong>: <code>ram(r1,c1)</code> – </p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG: Varying regularization and simulated results]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/08/01/sleep-eeg-varying-regularization-and-simulated-results/"/>
    <updated>2014-08-01T10:27:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/08/01/sleep-eeg-varying-regularization-and-simulated-results</id>
    <content type="html"><![CDATA[<h3 id="logistic-regression-untransformed-sleep-data-varying-regularization-term">Logistic Regression Untransformed Sleep Data: Varying regularization term</h3>
<p>The motivation for this line of inquiry is contained in <a href="http://ElPiloto.github.io/blog/2014/07/27/sleep-eeg-post-boosting-results/">this post</a> under the discussion section for figure 4.</p>

<p><a href="http://ElPiloto.github.io/images/research/untransformed_log_reg_lambda250and750.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/untransformed_log_reg_lambda250and750.jpg" width="700" height="350" /></a></p>

<p><strong>Figure 1:</strong> This shows the sleep classification accuracy for logistic regression using untransformed sleep data with <code>statmap_anova</code> feature selection (stat_thresh = 0.1).</p>

<p><strong>Thoughts</strong> Well this is interesting.  I had previously been using a regularization term of 1 for all the logistic regression analyses and to see this level of classification accuracy using a regularization penalty two orders of magnitude higher than previously suggests that I didn’t have much of a grasp on the role of the regularization term in this analysis.  The $\lambda = 750$ case is encouraging because it’s up there in terms of the highest average classification accuracy we’ve seen AND it includes that largest regularization term we’ve used thus far.  It’s worth investigating how far we can push regularization before it collapses, thus I’ve launched an additional analysis trying results with regularization = 1250 and 1750.  Will update this post when those results are in.  ALSO, this begs the question: should we re-run our sleep transformed analyses using stronger regularization for the wake classes?  <strong>YES</strong></p>

<p><a href="http://ElPiloto.github.io/images/research/untransformed_log_reg_lambda1250and1750.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/untransformed_log_reg_lambda1250and1750.jpg" width="700" height="350" /></a></p>

<p><strong>Figure 2:</strong> This shows the sleep classification accuracy for logistic regression using untransformed sleep data with <code>statmap_anova</code> feature selection (stat_thresh = 0.1).</p>

<p><strong>Thoughts:</strong> This still doesn’t make it seem like we’ve hit the upper bound on the regularization penalty. I’m not sure what to make of the fact that 750 = good regularzation, 1750 = good regularization, but 1250 gives pretty crappy results.  I guess I don’t have any strong theoretical justification to think classification accuracy would vary smoothly as we modify the regularization parameter.  I’m now launching sleep untransformed classification on <em>all</em> the subjects with a high regularization term.</p>

<p><a href="http://ElPiloto.github.io/images/research/untransformed_log_reg_lambda12000_ALLSUBJS.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/untransformed_log_reg_lambda12000_ALLSUBJS.jpg" width="700" height="350" /></a></p>

<p><strong>Figure 3:</strong> This shows the sleep classification accuracy for logistic regression using untransformed sleep data with <code>statmap_anova</code> feature selection (stat_thresh = 0.1).</p>

<p><strong>Thoughts:</strong> This is clearly too high for most subjects. Although <strong>some</strong> of the subjects that have been doing well in our “best 8” analysis get pretty good classification with $\lambda = 2000$, which is unsurprising given their results for $\lambda = 1750$, some other subjects (e.g. subject 7) that did well are now getting below-chance accuracy.  The flop of some of the good “best 8” subjects from above to below chance accuracy makes a good case for reducing the regularization penalty.  Thus, I’m gunna try running all subjects with a smaller value for $\lambda$ because the infrastructure is in place to do that.</p>

<h3 id="simulated-results">Simulated Results</h3>

<p><del><strong>STILL CODING</strong></del> <strong>PUT ON HOLD, GOING TO TRY HILBERT TRANSFORMED DATA ANALYSIS FIRST</strong></p>

<!--Ken is most interested in the hilbert-transformed data in the hopes that the Hilbert stuff will clean up (induce consistency across subjects) for wake.  Hopefully see good classification across subjects theta, 200-ish, also which electrodes are informative.-->
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG: Group Meeting]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/07/28/sleep-eeg-group-meeting/"/>
    <updated>2014-07-28T17:10:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/07/28/sleep-eeg-group-meeting</id>
    <content type="html"><![CDATA[<h3 id="ken-succinctly-described-two-levels-of-complexity-in-our-sleep-analysis">Ken succinctly described two levels of complexity in our sleep analysis:</h3>
<ol>
  <li>How do we transform the sleep?  Recall this can be face or scene transformed, we can use correlation or the dot product, we can use data from different frequencies and different time bins in the wake data, etc.<br />
    <ul>
      <li>Perhaps we don’t need to sleep transform?</li>
      <li>Perhaps we need to exclude electrodes explicitly? We thought the McDuff importance maps method implemented this, but correlation doesn’t care about the magnitude so we need to re-evaluate this (either use dot product OR filter out electrodes by some other measure)  <strong>(TODO ITEM: dot product)</strong></li>
    </ul>
  </li>
  <li>When does reinstatement happen?  How should we score reinstatement?  If we do classification, do we just look at one time bin for the sleep?  Do we sum up classification across all time bins, etc.<br />
    <ul>
      <li>Maybe we want to use a simpler method than classification a la Staresina paper: sum correlation between template and sleep pattern and threshold to indicate replay event.   </li>
      <li>We could and should look at trial-by-trial plots of correlation across time for both the incorrect and correct pattern - eyeball the crap out of this.  <strong>(TODO ITEM)</strong></li>
    </ul>
  </li>
  <li>James also mentioned trial-by-trial variability: if cue during down phase, won’t expect reactivation in next 500 ms when all neurons are turned off.</li>
</ol>

<h3 id="behavioral-results-look-great">Behavioral Results Look great</h3>
<ol>
  <li>Spindles too rare to use to limit the sleep data   </li>
</ol>

<h3 id="additional-thoughts">Additional thoughts:</h3>
<ol>
  <li>We should leave open the option to look at ERPs for classification, although Ehren’s thoughts were that ERP winds up showing in theta band of power spectrum, plus see things in power spectrum that you don’t see in the ERPs.</li>
  <li>We can theoretically get more classification juice if we exclude forgotten items from the sleep analysis.  </li>
  <li>Waiting on James to get power spectrum for bands using Hilbert transform  </li>
  <li>We have the data to look at wake classification during wake reactivation i.e. can we classify when they’re learning the associated locations?   </li>
</ol>

<h3 id="todo-items">TODO ITEMS:</h3>
<p><strong>dot product:</strong><br />
<del>currently running sleep log reg mcduff using both faces and scenes, results will be in <code>sleep_xform_mcduff_scene_face_8subjs_dotprod.mat</code></del>  Results in post: sleep-eeg-boosting-results
<del>currently running boosting gentle log reg using both faces and scenes, results will be in <code>boosting_mcduff_best8_gentle_scene_face_dotprod.mat</code></del>Results in post: sleep-eeg-boosting-results, <strong>EXCEPT</strong> for some reason</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG: Post Boosting Results]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/07/27/sleep-eeg-post-boosting-results/"/>
    <updated>2014-07-27T14:46:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/07/27/sleep-eeg-post-boosting-results</id>
    <content type="html"><![CDATA[<h3 id="our-to-do-list-consists-of-the-following">Our to-do list consists of the following:</h3>
<ol>
  <li>logistic regression on sleep data using data from all time bins of the sleep data and compare this to existing results from boosting  </li>
  <li>try boosting and logistic regression using <strong>untransformed</strong> sleep data (large volume of data)</li>
  <li>look at various other parameterizations of boosting classification result (just because it’s easy to do)</li>
  <li>Let’s look at what happens when the classifier gets fed in both face and scene transformed data   </li>
  <li>Let’s look at when the classifier gets transformed using both face and scene transformed data using the DOT product (see post )  </li>
</ol>

<h3 id="as-a-refresher-here-is-the-last-boosting-result">As a refresher, here is the last boosting result:</h3>

<p><a href="http://ElPiloto.github.io/images/research/ensemble_best_8_wake_subj_202ms_7hz_gentle.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/ensemble_best_8_wake_subj_202ms_7hz_gentle.jpg" width="700" height="350" /></a></p>

<p><strong>Figure 1:</strong> The y-axis shows classification accuracy.  Subjects and parameters are shown along the x-axis (g = gentleboost algorithm, f = face mcduff importance sleep transform).</p>

<h2 id="logistic-regression-sleep-importance-map-transformed-results-to-do-item-1">Logistic Regression Sleep Importance Map Transformed Results (To-Do Item #1)</h2>
<p><a href="http://ElPiloto.github.io/images/research/log_reg_best_8_subjects_mcduff_xform.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/log_reg_best_8_subjects_mcduff_xform.jpg" width="700" height="350" /></a></p>

<p><strong>Figure 2a:</strong> Displayed above is the classifier accuracy for all subjects(x-axis).  The classification accuracy was generated for various sleep importance map transformations (face, scene, or face minus scene), which is indicated along the y-axis.  The classifier features consisted of transforming the sleep data at each timebin and concatenating the sleep transformed data at each timebin into a single data matrix.  The entries along the x and y axis labelled “Mean” simply show the mean along the x and y axis respectively.</p>

<p><strong>Thoughts</strong>: These results lend further credence to the notion that it may be beneficial to sleep transform data according to both face and scene classifiers and feed both of those into a single classifier.  This idea that perhaps there is information in the scene pattern that isn’t in the face pattern (and vice versa) can be further scrutinized by looking at MDS plots of the untransformed data, the face transformed data, and scene transformed data - this may be worth doing depending on how much free time I have.  If I don’t end up looking at that, I’m not too upset since item #2 on the to-do list will also give us information about the role of the sleep transformation.  Additionally: note that the average classification accuracy, across both subjects and sleep transformation types, is pretty similar to that achieved with boosting, but none of the individual sleep transformations really give the same classification accuracy profile across subjects as the boosting results (this is easier to see in the plot below where I plot the rows of Figure 2 as separate horizontal bar graphs to match the format of the boosting results.  Why does it matter that accuracy across subjects in the logistic regression case doesn’t look like the accuracy achieved with boosting?  If our sleep transformation method yielded enough signal, then it should yield similar classification accuracy across subjects for the different classifier types.</p>

<p><a href="http://ElPiloto.github.io/images/research/log_reg_best_8_subjects_mcduff_xform_supplement.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/log_reg_best_8_subjects_mcduff_xform_supplement.jpg" width="700" height="350" /></a>
<strong>Figure 2b:</strong> A different visualization of the logistic regression plots useful for direct visual comparison against the ensemble results (Figure 1).</p>

<h2 id="to-do-item-3-boosting-classification-results-using-different-sleep-transforms-best-8-subjects">To-Do Item #3: Boosting Classification Results Using Different Sleep Transforms, Best 8 Subjects</h2>
<p><a href="http://ElPiloto.github.io/images/research/ensemble_best_8_wake_subj_202ms_7hz_gentle_ALL_TRANSFORMS.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/ensemble_best_8_wake_subj_202ms_7hz_gentle_ALL_TRANSFORMS.jpg" width="700" height="350" /></a></p>

<p><strong>Figure 3:</strong> This shows boosting classification accuracy using different sleep transforms.  The x-axis indicates which boosting algorithm (g for gentleboost) was used, the subject id (e.g. ‘15’ or ‘07’), and the wake pattern that was used for the sleep transformation (f = face, s = scene, M = face minus scene).</p>

<p><strong>Thoughts:</strong> This is a clear example where the classification accuracy results would look totally different based on which analysis parameters we use.  I suspect that if we get around to doing boosting classification using both the scene and face transformation, that will look entirely different, too.</p>

<h2 id="to-do-item-2-logistic-regression-classification-results-sleep---untransformed-best-8-subjects">To-Do Item #2: Logistic Regression Classification Results Sleep - UNTRANSFORMED, Best 8 Subjects</h2>
<p><del><strong>3/23 JOBS STILL RUNNING ON CLUSTER</strong>  This takes a relatively long time because the feature selection has to operate over ~100,000 features and that needs to run for each cross-validation fold for each subject.</del>  The results are displayed below</p>

<p><a href="http://ElPiloto.github.io/images/research/sleep_log_reg_raw_best8.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_log_reg_raw_best8.jpg" width="700" height="350" /></a></p>

<p><strong>Figure 4:</strong> This shows the result of performing logistic regression on the raw sleep data using all frequencies, all time bins, and all electrodes on raw data.  This amounts to about 100,00 features (64 electrodes x ~47 frequencies x ~38 time bins ) so obviously I need to run this analysis with feature selection.  I really wanted to include as many features as I could (without exceeding my RAM limits), so I used a loose stat threshold for statmap_anova feature selection (p = 0.1) which yielded about 10,000 features.</p>

<p><strong>Thoughts:</strong> This tells us that this data is damn noisy, we need to be smarter about what features we’re putting into the data.  Straightforward logistic regression isn’t working particularly well on this - it might be worth explicitly using LASSO instead of ridge regression as we’re currently using.  Additionally, I found something in the documentation of the <code>logRegFun</code> that suggests I should be trying much higher regularization values than I have been.  I’m going to try this out since it’s easy to run and I want to explicitly rule out the penalty term as why we’re not getting poor results. I will also look into whether or not it’s easy to use LASSO.  <strong>Update</strong> There does seem to be L1 regression in the MVPA toolbox via the <code>SMLR</code> function, although it implements multinomial regression (as compared to the current method of training a single logistic regression classifier for each class), however, it seems like it’ll bw</p>

<p>I think the fact that these results are comparable to the sleep transformed data in terms of classification accuracy tells us something about the sleep data, not necessarily about the sleep transformation.  I have to double-check the linear algebra behind this interpretation of the sleep data, but I think that when we transform the sleep data, we’re essentially projecting each dimension of the sleep data onto a single vector defined by the average pattern.  Now: if there is some best subspace of the untransformed data where the classes are just barely separable (i.e. you could get 52% accuracy), then whether or not we transform the sleep data won’t really matter.  If we don’t project, we can build a classifier that goes along the subspace I mentioned.  If we do project, then that same subspace will manifest in a different basis, but with the same separability.  Obviously you won’t get this barely classifiable subspace if the vector onto which you project is orthogonal to the subspace, but if there are multiple, weakly classifiable subspaces in the data - which is not crazy to think if the data are in general weakly separable, then one of these subspaces will not be orthogonal to the vector you’re choosing yielding once again a weakly separable subspace.  In fact, I suspect that we should be able to project onto random vectors and get the same classification accuracy on average - this comes from some light reading into random subspace projection theory where given a random projection you can give a probabilistic bound on the dot product between two vectors in a random subspace as a function of their dot product in the original space i.e. with some probability the dot product in the transformed space will be within some range of the dot product in the original space.  This is all pretty loosey goosey linear algebra logic, but I think it’s a start in the right direction for understanding what’s up with the transform vs untransformed data.</p>

<h2 id="to-do-item-4--feed-classifiers-face-and-scene-transformed-data-using-mcduff-importance-maps">To-Do Item #4:  Feed classifiers face AND scene transformed data using McDuff Importance Maps</h2>

<p><a href="http://ElPiloto.github.io/images/research/ensemble_best_8_wake_subj_202ms_7hz_gentle_face_AND_SCENE.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/ensemble_best_8_wake_subj_202ms_7hz_gentle_face_AND_SCENE.jpg" width="700" height="350" /></a></p>

<p><strong>Figure 5a:</strong> Boosting classification exactly like the Figure 1) except that these incorporate a feature vector that consists of transforming all frequencies with the importance map for scene and concatenating those features onto the feature vector generated by transforming all frequencies in the sleep data with the importance map for faces.</p>

<p><a href="http://ElPiloto.github.io/images/research/log_reg_sleep_mcduff_best8_faceSCENEtogether.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/log_reg_sleep_mcduff_best8_faceSCENEtogether.jpg" width="700" height="350" /></a></p>

<p><strong>Figure 5b:</strong> Exactly the same as figure 2a, recall we’re feeding in all the timepoints to logistic regression (no explicit feature selection in this version, although we still use regularization), except this time we used both face and scene transformed features in the data.</p>

<p><strong>Thoughts:</strong> Well, not uniformly super awesome classification results for both methods, BUT I’m pretty happy to see that they have the same general profile: across both analyses subjects 11,13,15 and 5,7,8 both pop out whereas classification is poor for both subject 9 and subject 1 in both analyses.  Consistency across analyses is the only proxy we currently have for assessing whether our results are due to random chance so this is a relatively nice result in that context.  That’s a score for the veracity of our results, but the other key aspect of evaluating our results, how good the results actually are, is still lacking.  Barring success with modifying the regularization term, we’ll have to look into more explicit electrode filtering and using Hilbert transformed data to try to improve our results.  <strong>Importantly</strong> I’d like to run future analyses this same way: modifying features, but running them through both classifiers to make sure we’re getting consistent results.</p>

<h2 id="to-do-item-5--feed-classifiers-face-and-scene-transformed-data-using-mcduff-importance-maps-with-dot-product-instead-of-correlation-as-pattern-similarity-measure">To-Do Item #5:  Feed classifiers face AND scene transformed data using McDuff Importance Maps with DOT PRODUCT instead of correlation as pattern similarity measure</h2>
<p><del><strong>IMPORTANT:</strong> For some reason, the boosting results look EXACTLY the same as the correlation-based boosting results.  This is strange because I explicitly verified the dot product and correlation calculations actually give different values. I’m investigating whether or not this is a bug as we speak, just gotta manually step through both the correlation and the dot product code which is a pain in the, as they say in the medical field, patootie.</del></p>

<p><strong>UPDATE:</strong> Comparing the preprocessed data using the dot product and correlation, it looks like the data being processed is in fact different for the boosting classifier i.e. I don’t think that this is a bug.  Looking into it more, given that the correlation is the normalized dot product, it makes sense that we get different results for logistic regression but not the boosting ensemble.  Why? Well, logistic regression regularization term penalizes the magnitude of the weights, thus keeping the same regularization penalty but scaling the weights is effectively the same as keeping the same weights but modifying the regularization term.  However, boosting just creates decision stumps which will split the data regardless of whether or not the data are normalized.  Thus these results make sense: all is right with the world.  Kind of.</p>

<p><a href="http://ElPiloto.github.io/images/research/8_wake_subj_202ms_7hz_gentle_faceSCENE_DOTPROD.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/8_wake_subj_202ms_7hz_gentle_faceSCENE_DOTPROD.jpg" width="700" height="350" /></a></p>

<p><a href="http://ElPiloto.github.io/images/research/log_reg_sleep_mcduff_best8_faceSCENEtogether_DOTPROD.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/log_reg_sleep_mcduff_best8_faceSCENEtogether_DOTPROD.jpg" width="700" height="350" /></a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG: Boosting Classification Results]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/07/15/sleep-eeg-boosting-classification-results/"/>
    <updated>2014-07-15T09:37:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/07/15/sleep-eeg-boosting-classification-results</id>
    <content type="html"><![CDATA[<h2 id="the-results-below-ensemble-classification-results-round-1-are-all-useless">The results below “Ensemble Classification Results, Round 1” are all useless</h2>

<p>I messed up the calculation of average pattern in wake classification (which is used in creating the mcduff importance map in the preprocessing below).</p>

<h3 id="ensemble-classification-results-round-6---cross-validation-subject-1">Ensemble Classification Results, Round 6 - Cross-validation subject 1</h3>

<p><a href="http://ElPiloto.github.io/images/research/ensemble_results_subj1_round6.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/ensemble_results_subj1_round6.jpg" width="700" height="350" /></a></p>

<p><a href="http://ElPiloto.github.io/images/research/ensemble_results_subj1_round6pt2.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/ensemble_results_subj1_round6pt2.jpg" width="700" height="350" /></a></p>

<p><a href="http://ElPiloto.github.io/images/research/ensemble_results_subj1_round6pt3.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/ensemble_results_subj1_round6pt3.jpg" width="700" height="350" /></a></p>

<h3 id="ensemble-classification-results-round-3">Ensemble Classification Results, Round 3</h3>

<p>I used the following values to produce the plot below:
10-fold cross validation, GentleBoost, 200 Learners, Wake Time = 152ms, Freq = 7 Hz, Importance Map for Face</p>

<p><a href="http://ElPiloto.github.io/images/research/ensemble_results_subj1thru8_round3.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/ensemble_results_subj1thru8_round3.jpg" width="700" height="350" /></a></p>

<h3 id="ensemble-classification-results-round-1-incorrect">Ensemble Classification Results, Round 1 (INCORRECT)</h3>
<p><strong>these results are wrong - leaving here for historical purposes only</strong></p>

<p>Previously, I went through and tried various combinations of boosting algorithms (LogitBoost, GentleBoost, Adaboost.M1), number of learners, and number of folds for a single subject (subject 01) to try to determine what to run across all subjects to get the best tradeoff between accuracy and running time.  Below is a summary of those results.</p>

<p><a href="http://ElPiloto.github.io/images/research/subj1_ensemble_params_testing.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/subj1_ensemble_params_testing.png" width="700" height="300" /></a></p>

<p>Ultimately, I chose the following: <br />
  <strong>GentleBoost:</strong> just as good accuracy as other algorithms <em>and</em> better running time than AdaBoost.M1 or LogitBoost for some parametrizations   <br />
  <strong>50-fold cross-validation:</strong> it seemed to me like accuracy would increase with the number of folds   <br />
  <strong>400 Learners:</strong> I was skeptical of AdaBoostM1’s results with 700 learners, that begin to overfit, so I aimed for something slightly higher than what I had tried (300 learners) that wouldn’t go too far in terms of overfitting.</p>

<p>Below are the results for running <strong>GentleBoost</strong> using the aforementioned parameters, transforming the sleep data with a face minus scene McDuff importance map using wake data from time-bin = 230ms and freq = 11 Hz. <br />
j  </p>

<p>This is below what we’d like to get and is lower than I would have expected for the first subject.  Possible reasons this could be the case:<br />
  - not enough data per fold<br />
  - I (accidentally )probed boosting results on subject 1 with different parameters than used for the current results: <br />
     - time bin = 152 ms<br />
     - freq = 7 Hz <br />
     - mcduff pattern = face   </p>

<p><a href="http://ElPiloto.github.io/images/research/ensemble_results_subj1thru8_round1.jpg" target="_blank"><img src="http://ElPiloto.github.io/images/research/ensemble_results_subj1thru8_round1.jpg" width="700" height="350" /></a></p>

<h3 id="next-steps">Next Steps</h3>

<p>The easiest problem to check for is to see if we get better results with more data in each fold, so I’ve launched a batch of results that use 20 folds.  This should give new results relatively quickly and we can move from there.  If that doesn’t ameliorate the poor accuracy, I’ll move to using the wake parameters used to probe subject 1 to make sure that the results for subject 1 are consistent with the results I got during the probing phase.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Towers of Hanoi SFA and 8-ball SFA results: Robustness and Potential Neural Data to Model]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/07/06/towers-of-hanoi-sfa-and-8-ball-sfa-results-robustness-and-potential-neural-data-to-model/"/>
    <updated>2014-07-06T15:29:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/07/06/towers-of-hanoi-sfa-and-8-ball-sfa-results-robustness-and-potential-neural-data-to-model</id>
    <content type="html"><![CDATA[<h4 id="towers-of-hanoi-to-do">Towers of Hanoi To-Do:</h4>
<ol>
  <li><strong>Run SFA on random walk using multiple different random number seeds</strong> </li>
  <li>I am using the same starting location, this might affect the solutions found by SFA.  However, looking at the average input values (<code>SFA_STRUCTS.avg0</code>) shows that each feature has an average value of approximately 0.33, indicating that we do traverse all configurations equally frequently.</li>
  <li><code>sfa_tk</code> toolbox does not give features with unit variance when performing linear SFA. This might be a problem in the general case, but shouldn’t be a problem for towers of hanoi because the point of getting unit variance is so that the weights you find that minimize the derivative are directly comparable to each other.  But in our case, the derivatives should already be directly comparable because the input features all have exactly the same distribution for a random walk on towers of hanoi. I’m not entirely sure about this last bit, so I’ll have to verify this if Ari doesn’t have any useful input on the matter - easy way forward: look at MDP (python SFA implementation).</li>
</ol>

<h4 id="general-tips-for-sfatk-toolbox">General Tips for sfa_tk toolbox</h4>
<ol>
  <li><code>SFA_STRUCTS.SF</code> - contains slow feature functions along the rows</li>
  <li>Can get degenerate eigendecompositions, where an eigenvalue has greater algebraic multiplicity than geometric multiplicity (e.g. $ (2 - \lambda)^2(3-\lambda)^2 $ – alg. mult = 2, geometric = 1 for both eigenvalues).  In this case, extracted slow features are redundant (though they’ll have opposite signs).</li>
</ol>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SFA Nonmarkov: Modify network, maze size and type, visualize gates]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/06/24/sfa-nonmarkov-modify-network/"/>
    <updated>2014-06-24T13:38:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/06/24/sfa-nonmarkov-modify-network</id>
    <content type="html"><![CDATA[<h3 id="overview-of-simulations">Overview of simulations</h3>
<ol>
  <li>Look at the gate values along the maze trajectories (previously we only looked at the hidden state values). Recall that a long-short term memory network has three gates: read, write, and forget   </li>
  <li>We also need to look at our old MDS plots of the hidden layers and label the trajectory (previously we only showed whether a representation belonged to the center hallway or right or left outer maze locations)</li>
  <li>We want to see if we get any qualitative differences as we change the width of the maze.  </li>
  <li>We want to see if changing the maze type to be a ring will still yield the segmenting behavior we see from SFA  </li>
  <li>We want to see if we get any qualitative differences as we change the number of hidden units</li>
</ol>

<h3 id="different-num-hidden-units">Different Num Hidden Units</h3>

<h4 id="hidden-unit-mds-plots">Hidden Unit MDS Plots</h4>
<p><a href="http://ElPiloto.github.io/images/research/sfa_lstm/hidden_layer_width3_hidden5_loop.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sfa_lstm/hidden_layer_width3_hidden5_loop.png" width="750" height="350" /></a> 
<a href="http://ElPiloto.github.io/images/research/sfa_lstm/hidden_layer_width3_hidden3_loop.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sfa_lstm/hidden_layer_width3_hidden3_loop.png" width="750" height="350" /></a> </p>

<h5 id="observations">Observations:</h5>
<ol>
  <li>Much cleaner separation of states with 5 hidden units compared to 3 hidden units based on the loop to which they belong.</li>
  <li>Also get cleaner separation of states based on center versus non-center.</li>
  <li><strong>These results suggest that the hidden layer representation generated using 5 hidden units might be a better representation for looking at the benefits of SFA.</strong></li>
</ol>

<h4 id="hidden-unit-similarity-plots-according-to-location-on-maze">Hidden Unit Similarity Plots According to Location on Maze</h4>
<p><a href="http://ElPiloto.github.io/images/research/sfa_lstm/Linear_corr_states_hidden_representation_width3_hidden5.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sfa_lstm/Linear_corr_states_hidden_representation_width3_hidden5.png" width="750" height="350" /></a> 
<strong>Figure 2a) 5 Hidden Units</strong>
<a href="http://ElPiloto.github.io/images/research/sfa_lstm/Linear_corr_states_hidden_representation_width3_hidden3.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sfa_lstm/Linear_corr_states_hidden_representation_width3_hidden3.png" width="750" height="350" /></a> 
<strong>Figure 2b) 3 Hidden Units</strong>  </p>

<h5 id="observations-1">Observations:</h5>
<p>This is just another way of seeing how much cleaner the 5 hidden units representation is.</p>

<h4 id="linear-sfa-top-2-slowest-features">Linear SFA Top 2 Slowest Features</h4>
<p><a href="http://ElPiloto.github.io/images/research/sfa_lstm/sfa_1st_2_features_rm_1st_visit_width3_hidden5.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sfa_lstm/sfa_1st_2_features_rm_1st_visit_width3_hidden5.png" width="750" height="350" /></a> 
<strong>Figure 3a) 5 Hidden Units</strong>
<a href="http://ElPiloto.github.io/images/research/sfa_lstm/sfa_1st_2_features_rm_1st_visit_width3_hidden3.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sfa_lstm/sfa_1st_2_features_rm_1st_visit_width3_hidden3.png" width="750" height="350" /></a> 
<strong>Figure 3b) 3 Hidden Units</strong>   </p>

<h5 id="observations-2">Observations:</h5>
<ol>
  <li>Here we see exactly what we hoped for in the 5 hidden units case: the first slow-feature indicates which loop we’re on!</li>
  <li>The question is: how much of this is the neural network and how much of this is SFA? Well let’s look at the plots below of the raw hidden layer values across the maze.  If the neural network is doing all the heavy lifting, then we would see a single, slowly varying feature that encodes the loops direction but this doesn’t seem to be the case!</li>
</ol>

<p><a href="http://ElPiloto.github.io/images/research/sfa_lstm/hidden_layer_on_maze_raw_left_width3_hidden5.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sfa_lstm/hidden_layer_on_maze_raw_left_width3_hidden5.png" width="750" height="350" /></a> 
<strong>Figure 4a) 5 Hidden Units</strong>
<a href="http://ElPiloto.github.io/images/research/sfa_lstm/hidden_layer_on_maze_raw_right_width3_hidden5.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sfa_lstm/hidden_layer_on_maze_raw_right_width3_hidden5.png" width="750" height="350" /></a> 
<strong>Figure 4b) 5 Hidden Units</strong>   </p>

<p><strong>NOTE:</strong> These colors look exactly the same along the central corridor (which would be troubling), but their values are actually different if you look at the numbers.</p>

<h3 id="different-maze-size----width-7-5-hidden-units">Different Maze Size  - Width 7, 5 Hidden Units</h3>

<h4 id="hidden-unit-mds-plots-1">Hidden Unit MDS Plots</h4>
<p><a href="http://ElPiloto.github.io/images/research/sfa_lstm/hidden_layer_mds_width7_hidden5.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sfa_lstm/hidden_layer_mds_width7_hidden5.png" width="750" height="350" /></a> </p>

<h4 id="linear-sfa-top-2-slowest-features-1">Linear SFA Top 2 Slowest Features</h4>

<p><a href="http://ElPiloto.github.io/images/research/sfa_lstm/sfa_linear_1st2features_width7_hidden5.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sfa_lstm/sfa_linear_1st2features_width7_hidden5.png" width="750" height="350" /></a> </p>

<h3 id="different-maze-structure">Different Maze Structure</h3>

<h4 id="hidden-unit-mds-plots-2">Hidden Unit MDS Plots</h4>
<p><a href="http://ElPiloto.github.io/images/research/sfa_lstm/hidden_layer_mds_width7_hidden5.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sfa_lstm/hidden_layer_mds_width7_hidden5.png" width="750" height="350" /></a> </p>

<h4 id="linear-sfa-top-2-slowest-features-2">Linear SFA Top 2 Slowest Features</h4>
<p><a href="http://ElPiloto.github.io/images/research/sfa_lstm/sfa_1wayloop_width7_hidden5.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sfa_lstm/sfa_1wayloop_width7_hidden5.png" width="750" height="350" /></a> </p>

<h3 id="meeting-with-mattbot">Meeting with Mattbot:</h3>

<h4 id="observations-3">Observations:</h4>
<ol>
  <li>It does seem like the first slow feature encodes which outer loop we’re on.   </li>
  <li>Cooler than that, our second slow feature for the width 7, 5 hidden units case, encodes which step of the outer loop we’re on, regardless of which direction the loop is.  This is motherfucking abstraction! We’re going to try to connect this to to-do item #4.</li>
</ol>

<h4 id="to-do">To-Do:</h4>
<ol>
  <li>Why are the slow-features on such a small scale?    </li>
  <li>Why are the hidden layer representations (think MDS width3, hidden5) so similar at the fork point - Matt thinks I might be off by one because the point just after the fork diverges which is what you would expect from the fork point - since they make vastly different predictions.  </li>
  <li>Will PCA give us anything useful?    </li>
  <li>I should read a paper on rat neurophysiology, an older paper - they showed that in an 8-arm radial maze, certain PFC cells encoded specific parts of an arm, without being specific to the particular arm.  This is similar to our second slow feature in the </li>
  <li>Check that these results are invariant o the number of hidden units and size of the maze.</li>
</ol>

<h4 id="other-to-do">Other to-do:</h4>
<p>This isn’t related to the non-markov, but to the factored state towers of hanoi: I was tasked with performing slow feature analysis on a random walk on a factored state representation of the towers of hanoi task.  It seems like I’m getting a result that Ari wasn’t able to: The first slow feature seems to represent the location of the largest ring.
* I should check with Ari to make sure I did things the way he did, too <br />
* I should show Matt the MDS of the slow features <br />
* I should make sure all of theslow features load on all dimensions of the original input space.<br />
* We’re not sure what the second slowest feature should look like, but it seems like it might be the configuration of the second largest ring conditioned on the location of the largest ring.  That’s definitely what the scatter plot of the first two features looks like.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG 8 Subjects]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/06/23/sleep-eeg-9-subjects/"/>
    <updated>2014-06-23T12:14:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/06/23/sleep-eeg-9-subjects</id>
    <content type="html"><![CDATA[<h2 id="all-the-interpretations">All the interpretations</h2>

<p>Hi Ken,  </p>

<p>Here’s the scoop.  We can definitely classify wake data (see Figure 1a below) The best time bin for the wake data isn’t consistent across all subjects, but it does seem to peak at 230ms, which is consistent with Ehren’s results ( compare figure 1b to figure 4: http://compmem.princeton.edu/publications/NewmanNorman10.pdf)  Our classification results aren’t as good as Ehren’s, he had a peak averaged AUC of 0.7, but I’m not too worried about it - what do you think?  There might be some utility to looking at classification for slightly later timebins (although Ehren’s results do peak at 200ms so we might already be looking at the best time bin)   </p>

<p><strong>Classifying sleep?</strong> 	</p>

<p>We tried three different sleep classification methods:<br />
1. Feature selection on all the frequencies within a particular time bin of sleep data.
2. Transform the sleep data using the average pattern of activity across all electrodes at a particular time-bin and frequency of the wake data.
3. Transform the sleep data using the importance map generated by the wake classification instead of the average pattern of activity - this should weight the different electrodes according to how informative they are for wake classification.</p>

<p>For method 1, we’re not getting any consistently classifiable timebins across subjects (figure 3a and 3b) which would have been the nicest result.  Additionally, having good wake AUC doesn’t mean that subject will have good sleep AUC.</p>

<p>The good news is that methods 2 and 3 show comparable classification AUC to method 1 and sometimes do even better.  Moreover, there isn’t a link between good classification for a time bin using method 1 compared to method 2 or 3 (e.g. method 1 classification for subject 2 doesn’t do so hot for timebins 152-230, but if you look at <a href="http://ElPiloto.github.io/images/research/sleep_eeg_9_subjects_06_23_2014/sleep_xform_avg_pattern/sleep_transform_cv_auc_scene_subj2.png"> this plot</a> for method 2 classification for subject 2 - it’s pretty clear that that is a helpful time bin ).  These two observations give me confidence that the two sleep transform methods are genuinely helpful preprocessing steps.   </p>

<p>Moreover, I’ve been pretty excited about these sleep transform methods because they frequently, though not always, produce strong bands of good classification for particular timebins of the sleep data.</p>

<p>So a logical next question is to determine which method of the two sleep transform methods are better.  It seems like we get stronger bands appearing with method 2 compared to method 3 (you can check this out for yourself just quickly scrolling through the dump of the plots for both <a href="http://ElPiloto.github.io/blog/2014/07/01/sleep-eeg-transform-avg-pattern/">method 2</a> and <a href="http://ElPiloto.github.io/blog/2014/07/01/sleep-eeg-mcduff-transform/">method 3</a>).  There is some agreement between these bands across the two methods, but not always (example plots: <a href="http://ElPiloto.github.io/images/research/sleep_eeg_9_subjects_06_23_2014/sleep_xform_avg_pattern/sleep_transform_cv_auc_pval_face&amp;scene_subj4.png"> method 2 </a> <a href="http://ElPiloto.github.io/images/research/sleep_eeg_9_subjects_06_23_2014/sleep_xform_mcduff/sleep_MCDUFF_transform_subj_4_cv_acc_pval_face&amp;scene.png"> method 3</a>   </p>

<p>Another degree of freedom is which wake pattern do we use to transform the data: face or scene or face minus scene.  I expected there to be variability between the face and scene transformations, but was surprised to find that the face minus scene didn’t just look like the combination of the face and scene results.  Here’s an example of that:   </p>

<p><a href="http://ElPiloto.github.io/images/research/sleep_eeg_9_subjects_06_23_2014/sleep_xform_avg_pattern/sleep_transform_cv_acc_pval_face_subj8.png">face</a>   </p>

<p><a href="http://ElPiloto.github.io/images/research/sleep_eeg_9_subjects_06_23_2014/sleep_xform_avg_pattern/sleep_transform_cv_acc_pval_scene_subj8.png">scene</a>    </p>

<p><a href="http://ElPiloto.github.io/images/research/sleep_eeg_9_subjects_06_23_2014/sleep_xform_avg_pattern/sleep_transform_cv_acc_pval_face&amp;scene_subj8.png">face minus scene</a>   </p>

<p>This leads me to believe that in the future we should train a sleep classifier that gets both face and a scene transformed data.  The dot-product is sensitive to magnitude, so it could be that there is some useful information in the indivudal face and scene patterns that gets lost when we subtract the two.  What do you think?</p>

<p>The main thing I’ve taken away from this data is that there are timebins of the sleep data that are classifiable, but that they vary across subjects.  This implication of this being that we have to combine the results of multiple classifiers (tested on different timebins) when we try to connect the classification to subsequent memory - but that the particular classifiers we use will have to vary across subjects.  I think an important thing to figure out is how we want to select which classifiers to use for a subject.  I’ve gone through and written down a bunch of time bins that I think look good for each subject on the second page of our EEG to-do <a href="https://docs.google.com/spreadsheets/d/1TIOy-4DN4adDDVBKbuRZ2MuI5g-Hg7KWm1nBKjwSHsU/edit?usp=sharing">spreadsheet</a>, but that was done entirely by hand which doesn’t feel right.  </p>

<p>In general, I think we could proceed to linking the classifier to the behavioral or that we could try to improve the classification.  For the latter, I have a crazy idea bout using an “auto-encoder” for preprocessing the sleep data.  We could feed the network sleep data and have it’s target be the corresponding wake image.  We could apply this network in a leave-one-out fashion in order to preprocess sleep data for a regular classifier.  Seems like it’d be pretty tough to train the network given the dimensionality of the data and the relatively small number of training samples, but I figured it’s worth mentioning.  I’ll wait to hear what you think about all this, but I’m going to dedicate my brain cycles to the problem of figuring out how to select which time bins to use.</p>

<h3 id="wake-classification-results">Wake Classification Results</h3>
<p>These are important because it’s likely we’ll want to exclude subjects with poor wake cross-validation AUC from subsequent analyses.</p>

<p><a href="http://ElPiloto.github.io/images/research/sleep_eeg_9_subjects_06_23_2014/wake_AUC_most_subjects.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_9_subjects_06_23_2014/wake_AUC_most_subjects.png" width="700" height="350" /></a></p>

<p><strong>Figure 1a:</strong> This shows the AUC for leave-one-out cross-validation on wake EEG data training on all the z-scored frequencies corresponding to a particular time bin as indicated on the y-axis.  The AUC was calculating by training a classifier that distinguishes between all classes, but only testing on patterns that corresponded to either a celebrity or a landmark.  The AUC was calculated by feeding in the difference between the output for the celebrity one-vs-all classifier and the landmark one-vs-all classifier for each cross-validation fold and the plotted values show the mean AUC across all cross-validation folds.  We had previously looked at the cross-validation accuracy for the first two subjects across a wider range of times, but narrowed it down to these four windows.</p>

<p><code>plot_loopify_time_sweep_results_AUC.m </code>  <br />
<code>classify_piloy_log_reg_time_sweep_driver.m </code>      </p>

<p><a href="http://ElPiloto.github.io/images/research/sleep_eeg_9_subjects_06_23_2014/wake_auc_avgd.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_9_subjects_06_23_2014/wake_auc_avgd.png" width="700" height="350" /></a></p>

<p><strong>Figure 1b:</strong> Average AUC per time bin</p>

<h3 id="sleep-cross-validation-classification-all-frequencies-per-time-bin">Sleep Cross-Validation Classification: All Frequencies Per Time Bin</h3>
<p><strong>NOTE:</strong> These results were supposed to be for performing feature selection, but I accidentally set the feature selection statistical threshold to 1, which is the same as not using feature selection.  <del>Re-running these results with feature selection.</del>  The results actually containing feature selection are just below.</p>

<p><a href="http://ElPiloto.github.io/images/research/sleep_eeg_9_subjects_06_23_2014/all_9subjects_sleep_CV_AUC_feature_select.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_9_subjects_06_23_2014/all_9subjects_sleep_CV_AUC_feature_select.png" width="700" height="350" /></a>
<strong>Figure 2a)</strong> We plot the AUC for classification of celebrity vs. landmarks during sleep across all our subjects (y-axis).  This was calculated by performing cross-validation (somewhere between 15-fold and 40-fold depending on the number of sleep patterns available) using all frequencies z-scored across electrodes for a particular timebin (x-axis). <br />
<a href="http://ElPiloto.github.io/images/research/sleep_eeg_9_subjects_06_23_2014/all_9subjects_sleep_CV_AUC_feature_select_PVAL.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_9_subjects_06_23_2014/all_9subjects_sleep_CV_AUC_feature_select_PVAL.png" width="700" height="350" /></a>
<strong>Figure 2b)</strong> We plot the p-values for combinations of subjects and time bins with sleep cross-validation AUC values that are calculated via a shuffled permutation test whereby the labels of the classes are shuffled, we train a classifier with the shuffled patterns, and look at the cross-validation AUC for this shuffled dataset. Areas in red have p-values &gt; 0.05.
<a href="http://ElPiloto.github.io/images/research/sleep_eeg_9_subjects_06_23_2014/all_9subjects_sleep_CV_acc_feature_select.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_9_subjects_06_23_2014/all_9subjects_sleep_CV_acc_feature_select.png" width="700" height="350" /></a>
<strong>Figure 2c)</strong> Instead of calculating the AUC, we plot the classification accuracy - this should be pretty similar to Figure 2b.
<a href="http://ElPiloto.github.io/images/research/sleep_eeg_9_subjects_06_23_2014/all_9subjects_sleep_CV_acc_feature_select_PVAL.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_9_subjects_06_23_2014/all_9subjects_sleep_CV_acc_feature_select_PVAL.png" width="700" height="350" /></a>
<strong>Figure 2d)</strong> Here we plot the p-values for the classification accuracies.</p>

<h3 id="sleep-cross-validation-classification-feature-selection-all-frequencies-per-time-bin">Sleep Cross-Validation Classification Feature Selection: All Frequencies Per Time Bin</h3>

<p><a href="http://ElPiloto.github.io/images/research/sleep_eeg_9_subjects_06_23_2014/9_subjects_CV_ACTUAL_AUC_feature_select.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_9_subjects_06_23_2014/9_subjects_CV_ACTUAL_AUC_feature_select.png" width="700" height="350" /></a>
<strong>Figure 3a)</strong> We plot the AUC for classification of celebrity vs. landmarks during sleep across all our subjects (y-axis).  This was calculated by performing cross-validation (somewhere between 15-fold and 40-fold depending on the number of sleep patterns available) using all frequencies z-scored across electrodes for a particular timebin (x-axis). <br />
<a href="http://ElPiloto.github.io/images/research/sleep_eeg_9_subjects_06_23_2014/9_subjects_CV_ACTUAL_AUC_feature_select_PVAL.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_9_subjects_06_23_2014/9_subjects_CV_ACTUAL_AUC_feature_select_PVAL.png" width="700" height="350" /></a>
<strong>Figure 3b)</strong> We plot the p-values for combinations of subjects and time bins with sleep cross-validation AUC values that are calculated via a shuffled permutation test whereby the labels of the classes are shuffled, we train a classifier with the shuffled patterns, and look at the cross-validation AUC for this shuffled dataset. Areas in red have p-values &gt; 0.05.
<a href="http://ElPiloto.github.io/images/research/sleep_eeg_9_subjects_06_23_2014/9_subjects_CV_ACTUAL_acc_feature_select.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_9_subjects_06_23_2014/9_subjects_CV_ACTUAL_acc_feature_select.png" width="700" height="350" /></a>
<strong>Figure 3c)</strong> Instead of calculating the AUC, we plot the classification accuracy - this should be pretty similar to Figure 2b.
<a href="http://ElPiloto.github.io/images/research/sleep_eeg_9_subjects_06_23_2014/9_subjects_CV_ACTUAL_acc_feature_select_PVAL.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sleep_eeg_9_subjects_06_23_2014/9_subjects_CV_ACTUAL_acc_feature_select_PVAL.png" width="700" height="350" /></a>
<strong>Figure 3d)</strong> Here we plot the p-values for the classification accuracies.</p>

<h3 id="sleep-cross-validation-transform-sleep-pattern-by-average-wake-pattern">Sleep Cross-Validation: Transform Sleep Pattern By Average Wake Pattern</h3>
<p><a href="http://ElPiloto.github.io/blog/2014/07/01/sleep-eeg-transform-avg-pattern/"> dump of plots </a>   </p>

<p>We have 4 x 3 plots per subject (4 plots (AUC, accuracy, and p-val map for both of those) for each result, 3 different results: one for transforming the sleep data with the average face pattern, one for transforming with the average scene pattern, and one for transforming with the difference between the average face and average scene)</p>

<h3 id="sleep-cross-validation-transform-sleep-pattern-by-mcduff-importance-map">Sleep Cross-Validation: Transform Sleep Pattern By McDuff Importance Map</h3>
<p><a href="http://ElPiloto.github.io/blog/2014/07/01/sleep-eeg-mcduff-transform/"> dump of plots </a>   </p>

<p>We have 4 x 3 plots per subject (4 plots (AUC, accuracy, and p-val map for both of those) for each result, 3 different results: one for transforming the sleep data with the face mcduff importance map, one for transforming with the scene mcduff importance map, and one for transforming with the difference between the face and scene importance maps)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Understanding SFA]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/06/11/understanding-sfa/"/>
    <updated>2014-06-11T11:14:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/06/11/understanding-sfa</id>
    <content type="html"><![CDATA[<h3 id="questions">Questions</h3>
<pre> Estimating Driving Forces of Nonstationary Time Series with Slow Feature Analysis, Wiskott (2003)</pre>
<p>What does it mean that slow-feature analysis only considers one point at a time?</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Representational Similarity for Non-markov Task in LSTM Hidden Layers and SFA of Hidden Layers]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/06/10/representational-similarity-for-non-markov-task-in-lstm-hidden-layers-and-sfa-of-hidden-layers/"/>
    <updated>2014-06-10T15:48:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/06/10/representational-similarity-for-non-markov-task-in-lstm-hidden-layers-and-sfa-of-hidden-layers</id>
    <content type="html"><![CDATA[<p>We’re interested in how slow-feature analysis processes representations generated for a non-Markov task.  Towards this end, we apply slow-feature analysis to the hidden layer representation of a recurrent long short-term memory network that solves a non-Markovian task.  Importantly, we must first understand the hidden layer representation to see which components to the SFA’d hidden layer representation are generated from the slow-feature process and which were already present in the data.</p>

<h3 id="raw-hidden-layer-and-sfa-values">Raw Hidden Layer and SFA Values</h3>

<p>In a previous post, we have the raw hidden layer representation as we loop around the maze.  Here is an alternate version of that plot:
<a href="http://ElPiloto.github.io/images/research/hidden_layer_time_series.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/hidden_layer_time_series.png" width="700" height="350" /></a>
<strong>Figure 1:</strong> The first three subplots show the hidden layer activations for all three hidden nodes.  The last subplot shows the sum of all three activations.  The blue stripes in the background correspond to the times during which the “rat” is at the fork.  Alternating bands correspond to travelling in alternate directions of the maze.</p>

<p>This task is only non-Markovian in the center hallway: the correct action is entirely determined by the current location for all locations on the outside of the maze, but this is not true of the center hallway locations.  If we look at <code>hidden node 3</code>, we see that it only differs in value across loop directions at the timepoints near the fork.  Essentially, <code>hidden node 3</code> provides a linearly separable signal that indicates the direction of travel.  This is made even clearer in the two plots below showing the hidden layer activations for each location on the maze either on a leftward or rightward loop.</p>

<p><a href="http://ElPiloto.github.io/images/research/hidden_layer_on_maze_raw_left.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/hidden_layer_on_maze_raw_left.png" width="700" height="350" /></a>
<a href="http://ElPiloto.github.io/images/research/hidden_layer_on_maze_raw_right.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/hidden_layer_on_maze_raw_right.png" width="700" height="350" /></a></p>

<p><strong>Figure 2:</strong>  The normalized (-1 to 1) hidden layer representation, a vector of three values: one for each hidden node, for each location in the maze is plotted using <em>imagesc</em> at that location in the maze.  This is done for both leftward (top) and rightward (bottom) loops.  For example, the subplot in the middle row of the center column on the top plot (“Raw Hidden Layer Values - Left”) shows the hidden layer representation for that location in the maze during a leftward loop.  The hidden layer representation for that location during a rightward loop is shown on the bottom plot at the same location.  Do not be confused by the fact that the bottom plot for a rightward loop has filled in values for locations on the maze that are only visited during a leftward loop - these are just pasted in from the leftward loop for visualization purposes (and vice-versa for the leftward loop and rightward loop maze locations).</p>

<p>We see that <code>hidden node 3</code> has different values for the different loops along the center hallway, but nearly symmetric values along the outsides of the maze.  It is less clear what the other hidden units may be encoding.  A comparison of the first two hidden unit activations across loops shows that they code the outer loops of the maze differently, but not points along the center hallway (Figure 3 below).</p>

<p><a href="http://ElPiloto.github.io/images/research/hidden_nodes_1_and_2_comparison.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/hidden_nodes_1_and_2_comparison.png" width="700" height="350" /></a>
<strong>Figure 3:</strong> Above we plot the activation for the first and second hidden units for each loop direction of the maze and subsequently their sum (bottom plot).  The x-axis labels analogous locations in the maze across the loops e.g. “Top Corner” on the middle plot shows the activation of <code>hidden node 2</code> for the top left corner location (blue) and the the top right corner (green).   Notably, the activations are pretty similar for the center hallway locations (“Fork”, “Bottom Center”, and “Mid Center”).</p>

<p>The network does find 20 unique states (the true number of unique states in our task) and we can see this just using the first two hidden unit activations.  This begs the question: is the third hidden unit needed?  It would be <strong>very</strong> helpful to plot the multidimensional scaling for all three hidden unit activations, <del>so maybe somebody (<em>cough</em> Luis <em>cough</em> <em>cough</em> a.k.a. my own self <em>cough</em>) should do that</del> here is that <a href="http://ElPiloto.github.io/images/research/hidden_layer_mds.png">plot</a>. Another diagnostic to assess the utility of this third hidden unit would be to look at how the slow-feature analysis output varies based on whether we give all three hidden units or just the first two.
<a href="http://ElPiloto.github.io/images/research/hidden_layer_representation_mds_rm_1st_visit.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/hidden_layer_representation_mds_rm_1st_visit.png" width="700" height="350" /></a> 
<strong>Figure 4:</strong> Above we plot the hidden unit activations for each time step in our trajectory of 20 loops around the maze.  Importantly, locations on the outside of the maze (circles with black or magenta outlines) cluster together across visits. The center locations, however, are cluster according to loop direction.</p>

<p>We can also look at the hidden layer represtation similarity from one location to the other locations as shown below.
<a href="http://ElPiloto.github.io/images/research/hidden_layer_self_similarity.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/hidden_layer_self_similarity.png" width="700" height="350" /></a> 
<strong>Figure 5:</strong> This shows the representational similarity for each unique state (recall that not all locations correspond to unique states).  The labels along the axes correspond to different locations on the maze (also indicating the loop direction when relevant) using the key below.</p>
<center>
<a href="http://ElPiloto.github.io/images/research/legend_correlation_plots.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/legend_correlation_plots.png" width="450" height="65" /></a> 
</center>

<p>Below we show an alternative method for visualizing the hidden layer representation of a location to other locations.
<a href="http://ElPiloto.github.io/images/research/Linear_corr_states_hidden_and_sfa_hidden_representation.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/Linear_corr_states_hidden_and_sfa_hidden_representation.png" width="700" height="350" /></a>
<strong>Figure 6:</strong> For each location in the maze, we create a subplot that shows the hidden layer representation for that location in the maze against all other locations in the maze.  For example, to look at the representational similarity between the top-left corner and the bottom-right corner, look at the top-left subplot and the bottom-right square within that subplot.  The similarity between a location and itself is also plotted, which is uninformative in the case of locations on the outside of the maze.  However, for locations that have multiple states (i.e. the fork location corresponds to both states “Fork - Leftward loop” and “Fork - Rightward loop”), the similarity is plotted across loop directions e.g. the top-center subplot shows the representational similarity for the fork location and the top-center value in that subplot corresponds to the representational similarity between the fork on a leftward loop and the fork on a rightward loop.</p>

<h3 id="dump-of-sfa-similarity-plots">Dump of SFA Similarity Plots</h3>
<p><a href="http://ElPiloto.github.io/images/research/Linear_corr_states_hidden_and_sfa_sfa3.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/Linear_corr_states_hidden_and_sfa_sfa3.png" width="700" height="350" /></a>
<a href="http://ElPiloto.github.io/images/research/Linear_corr_states_hidden_and_sfa_sfa2.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/Linear_corr_states_hidden_and_sfa_sfa2.png" width="700" height="350" /></a>
<a href="http://ElPiloto.github.io/images/research/Nonlinear_corr_states_hidden_and_sfa_sfa3.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/Nonlinear_corr_states_hidden_and_sfa_sfa3.png" width="700" height="350" /></a>
<a href="http://ElPiloto.github.io/images/research/Nonlinear_corr_states_hidden_and_sfa_sfa2.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/Nonlinear_corr_states_hidden_and_sfa_sfa2.png" width="700" height="350" /></a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG Ken Meeting]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/06/02/sleep-eeg-ken-meeting/"/>
    <updated>2014-06-02T11:27:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/06/02/sleep-eeg-ken-meeting</id>
    <content type="html"><![CDATA[<h3 id="context">Context</h3>

<p>After ending the first-year rotation, I’m now back on the hook for working on this sleep EEG project full-time.  This meeting was simply to re-orient ourselves and plan the next immediate steps.</p>

<h3 id="next-steps">Next Steps</h3>
<ol>
  <li>Area under ROC instead of classification accuracy   </li>
  <li>Represent goodness in terms of where it falls on the empirical null distribution (as opposed to previously where we were showing raw-classification accuracy which we really have no clue how good that should be)  </li>
  <li>Train sleep classifier using wake WEIGHTS instead of wake average pattern as template.  The rationale behind this is that the average pattern templates treat all electrodes as equally important, but this just isn’t true.  The importance maps will take that into account   </li>
  <li>Create existing plots for new subjects [n = 8] (!)</li>
</ol>

<h3 id="next-next-steps">Next Next Steps</h3>
<ol>
  <li>We need to think about how we can collapse down each sleep trial (playing of a sleep sound) into a single reactivation metric.  Currently, we have classifier output for different timebins of the sleep trial and it seems like the maximally-classifiable time bin varies across subjects. <br />
    <ul>
      <li>Train an “uberclassifier” that classifies across all timebins - this would give us a single reactivation score per trial   </li>
      <li>Is there a smart way of reducing the number of features we have?<br />
        <ul>
          <li>we might have some <em>a priori</em> ideas about which features to include.  </li>
          <li>we might want to look at the cross-validation accuracy along certain time-bins to narrow things down.   </li>
        </ul>
      </li>
      <li>This is where I thought a recurrent neural network might be helpful for us.  </li>
      <li>Alternative idea: train a higher-level classifier on the outputs of the individual time bin classifiers    </li>
    </ul>
  </li>
  <li>How do we relate reactivations to subsequent memory?  Recall that there will be multiple classifier readouts per item, perhaps something a la [P-CIT] (https://code.google.com/p/p-cit-toolbox/) would work for us.</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Paper: Deep Learning from Temporal Coherence in Video]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/05/30/paper-deep-learning-from-temporal-coherence-in-video/"/>
    <updated>2014-05-30T15:32:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/05/30/paper-deep-learning-from-temporal-coherence-in-video</id>
    <content type="html"><![CDATA[<h3 id="punchline">Punchline:</h3>

<p>They used a convolutional neural network to do object recognition on video streams.  Motivated by the idea that consecutive video frames likely contain the same objects and therefore should have similar representations, they modify the neural network cost function to include a “coherence” term:  </p>

<script type="math/tex; mode=display">% &lt;![CDATA[
 L_{coh}(\theta, inputx, inputy) = \begin{cases}
		  \|z_{\theta}(input_x) - z_{\theta}(input_y) \|_1  & \textbf{if x,y consecutive}\\
		  max(0, \delta - \|z_{\theta}(input_x) - z_{\theta}(input_y) \|_1)   & \textbf{otherwise}\\
\hline
\end{cases}  \\
\delta: \text{hyperparam push apart non-consecutive representations} \\
z_{\theta}(x) = \text{hidden layer representation for input x just before output layer}  %]]&gt;</script>

<h3 id="noteworthy-details">Noteworthy Details:</h3>
<p>Training: They do some weird training where you have to look at the output of the network for two different outputs, they call this a siamese architecture.</p>

<h3 id="ideas">Ideas:</h3>
<ul>
  <li>Modify $ L_{coh} $ to be a function of the number of time steps between inputs  </li>
  <li>What if we could use slow-features as a better proxy for temporal distance between training samples i.e. push representations closer together based on the difference between their slowest features?</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[LSTM trained multiple loops]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/05/22/lstm-trained-multiple-loops/"/>
    <updated>2014-05-22T15:46:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/05/22/lstm-trained-multiple-loops</id>
    <content type="html"><![CDATA[<h3 id="status">Status:</h3>

<ul>
  <li>Rudimentary system for importing rnnlib log information to matlab for performing SFA</li>
  <li><strong>Successfully trained LSTM on 20 loops through an eightball maze (same dimensions as last time): 0% misclassification</strong></li>
</ul>

<p><a href="http://ElPiloto.github.io/images/research/hidden_state_activations_width3_loops20.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/hidden_state_activations_width3_loops20.png" width="700" height="350" /></a></p>

<h3 id="questions-for-meeting">Questions for meeting</h3>
<ul>
  <li>What should we perform the SFA on?</li>
  <li>How many loops around the maze do we need?</li>
</ul>

<h3 id="next-steps">Next steps</h3>
<p>Look at similarity metric for each timestep using correlation matrix - send to Matt
Be wary of SFA on multidimensional data and throwing away certain features
Send SFA
Waldo task: Ari did SFA on the task (using sparse representation, 8 units
perhaps LSTM’ing Waldo task
generate figures for a week from now</p>

<p>short term memory for serial order - matt bot paper, network needs to remember things that aren’t relevant to current output, 
single unit recording analysis paper by bill newsome, analyzed by valerio monte, nature 2013 “context-dependent computations by recurrent dynamics in prefrontal cortex”</p>

<p>LSTM seems different than orthogonal</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[LSTM Trained on Non-Markov Loop]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/05/22/lstm-trained/"/>
    <updated>2014-05-22T13:42:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/05/22/lstm-trained</id>
    <content type="html"><![CDATA[<h3 id="status">Status:</h3>

<p>Successfully trained (finally) a LSTM network on a single loop through an eightball maze.
The eightball maze had a width $= 7$, height $= 3$, giving a total of 17 locations in the maze.</p>

<p><a href="http://ElPiloto.github.io/images/research/non_markov_maze.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/non_markov_maze.png" width="700" height="350" /></a></p>

<p><strong>This is the architecture used:</strong> <br />
- input layer: represent each location as an input node<br />
- hidden layer: 2 LSTM blocks<br />
- output layer: represent each output location as an output node  </p>

<p><strong>Training/Testing:</strong> <br />
Input was a single sequence which consisted of a full loop around the maze, which takes 21 time steps. Testing was done simply by specifying the next step in the sequence.</p>

<p><strong>RNNLIB Config File &amp;&amp; CDL</strong></p>

<h3 id="next-steps">Next Steps:</h3>
<ul>
  <li>Run SFA on this output</li>
  <li>Train/test on sequences that have more loops around the maze  </li>
  <li>Run SFA on this output</li>
  <li>Logistical<br />
    <ul>
      <li>Develop method of transferring from rnnlib log files to matlab so that we can do SFA</li>
    </ul>
  </li>
</ul>

<h3 id="rnnlib-discoveries--questions">RNNLIB Discoveries &amp; Questions</h3>
<ul>
  <li>Default behavior is to have hidden layers be recurrent in time both forwards and backwards, this is <strong>NOT</strong> what we want for our current usage.  </li>
  <li><code>hiddenSize</code> controls the number of units in the hidden layer, to specify multiple hidden layers can add values here  </li>
  <li><code>hiddenBlock</code> still unclear what this does   </li>
  <li><code>inputBlock</code> not sure what the benefit of creating an input block is, it seems to just replicate the input layer and has the same exact activations when I look at the log file  </li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG Update 2]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/05/04/sleep-eeg-update-2/"/>
    <updated>2014-05-04T20:53:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/05/04/sleep-eeg-update-2</id>
    <content type="html"><![CDATA[<p>Here are some questions/conclusions regarding the new data:</p>

<h3 id="are-the-trained-classifier-weights-highest-at-the-frequencies-used-to-generate-the-wake-template--for-instance-given-that-sleep-data-was-transformed-using-the-faces-template-at-230-ms-and-10-hz-does-the-trained-sleep-classifier-have-high-weights-at-10-hz">Are the trained classifier weights highest at the frequencies used to generate the wake template?  For instance, given that sleep data was transformed using the faces template at 230 ms and 10 Hz, does the trained sleep classifier have high weights at 10 Hz?</h3>
<p>This does not seem to be the case.  If we look at the averaged (across time bins) classifier weights at each frequency, we see that there are hot spots of increased feature importance at frequencies that do not depend on the parameters used for the wake template generation.  This is illustrated in the figure below: we see increased feature importance at 3 Hz regardless of the  wake time bin and frequency used for generating the wake template.  We might be tempted to say there is a relationship between the encoding frequency and the feature importance at 10 Hz, but I don’t buy it given that we only see increased feature importance along the y-axis (indicating that there seems to be something informative about the sleep data at 10 Hz) but not the x-axis.</p>

<p><a href="http://ElPiloto.github.io/images/research/collaped_weights_by_freq_faceMINUSscene_wake_template_subj1.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/collaped_weights_by_freq_faceMINUSscene_wake_template_subj1.png" width="700" height="350" /></a></p>

<h3 id="are-there-any-trends-in-increased-weights-across-any-of-the-following-parameters">Are there any trends in increased weights across any of the following parameters?</h3>

<h4 id="frequencies">Frequencies?</h4>
<p>Across both subjects, we see increased feature importance around 3 Hz.  Additionally, we can see, in both subjects, something around the 10 Hz range (10 Hz for subject 1,  7-12 Hz for subject 2) and a faint </p>

<h4 id="time-bins">Time bins?</h4>

<h4 id="face-vs-scene-vs-face-minus-scene">Face vs. Scene Vs. Face minus Scene</h4>

<h4 id="is-there-a-link-between-the-magnitude-of-the-trained-sleep-classifier-weights-and-the-maximum-cv-accuracy-achieved-at-that-particular-time-bin--this-will-indicate-whether-or-not-there-is-a-benefit-to-reducing-the-classification-process-even-further-using-a-subset-of-the-features">Is there a link between the magnitude of the trained sleep classifier weights and the maximum CV accuracy achieved at that particular time bin?  This will indicate whether or not there is a benefit to reducing the classification process even further using a subset of the features.</h4>

<p>Future To-do: Is there a link between the trained sleep classifier weights and the average cross-validation accuracy for the shuffled sleep?  What would this tell us about the underlying space?</p>

<h4 id="what-is-the-relationship-between-the-weights-for-the-sleep-transformed-classifier-compared-to-the-feature-selected-weights">What is the relationship between the weights for the sleep transformed classifier compared to the feature selected weights?</h4>
<p>I don’t have this data yet.</p>

<h4 id="shuffling-results-given-that-we">Shuffling results: given that we</h4>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG Update]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/05/04/sleep-eeg-update/"/>
    <updated>2014-05-04T18:45:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/05/04/sleep-eeg-update</id>
    <content type="html"><![CDATA[<h3 id="results">Results</h3>

<h4 id="shuffled-cross-validation-accuracy-using-transformed-sleep-data-item--5-on-to-do-list">Shuffled Cross-validation Accuracy Using Transformed Sleep Data (Item # 5 on To-Do List)</h4>
<p>The plots below were using the generate-template-on-wake-then-transform-sleep-data-then-train-a-sleep-classifier approach.  The x-axis corresponds to training on different sleep time bins (although we use all the frequencies) and the y-axis corresponds to using different time-freq pairs to generate the wake template.  The key part of this plot is that we <strong>randomly permuted the sleep labels</strong> - so we would hope to see 50% classification accuracy here (which unfortunately doesn’t seem to be the case).  Ideally, I would have multiple iterations of this shuffling, but so far I’ve only run a single shuffle - <strong>how serious is it that I haven’t run multiple iterations?</strong></p>

<p><a href="http://ElPiloto.github.io/images/research/subject_1sleep_transform_CV_shuffled.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/subject_1sleep_transform_CV_shuffled.png" width="700" height="350" /></a>
<a href="http://ElPiloto.github.io/images/research/subject_2sleep_transform_CV_shuffled.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/subject_2sleep_transform_CV_shuffled.png" width="700" height="350" /></a></p>

<h4 id="sleep-cross-validation-accuracy-using-different-sleep-transformations-item-2-on-to-do-list">Sleep Cross-validation Accuracy using different sleep transformations (Item #2 on To-Do List)</h4>
<p>Previously, we had only tried the generate-template-on-wake-then-transform-sleep-data-then-train-a-sleep-classifier using the FACE template, the results below show cross-validation accuracy using the face template (same results as presented previously), the scene template, and face-minus-scene.</p>

<p><a href="http://ElPiloto.github.io/images/research/subject_1sleep_transform_CV.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/subject_1sleep_transform_CV.png" width="700" height="350" /></a>
<a href="http://ElPiloto.github.io/images/research/subject_2sleep_transform_CV.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/subject_2sleep_transform_CV.png" width="700" height="350" /></a>
<a href="http://ElPiloto.github.io/images/research/subject_3sleep_transform_CV.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/subject_3sleep_transform_CV.png" width="700" height="350" /></a>
<a href="http://ElPiloto.github.io/images/research/subject_4sleep_transform_CV.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/subject_4sleep_transform_CV.png" width="700" height="350" /></a>
<a href="http://ElPiloto.github.io/images/research/subject_5sleep_transform_CV.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/subject_5sleep_transform_CV.png" width="700" height="350" /></a>
<a href="http://ElPiloto.github.io/images/research/subject_6sleep_transform_CV.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/subject_6sleep_transform_CV.png" width="700" height="350" /></a></p>

<h4 id="sleep-cross-validation-accuracy-using-all-frequencies-feature-select-item-4-on-to-do-list">Sleep Cross-validation Accuracy Using All Frequencies Feature Select (Item #4 on To-Do List)</h4>
<p>This is pretty straightforward cross-validation accuracy performed on the labelled sleep data.  For each time bin of the sleep data, we z-scored the power spectrum at all frequencies across electrodes and trained a classifier using 10-fold cross-validation using standard p-value based feature selection.</p>

<p><a href="http://ElPiloto.github.io/images/research/all_2_subjects_sleep_CV_feature_select.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/all_2_subjects_sleep_CV_feature_select.png" width="700" height="350" /></a></p>

<h4 id="classifier-weights-for-sleep-transformed-data-item-1-on-to-do-list">Classifier Weights for sleep transformed data (Item #1 on To-Do List)</h4>
<p>Recall that we have a classifier for each combination of wake time-freq pair and sleep time.  Each classifier sweeps the template across each frequency of the time bin, yielding a classifier with as many features as there are frequencies.  Thus, visualizing the weights would require displaying the magnitude of the weight across three dimensions: wake time-freq pair, sleep time bin, and sleep frequency.  Instead of trying to plot some whacky surface, I’ve created plots that average the magnitude of the weights either across the sleep time bins OR across sleep frequencies - if a particular time bin or frequency looks promising from these plots, we can look at the uncollapsed results as needed.</p>

<p>The value displayed below is taken as follows:<br />
<code>
for each wake time-freq bin x sleep time bin pair:<br />
 &nbsp; &nbsp; &nbsp; &nbsp;for each cross-validation iteration:<br />
		 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;z-score weights<br />
	&nbsp; &nbsp; &nbsp; &nbsp;Avg weights over cross-validation iterations and take absolute value<br />
</code></p>

<p>This 3d matrix was then collapsed either across sleep time bins or sleep frequencies.  This was done for the various wake templates.  </p>

<p><strong>KEN:</strong> I know this is pretty different than the McDuff method for interpreting feature weights - but the requirement to average over the patterns made things difficult so I did things this way as a much easier to accomplish first pass.  It’s worth noting that the EEG analysis code rescales all features to be within the range of 0 and 1, so I think this crude way of looking at the features should still be pretty informative.</p>

<h4 id="face-minus-scene-wake-template">Face Minus Scene Wake Template</h4>
<p><a href="http://ElPiloto.github.io/images/research/collaped_weights_by_freq_faceMINUSscene_wake_template_subj1.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/collaped_weights_by_freq_faceMINUSscene_wake_template_subj1.png" width="700" height="350" /></a>
<a href="http://ElPiloto.github.io/images/research/collaped_weights_by_freq_faceMINUSscene_wake_template_subj2.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/collaped_weights_by_freq_faceMINUSscene_wake_template_subj2.png" width="700" height="350" /></a></p>

<h4 id="face-wake-template">Face Wake Template</h4>
<p><a href="http://ElPiloto.github.io/images/research/collaped_weights_by_freq_face_wake_template_subj1.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/collaped_weights_by_freq_face_wake_template_subj1.png" width="700" height="350" /></a>
<a href="http://ElPiloto.github.io/images/research/collaped_weights_by_freq_face_wake_template_subj2.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/collaped_weights_by_freq_face_wake_template_subj2.png" width="700" height="350" /></a></p>

<h4 id="scene-wake-template">Scene Wake Template</h4>
<p><a href="http://ElPiloto.github.io/images/research/collaped_weights_by_freq_scene_wake_template_subj1.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/collaped_weights_by_freq_scene_wake_template_subj1.png" width="700" height="350" /></a>
<a href="http://ElPiloto.github.io/images/research/collaped_weights_by_freq_scene_wake_template_subj2.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/collaped_weights_by_freq_scene_wake_template_subj2.png" width="700" height="350" /></a></p>

<h4 id="face-minus-scene-wake-template-1">Face Minus Scene Wake Template</h4>
<p><a href="http://ElPiloto.github.io/images/research/collapsed_weights_by_sleep_time_faceMINUSscene_wake_template_subj1.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/collapsed_weights_by_sleep_time_faceMINUSscene_wake_template_subj1.png" width="700" height="350" /></a>
<a href="http://ElPiloto.github.io/images/research/collapsed_weights_by_sleep_time_faceMINUSscene_wake_template_subj2.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/collapsed_weights_by_sleep_time_faceMINUSscene_wake_template_subj2.png" width="700" height="350" /></a></p>

<h4 id="face-wake-template-1">Face Wake Template</h4>
<p><a href="http://ElPiloto.github.io/images/research/collapsed_weights_by_sleep_time_face_wake_template_subj1.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/collapsed_weights_by_sleep_time_face_wake_template_subj1.png" width="700" height="350" /></a>
<a href="http://ElPiloto.github.io/images/research/collapsed_weights_by_sleep_time_face_wake_template_subj2.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/collapsed_weights_by_sleep_time_face_wake_template_subj2.png" width="700" height="350" /></a></p>

<h4 id="scene-wake-template-1">Scene Wake Template</h4>
<p><a href="http://ElPiloto.github.io/images/research/collapsed_weights_by_sleep_time_scene_wake_template_subj1.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/collapsed_weights_by_sleep_time_scene_wake_template_subj1.png" width="700" height="350" /></a>
<a href="http://ElPiloto.github.io/images/research/collapsed_weights_by_sleep_time_scene_wake_template_subj2.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/collapsed_weights_by_sleep_time_scene_wake_template_subj2.png" width="700" height="350" /></a>
<!--### Questions-->
<!--##### Classifier Weights-->
<!--#. When we look at the magnitude of the trained classifier weights for various wake-template-frequencies, do we see that the weights are generally bigger at the frequencies that --></p>

<!--#### -->
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG Update]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/04/20/sleep-eeg-update/"/>
    <updated>2014-04-20T14:41:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/04/20/sleep-eeg-update</id>
    <content type="html"><![CDATA[<h3 id="classification-results">Classification results</h3>
<p>Last time I showed a plot for CV accuracy of wake data generated by looking at individual time bins, but using all possible frequencies.  This plot is the logical next step: it considers individual frequencies - which I did for only the most promising time bins (again independently).  I didn’t use various regularization penalties this time ($\lambda = 1$ for current plot).  Additionally, this data was z-scored across electrodes for each frequency.  </p>

<p><a href="http://ElPiloto.github.io/images/research/sweep_time_freq.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sweep_time_freq.png" width="700" height="350" /></a></p>

<p>It’s harder to pick out the “best” frequencies from this plot (compared to picking the “best” times previously).  The ~alpha band seems to pop out for both subjects, but it might be worthwhile to investigate some of the gamma stuff going on in subject 2.   Also, note that the colorbars are on different scales for both subjects.  Also we can actually get higher classification accuracy for both subjects than we did using all frequencies like last time.</p>

<p>The next analysis step we discussed was to pick a vector consisting of the power across electrodes at a best time and frequency and use this to transform the sleep data and perform cross-validation on the sleep data using this transformed dataset. With the idea being that after we get good sleep cv-accuracy, we could look at the relationship between classifier output and memory.</p>

<p>Sleep Transformation refresher:
- for each time bin of the sleep data, compute dot-product between the z-scored power across electrodes at a given frequency and the “best vector” described above and do this for each frequency</p>

<h3 id="next-steps">Next steps</h3>
<p>I see two approaches:<br />
1. glob across subjects and generate transformed-sleep CV results across a range of times and frequencies that seemed promising for both subjects (150ms - 230ms, 7-13Hz)<br />
2. treat subjects independently, rank (time x freq) pairs by highest CV accuracy and generate transformed-sleep CV results for top 10 (time x freq) pairs   </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG Update]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/04/16/sleep-eeg-update/"/>
    <updated>2014-04-16T12:50:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/04/16/sleep-eeg-update</id>
    <content type="html"><![CDATA[<h3 id="classification-results">Classification results:</h3>

<p>So far I’ve swept through the various time bins for two subjects and ran cross-validation on the wake data.  This was performed using leave one out cross-validation, zscoring the data across electrodes.   Additionally, I trained the classifier on all five categories (Faces, places, objects, scrambled faces, scrambled places), but only used the examples corresponding to faces or places for cross validation accuracy.  </p>

<p>Git branch: <code>sweep_features</code>
Git commit id#: <code>59c7561e0</code></p>

<p>Below is a plot of the results I got, although the <strong>columns are incorrect:</strong> <br />
- Column 1: Subject 1, Lambda = 1<br />
- Column 2: Subject 2, Lambda = 1<br />
- Column 3: Subject 1, Lambda = 50<br />
- Column 4: Subject 2, Lambda = 50<br />
- Column 5: Subject 1, Lambda = 100<br />
- Column 6: Subject 2, Lambda = 100  </p>

<p><a href="http://ElPiloto.github.io/images/research/sweep_time_bins.png" target="_blank"><img src="http://ElPiloto.github.io/images/research/sweep_time_bins.png" width="650" height="550" /></a></p>

<h3 id="next-steps">Next Steps:</h3>

<ul>
  <li>Look at next steps from previous meeting, they haven’t changed.</li>
</ul>
]]></content>
  </entry>
  
</feed>
