<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: convolutional | Something Witty]]></title>
  <link href="http://ElPiloto.github.io/blog/categories/convolutional/atom.xml" rel="self"/>
  <link href="http://ElPiloto.github.io/"/>
  <updated>2014-11-13T15:33:52-05:00</updated>
  <id>http://ElPiloto.github.io/</id>
  <author>
    <name><![CDATA[Luis R. Piloto]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[ZNN: Cpu-based Conv Nets]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/10/20/znn-cpu-based-conv-nets/"/>
    <updated>2014-10-20T12:08:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/10/20/znn-cpu-based-conv-nets</id>
    <content type="html"><![CDATA[<h1 id="znn">ZNN</h1>

<h3 id="motivation">Motivation</h3>
<ul>
  <li>not that many neural network implementations utilize multi-cpu</li>
  <li>o</li>
</ul>

<h2 id="convoluational-neural-net-training">Convoluational Neural Net Training</h2>

<h4 id="forward-pass">Forward Pass:</h4>
<p><em>Direct:</em>
- each edge is a filter
- convolve f &amp; w
<em>FFT Method:</em>
- FFT of image
- FFT of filter
- Multiple
- inverse FFT</p>

<h4 id="backward-pass">Backward Pass:</h4>
<p><em>Direct:</em>   <br />
calculate G   <br />
convolve g and w   <br />
<em>FFT Method:</em>   <br />
- calculate FFT of G   <br />
- FFT of W (already calculcated during forward pass)   <br />
- inverse FFT     </p>

<h4 id="updating-weights">Updating Weights</h4>
<p><em>Direct:</em>   <br />
- calculate $\frac{dE}{dB}$ and update the biases  <br />
- convolve F(ilter) &amp; G’ and update W   <br />
<em>FFT:</em>   <br />
- calculate $\frac{dE}{dB}$ and update the biases   <br />
- IFFT(FFT(F),FFT(G)), update W     </p>

<h2 id="parallel-models--data-vs-thread-vs-task-model">Parallel Models:  Data vs. Thread vs. Task Model</h2>

<p><strong>Data Model:</strong>
- parallelize a single operation (convolution)  <br />
- example: parallel loops   <br />
<strong>Thread Model:</strong>  <br />
- each thread has its own duties    <br />
- thread communicate with signals/messages   <br />
<strong>Task Model:</strong>  <br />
- taks is an abstract concept - a function <br />
- CPUs pick up tasks that are ready to be executed  <br />
- global queue - can be prioritized  <br />
- tasks can be stolen   </p>

<h2 id="znns-task-model">ZNN’s Task Model</h2>
<p><strong>Prioritized Task Model</strong>
<strong>Scheduling strategy</strong>
- priority based on the numbder of tasks depending on the current task’s completion
<strong>Stealing tasks</strong>     </p>

<h2 id="general-idea-for-parallelizing-forward-pass">General Idea for parallelizing forward pass:</h2>
<p>Let’s look at dependencies (for instance, need to load a training example before you’ll need to FFT it, maybe even before you FFT individual filters, etc) and to grab job’s as needed.  <br />
- Ultimately this leads to the only bottleneck being waiting for the output peceptron    </p>

<h2 id="general-idea-for-parallelizing-backward-pass">General Idea for parallelizing backward pass:</h2>
<ul>
  <li>can update weights as you do backward pass   </li>
</ul>

<h1 id="how-to-znn">How to Znn?</h1>

<p>Three input files:  <br />
1.  Network spec  <br />
2. Data spec  <br />
3. Training Options     </p>

<h4 id="network-spec">Network Spec:</h4>
<p>Can be specified as a graph:  <br />
	- node groups and edge groups  <br />
[C1]    % unique name of node group
size=5   % number of nodes
activation=tanh
act_params=1.712,0.666
% mass pooling operation(only support max pooling, atm)
filter=max
filter_size=2,2,1
filter_stride=2,2,1
% weight/bias initialization
init_type=Uniform
init_params=0.05
bias=0
% learning
eta=0.01
mom=0.9
wc=0
% fft switch - unnecessary
fft=0</p>

<p>Edgegroup spec:
[SOURCENODE_TARGETNODEGROUP] 
size=4,4,1
% more initialization
init_type=Uniform
init_params=0.05</p>

<h4 id="data-spec">Data Spec:</h4>
<p>[INPUT1]
path=
ext=image
size=512,512,30
pptype=standard2D %preprocessing -</p>

<p>[LABEL1]</p>

<h4 id="training-options">Training Options:</h4>
<p>[PATH]
config=./networks/N3.spec
load= (can load previously saved network instances)
data= DATA_SPEC_FILE
save = SAVE_FILE
[OPTIMIZE]
n_threads=64
force_fft=0 % force all edge groups to use FFT
optimize_fft=1
[TRAIN]
train_range=1
test_range=2
outsz=100,100,1
dp_type=volume % data provider types - only allows volume currently   <br />
cost_fn=cross_entropy   <br />
cost_fn_param=0    <br />
data_aug=1 % randomly transform input data (random rotation, random flip - each data provider is responsible for implementing its own data augmentation logic)   <br />
clas_thresh=0.5 % classification threshold   <br />
softmax=1 % apply softmax at output layer  <br />
[UPDATE]
force_eta=0.01
momentum=0.9
wc=0
anneal_factor=0.997
anneal_freq
[MONITOR] % saves current network instance
n_iters=100000
check_freq=10
test_freq=100
test_samples=10</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Paper: Deep Learning from Temporal Coherence in Video]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/05/30/paper-deep-learning-from-temporal-coherence-in-video/"/>
    <updated>2014-05-30T15:32:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/05/30/paper-deep-learning-from-temporal-coherence-in-video</id>
    <content type="html"><![CDATA[<h3 id="punchline">Punchline:</h3>

<p>They used a convolutional neural network to do object recognition on video streams.  Motivated by the idea that consecutive video frames likely contain the same objects and therefore should have similar representations, they modify the neural network cost function to include a “coherence” term:  </p>

<script type="math/tex; mode=display">% &lt;![CDATA[
 L_{coh}(\theta, inputx, inputy) = \begin{cases}
		  \|z_{\theta}(input_x) - z_{\theta}(input_y) \|_1  & \textbf{if x,y consecutive}\\
		  max(0, \delta - \|z_{\theta}(input_x) - z_{\theta}(input_y) \|_1)   & \textbf{otherwise}\\
\hline
\end{cases}  \\
\delta: \text{hyperparam push apart non-consecutive representations} \\
z_{\theta}(x) = \text{hidden layer representation for input x just before output layer}  %]]&gt;</script>

<h3 id="noteworthy-details">Noteworthy Details:</h3>
<p>Training: They do some weird training where you have to look at the output of the network for two different outputs, they call this a siamese architecture.</p>

<h3 id="ideas">Ideas:</h3>
<ul>
  <li>Modify $ L_{coh} $ to be a function of the number of time steps between inputs  </li>
  <li>What if we could use slow-features as a better proxy for temporal distance between training samples i.e. push representations closer together based on the difference between their slowest features?</li>
</ul>

]]></content>
  </entry>
  
</feed>
