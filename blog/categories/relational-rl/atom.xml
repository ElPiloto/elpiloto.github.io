<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: relational RL | Something Witty]]></title>
  <link href="http://ElPiloto.github.io/blog/categories/relational-rl/atom.xml" rel="self"/>
  <link href="http://ElPiloto.github.io/"/>
  <updated>2014-09-02T15:10:36-04:00</updated>
  <id>http://ElPiloto.github.io/</id>
  <author>
    <name><![CDATA[Luis R. Piloto]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Relational RL]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/08/10/relational-rl/"/>
    <updated>2014-08-10T16:12:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/08/10/relational-rl</id>
    <content type="html"><![CDATA[<h2 id="motivation">Motivation</h2>

<p>Why do we even care about relational reinformcent learning?  </p>

<ol>
  <li>
    <p><em>Rule Learning by Seven-Month-Old Infants.</em> Marcus, Vijayan, Rao, &amp; Vishton, 1999:<br />
<strong>Methods:</strong>  <br />
7-month old infants were exposed to sentences of the form: ABB or ABA during a training phase (e.g. “ga ti ti” or “ga ti ga”), testing phase entailed consistent or inconsistent conditions where presented with entirely new words either in ABB or ABA format (e.g. “wo fe fe” or “wo fe wo”). <br />
<strong>Results:</strong> Infants looked longer at flashing light during inconsistent sentences in test period ( e.g. train “ABB”, test “ABA”) compared to consistent.<br />
<strong>Discussion:</strong> This indicates rule learning in infants, cannot be explained by statistical learning of transition probabilities because test set consisted entirely of new words (no estimate of transition probability).  Requires extraction of relationships: are these two entities the same?  </p>
  </li>
  <li>
    <p>Can generalize relationships over identities of objects in the world - variable abstraction.</p>
  </li>
  <li>
    <p>Introspection: our own capacities for abstraction and transfer suggest the importance of symbolic processing (Buchheit, 1999).</p>
  </li>
  <li>
    <p>Allows rich, intuitive specification of background knowledge: facilitating learning to new tasks (via bootstrapping from previously learned experiences): using a key on a lock is not dependent on the particular task at hand: if we learn through trial and error the abstract transition model $ T(isLocked(lock) = true, unlocks(lock,key) = true, useKeyOnLock(lock,key)) \rightarrow  isLocked(lock) = false $, then we can utilize that information in any domain that uses keys and locks.  While this allows for humans to specify background knowledge to an artificial agent by inserting that definite clause into the initial transition structure for a new RL problem, the more satisying use case is one where an agent builds up its own set of abstract relations and takes a library of relations into each new task.   </p>
  </li>
  <li>
    <p>Once rules have been learned, acting with them is a well-studied research problem (Pasula, Zettlemoyer, and Kaelbling, 2004) allowing for one-shot policy adjustment given novel domain configurations (e.g. a previously blocked path is now open).</p>
  </li>
  <li>
    <p>Propositional logic isn’t sufficient, need first-order a.k.a. relational representations: chess can be propositionalized as follows via propositions as follows: <br />
<code>NumberOfBlackPawnsIsNotFive</code> and <code>whiteKingOnSameLineAsBlackKing</code>, but requires fixed number of objects, proposition to be constructed for all possible relations between all objects.  Compare to the relational representation: <code>sameLine(blackKing,whiteKing)</code>.  The effect of this being that we cannot easily generalize over objects or similar situations (Van Otterlo 2005). See Bongard problem for another example where relational representation is much more natural than propositional.  <strong>Take home:</strong> the allure of first-order logic is not simply that there exist theorem provers and goal regression (which is also a property of propositional logic), but the fact that it explicitly provides a mechanism for relationships between entities.   </p>
  </li>
  <li>
    <p>Lookup tables or propositional representations aren’t able to represent structural aspects of states and actions in relational domains such as Block World.   </p>
  </li>
  <li>
    <p>A representation should enable representing and reasoning about <em>objects</em> (Kaelbling et al., 2001); one has to be be able to represent objects and relations in our language if an intensional stance is taken (Dennett, 1987).</p>
  </li>
</ol>

<h2 id="relational-learning">Relational Learning</h2>

<p>Learning in first-order logic can be broken into two categories: <br />
1. <strong>parameter learning:</strong> assuming that we’re given a set of definite clauses, let’s learn the parameter values (i.e. probabilities).<br />
2. __ structure learning:__ let’s learn both the definite clauses and the associated parameters (using refinement and generalization operators to maximize).   </p>

<h2 id="what-is-relational-reinforcement-learning">What is relational reinforcement learning?</h2>

<p>Relational reinforcement learning hinges on upgrading representations of the individual components of an MDP e.g. states, transition function, etc.  There are various relational representations with which the components of an MDP can be upgraded: first-order logic (instantiated as stochastic context-free grammars or probabilistic relational models), graph-based relational representations.  Here we’ll stick with a probabilistic first-order logic:</p>

<p>Formally, given a first-order probabilistic logic $ \Lambda $ (e.g. <em>inductive logic programming</em>), a hypothesis $ \Upsilon \in \Lambda $, predicates $p$, constants $c$, a special set of predicates $A$, then we can define the MDP as:<br />
  $ S: {s \in HB^{P \cap C} | s \models \Upsilon } $ <br />
  $ A: {a \in HB^{A \cap C} | a \models \Upsilon } $<br />
  $ T: S x A \longmapsto S $ <br />
  $ R: S x A \longmapsto R $ <br />
  $ HB = \text{Herbrand Base} $ <br />
  $ \Upsilon $ defines which states are possible in the current domain<br />
  $ \models $ means is an interpretation of   </p>

<h4 id="example-relational-mdp-definition-of-states-and-actions">Example relational MDP definition of states and actions</h4>
<p>$ P = {on/2, clear/1} $<br />
$ C = a,b,c,d,e, floor $<br />
$ A = {move/2} $</p>

<p>This defines the set of all possible states by filling in each possible constant into each possible slot of the predicates such that $ \mid S\mid = 501 $ legal states</p>

<p>Holy Moley Batman, that’s a lot of states!  If $\mid C\mid = k$ and the arity (number of arguments accepted by a predicate) is $ \alpha_p $, then the total number of possible interpretations is $ \prod_p 2^{k^{\alpha_p}} \implies $ <strong>intractable</strong>.  </p>

<p>Clearly, like many other state representations, we are in dire need of abstraction.  This is done by using <em>ungrounded</em> predicates for learning a transition structure or policy, that is, predicates where the arguments are populated entirely or partially by variables.  </p>

<h4 id="example-abstract-policy">Example abstract policy</h4>
<p>Assuming the current goal is to place object a on object b, here is an abstract policy utilizing the decision list construction where we follow the rules sequentially and apply the first rule that matches: <br />
  $ r<em>0: on(a,b) $ $\rightarrow \text{no move} $  <br />
  $ r</em>1: onTop(X,b) \rightarrow move(X,floor) $  <br />
  $ r<em>2: onTop(X,a) \rightarrow move(X, floor) $  <br />
  $ r</em>3: clear(a), clear(b) \rightarrow move(a,b) $   </p>

<p><strong>Note:</strong> It is important that any ungrounded terms (i.e. <em>variables</em>) that appear on the lefthand side of a rule, also appear on the righthand side.   </p>

<h2 id="model-free-relational-rl">Model-free relational RL:</h2>

<h4 id="q-value">Q-Value</h4>
<p>The general idea is to use relational regression in order to approximate q-value function.  This has yielded things like:<br />
    k-nearest neighbor: store examples, predict new q-values using distance-weighted average (requires defining distance over relational states) - also called relational instance-based regression (I think)   <br />
    kernel-based regression using Gaussian processes: pretty similar to above, requires defining kernel over graphs or using convolution kernel, two benefits: can control generalization via covariance function smoothing parameter and since it’s embedded in a statistical framework, can give confidence of q-values which can be used to guide exploration using any of a bunch of algorithms such as upper-confidence bound, etc. <br />
    learning decision list: use relational regression to learn <em>grounded</em> Q-value function, calculate optimal <em>grounded</em> policy, feed optimal ground policy to decision list learner to produce <em>abstract</em> policy, keep both grounded and abstract policy around, use abstract policy to guide learning via generalization <br />
    TG relational regression:  incrementally build first-order trees; relational MDP specification defines which predicates are posssible, TG algorithm incrementally adds them to a regression tree as needed  </p>

<h4 id="approximate-policy-iteration">Approximate Policy Iteration</h4>

<h4 id="relational-naive-bayes">Relational Naive Bayes</h4>

<h4 id="q-larc">Q-LARC:</h4>
<p>feature construction based on predictive features   </p>

<h2 id="model-based-relational-rl">Model-based relational RL:</h2>

<p>Idea: Translate Bellman Backup Operator into relational domain.  Requires finding “preconditions” i.e. what states could lead to this state (this is called <em>regression), then a _combination</em> operator, and a <em>maximization</em> operator to define relational Bellman operator.   Defines new abstract policy from old abstract policy.  This is called <strong>decision-theoretic regression</strong>.  See explanation-based reinforcement learning.</p>

<h2 id="applications-of-relational-reinforcement-learning">Applications of relational reinforcement learning</h2>

<h4 id="learning-grounded-relational-symbols-from-continuous-data-for-abstract-reasoning">Learning Grounded Relational Symbols from Continuous Data for Abstract Reasoning</h4>

<h4 id="learning-probabilistic-relational-planning-rules">Learning Probabilistic Relational Planning Rules</h4>

<p><strong>learnRules:</strong> performs a search through a rule set using ILP operators (i.e. refinement and generalization operators)<br />
<strong>induceOutcomes:</strong>  finds a best set of outcomes given a context and an action <br />
<strong>learnParameters:</strong> learns a probability distribution over a set of outcomes  </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Probabilistic Logic Learning - Luc De Raedt and Kristian Kersting]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/08/05/probabilistic-logic-learning-luc-de-raedt-and-kristian-kersting/"/>
    <updated>2014-08-05T12:51:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/08/05/probabilistic-logic-learning-luc-de-raedt-and-kristian-kersting</id>
    <content type="html"><![CDATA[<h4 id="what-does-probabilistic-logic-learning-mean">What does probabilistic logic learning mean?</h4>
<p><strong>probabilistic</strong> - probabilistic representations and reasoning mechanisms <br />
<strong>logic</strong> - first order logical and relational representations (compared to propositional logic affords reasoning about objects)</p>

<h3 id="logic-glossary">Logic Glossary</h3>
<p>first-order logic $\leftrightarrow$ relational representation <br />
allows for:<br />
<strong>extensional</strong>: <code>ram(r1,c1)</code> – </p>
]]></content>
  </entry>
  
</feed>
