<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: hide | Something Witty]]></title>
  <link href="http://ElPiloto.github.io/blog/categories/hide/atom.xml" rel="self"/>
  <link href="http://ElPiloto.github.io/"/>
  <updated>2014-10-06T13:24:57-04:00</updated>
  <id>http://ElPiloto.github.io/</id>
  <author>
    <name><![CDATA[Luis R. Piloto]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Sleep EEG: LOSO (post-bug) and ERP]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/10/06/sleep-eeg-loso-post-bug-and-erp/"/>
    <updated>2014-10-06T10:29:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/10/06/sleep-eeg-loso-post-bug-and-erp</id>
    <content type="html"><![CDATA[<h3 id="as-described-in-a-hrefhttpelpilotogithubioblog20140925sleep-eeg-replicating-best-p-val-resultsthis-posta-there-was-a-bug-that-messed-things-up-for-the-results-posted-a-hrefhttpelpilotogithubioblog20140904sleep-eeg-finding-optimal-wake-pattern-for-sleep-transformherea">As described in <a href="http://ElPiloto.github.io/blog/2014/09/25/sleep-eeg-replicating-best-p-val-results/">this post</a>, there was a bug that messed things up for the results posted <a href="http://ElPiloto.github.io/blog/2014/09/04/sleep-eeg-finding-optimal-wake-pattern-for-sleep-transform/">here</a></h3>

<p>In general, we were producing a non-random, though erroneous, transformation to our data so now that that bug is fixed we should be getting pretty similar results just along different times and frequencies.   </p>

<h4 id="nothing-has-changed-for-the-optimal-lambda-choice">Nothing has changed for the optimal lambda choice:</h4>
<p>Still seems pretty close between the two, but let’s stick with <a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/LOSO_WAKE_LOGREG_POST_BUG_face_sceneAUC_lambda10.png">$\lambda = 10$</a> over <a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/LOSO_WAKE_LOGREG_POST_BUG_face_sceneAUC_lambda50.png">$\lambda = 50$
</a></p>

<h4 id="looking-for-best-timebin-and-frequency">Looking for best timebin and frequency:</h4>
<p>previously we liked <strong>300 ms</strong> time bin and were slightly frequency agnostic based on <a href="http://ElPiloto.github.io/images/research/LOSO_sleep_transform/LOSO_WAKE_LOGREG_SWEEP_PVAL_face_sceneAUC_lambda10.png">this</a>, but now we get totally different looking results:</p>

<p><a href='/images/research/LOSO_POSTBUG/LOSO_WAKE_LOGREG_POST_BUGPVAL_face_sceneAUC_lambda10.png' target='_blank'><img src="/images/research/LOSO_POSTBUG/LOSO_WAKE_LOGREG_POST_BUGPVAL_face_sceneAUC_lambda10.png" width="700" height="350"></a></p>

<p><strong>Thoughts:</strong> Cool things: we’re getting something around when you’d expect to see the N170 for faces!  We were getting hints of this when we were doing individual subject classification, but it seems that this is the only thing from the <a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/wake_hilb_mean_across_subjects.png">subject-specific classification</a> that comes through in the LOSO analysis (although we also get other strongly classifiable time bins in the LOSO analsis, it’s just that those didn’t pop out in the subject-specific classification results).  <strong>SO</strong>, I say let’s generate transformed sleep data plotaccording to four or five different patterns: 
	- something in the 4 Hz, 125-275 ms timebin range
	- something in the 4 Hz, 450-600 ms timebin range
	- something in the 4 Hz, 725-925 ms timebin range
	- something in the 8 Hz, 100-175 ms timebin range 
	- something in the 8 Hz, 275-325 ms timebin range   </p>

<p>This will be A LOT of plots too look at, but if we see something we can always check on James’ second batch of spindle-cued subjects (NOTE: given that have up to 500ms of untarnished sleep data with James’ second batch of spindle-cued subjects, we might want to only look at results before 500 ms)</p>

<h4 id="lets-look-at-the-recursive-feature-analysis-results-for-lambda--10">Let’s look at the recursive feature analysis results for $\lambda = 10$</h4>
<p>And see what’s the best we get across the different time bins for the 4 Hz and 8 Hz frequencies.</p>

<h4 id="additionally-lets-look-at-wake-loso-classification-using-just-erps">Additionally, let’s look at wake LOSO classification using just ERPs:</h4>
<p>Well, this is awkward: our best classification results are for using the ERPs: 68% accuracy.</p>

<p><a href='/images/research/LOSO_POSTBUG/LOSO_WAKE_LOGREG_ERP25_face_sceneAUC_lambda10.png' target='_blank'><img src="/images/research/LOSO_POSTBUG/LOSO_WAKE_LOGREG_ERP25_face_sceneAUC_lambda10.png" width="700" height="350"></a></p>

<h4 id="additionally-we-get-even-better-results-for-when-we-do-recursive-feature-elimintation">Additionally. we get even better results for when we do recursive feature elimintation:</h4>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[(PDP) Dan Yamins: Object recognition]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/10/03/dan-yamin-object-recognition/"/>
    <updated>2014-10-03T10:34:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/10/03/dan-yamin-object-recognition</id>
    <content type="html"><![CDATA[<p>Computation models of vision</p>

<p>challenging computational problems:</p>

<p>view: position, size, pose and illumination</p>

<p>clutter and occlusion</p>

<p>distortion and noise</p>

<p>object category hierarchy: basic categories + individual identification of category members (category = car, identify honda civic, toyota corolla, etc)</p>

<p>pixel –&gt; RGC –&gt; LGN –&gt; V1 –&gt; V2 –&gt; V4 –&gt; IT (at some point, task has been solved)</p>

<p>high-variation test image set:</p>

<p>objects on complex background (64 objects, 8 from 8 categories), presented on uncorrelated realistic backgrounds</p>

<p>create images with different degrees of variation: 
low-variation: size/pose/position/illumination are normalized
high-variation: size/pose/position/illumination can vary pretty widely</p>

<p>subsequently recorded from macaque V4 and IT: 300 units recorded</p>

<p>output = binned spike counts in 70ms - 170ms, averaged over 25-50 reps of each image</p>

<p>trained linear classifier on recordings from IT and V4 and varied variation levels of images, found that IT-based classifier basically tracks human performance   </p>

<p>Lower areas (RGC, LGN, V1) have been reaosnably explained by models (~50% explained variance)   </p>

<p><strong>general idea:</strong> start off with code in early cortex, eventually gets transformed to a linearly decodable code, let’s try to see what transformation supports that</p>

<p>Modeling: look at performance (can do object recognition tasks) and neural predictivity (do individual layers of model correlate with corresponding layers of cortex?)   </p>

<p>Instead, optimize for performance, and then <em>exo facto</em> look at how it predicts neural responses   </p>

<p>Architectural Parameters: Hierarchical Convolutional Networks</p>

<p>filter, theshold and saturate, pool, and normalize</p>

<p>filter: kernel size, how many filters
pooling: what kind of pooling
normalizing: do we normalize at all</p>

<p>neural predictivity: predict each unit as a linear combination of model outputs (for a given layer)</p>

<p>HT experiments: tried 3 methods of training hiearchical convolutional neural network</p>

<ol>
  <li>random selection of models;  look at neural predictivity </li>
  <li>optimizie models for performance; look at neural predictivity</li>
  <li>optimize for IT predictivity;</li>
</ol>

<p>look at performance on task vs. IT predictivity for 1 through 3  <br />
method 3 didn’t do much better than method 2    </p>

<p>but more than anything: any of the models aren’t doing that well looking at performance vs IT predictivity, so how do we improve performance?   </p>

<p>try boosting (hierarchical modular optimization): some architectures are good at individual object recognition sub-tasks (faces vs. boats, buildings vs. flowers, etc) so let’s try to combine them,   </p>

<p>their hypothesis: ability to perform sub-tasks (even though they’re on a different trainig data set, specifically different categories and simpler background), finds a basis set with which a combined model can perform maximally</p>

<p>this gets up to 50-65% variance explained of IT, next question V4 to model layer fit isn’t great - but actually intermediate layer in model “model V3” explains V4 - so what are the transformations that happen there? still trying to figure it out, suggestion by jon cohen: “statistical topology”   </p>

<p>STATISTICAL TOPOLOGICAL DATA ANALYSIS USING PERSISTENCE LANDSCAPES</p>

<p>Statistical topology</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Norman Lab Meeting: Victoria Jackson-Hanen]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/10/02/norman-lab-meeting-victoria-jackson-hanen/"/>
    <updated>2014-10-02T12:33:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/10/02/norman-lab-meeting-victoria-jackson-hanen</id>
    <content type="html"><![CDATA[<h2 id="the-role-of-competition-in-the-differentiation-of-visual-memories">The role of competition in the differentiation of visual memories</h2>

<p>Misha and Danya (two half-brothers), look similar, don’t see them often  </p>

<p>Retrieval Induced Forgetting: think about Misha (RP+), perhaps forget Danya (RP-)   </p>

<p>REV-RIF: if you are allowed an opportunity to restudy an RP-, no longer activate (usurped, previously shared RP+) nodes, creates pattern separation with increased memory for REV-RIF</p>

<pre>
RIF:

RP-: --|    
NP : ----|    
RP+: ------|    
     memory (NRP = no retrieval practice, still same number of exposures, but don't actually have to retrieve)
</pre>

<p>Question: why is there a set level of activated nodes? i.e. why should the RP- pickup new nodes?   </p>

<h3 id="how-does-rev-rif-change-visual-features-of-memories">How does REV-RIF change visual features of memories?</h3>

<p>Similar shapes with similar names, how does this REV-RIF affect perception of color presentation?   </p>

<p>Study all items, retrieval practice (some items), restudy all items (REV-RIF effect here), test.    </p>

<p>Train pairs within same family name (e.g. advo and adva), only showing family prefix:</p>

<p>present in black and white:
       weird character 1 + weird character name 2
                    <strong>__ADV</strong><em>__</em>         &lt;– family prefix</p>

<p>SCANNER: <br />
study 1: present individual character + full name:   <br />
				weird character 1   <br />
                <strong>____ADVO</strong><strong>___   <br />
                weird character 7
                __</strong><strong>MAGY</strong>_____   </p>

<p>scanner 2: competitive trial for RP+ items, non-compettitive trial for RP- and Nrp items    </p>

<p>Idea: RP+ items should </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG: Replicating best p-val results]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/09/25/sleep-eeg-replicating-best-p-val-results/"/>
    <updated>2014-09-25T14:51:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/09/25/sleep-eeg-replicating-best-p-val-results</id>
    <content type="html"><![CDATA[<h3 id="we-are-trying-to-replicate-these-results-below">We are trying to replicate these results below:</h3>

<h4 id="so-far-ive-tried">So far I’ve tried:</h4>
<ul>
  <li>Running the same exact analysis (driver_LOSO_replicate_PVAL.m)   </li>
  <li>Running the same exact analysis but only classifying between faces and scenes via:
<code>preprocessing.type_specifics_args.classes = [1 2];</code>    <br />
    <ul>
      <li>this did not help   </li>
    </ul>
  </li>
  <li>Try loading up results from Sept 09, and regenerating plots - will indicate whether or not something changed in plots or results. <br />
    <ul>
      <li>Doing this, I’ve found that the results of interest are specifically in this file:  <br />
<code> 09-Sep-2014_14_04_44LOSO_WAKE_logreg_sweep.mat </code>   </li>
      <li>I’ve found that it is in fact for $ \lambda = 10 $ (I was worried I might have accidentally documented the wrong value of $\lambda$)       </li>
      <li>We were classifying against all 5 classes (validating our findings above that classifying only faces vs scenes did not reproduce the results)   </li>
    </ul>
  </li>
  <li>Try duplicating exact time range used originally, perhaps there is some bug in my code <br />
    <ul>
      <li><strong>BAD NEWS:</strong> this reproduced the results, meaning there has to be some BUG in my code where I reshape the data, append subject data together, OR iterate through individual timebins and frequencies. This is why it would be nice to have the time to create some simulated data.</li>
    </ul>
  </li>
</ul>

<h3 id="fixing-the-damn-bug">FIXING THE DAMN BUG</h3>
<p>Relevant files:  </p>

<p>So far, I’ve ensured that when I reshape and permute the subject’s data after loading it (<code>get_preprocessed_subj_data.m, Line: 128 </code>), that I have not messed up the ordering of the data.</p>

<p>Next: when I filter timebins out, do I mess that process up?  This also checks out(<code>get_preprocessed_subj_data.m, Line: 60 </code>)   </p>

<p>Next: what about when I call <code>iterate_over_data</code>?  The data was misordered here, so somewhere between <code>get_preprocessed_subj_data</code> and <code>iterate_over_data</code> we messed things up.</p>

<p>Next: let’s check that the anonymous function that is used for reshaping the data (after it’s been flattened) is correct, this gets used in a few places.  YEP, this is the problem: someone please kindly shoot me in the face.   </p>

<h3 id="bug-the-sequel-discrepancy-between-the-recursive-feature-elimination-results-and-logistic-regression-results">BUG, The Sequel: DISCREPANCY BETWEEN THE RECURSIVE FEATURE ELIMINATION RESULTS AND LOGISTIC REGRESSION RESULTS</h3>
<p>I’ve verified that there is no discrepancy between the accumulated values and the plotted values, so it does not seem to be the case that the plotting function is messing things up.   </p>

<p>Recursive feature elimination (RFE) analysis has a setting for the minimum number of features i.e. remove features until we hit some minimum number of features.  In general, this is set to 1, so that we can see the effect of removing 0 to all but one features (electrodes).  I have found that if I set that value to 60 (only removing 4 features), then the RFE results align with the logistic regression results, however, if I set that to 1 or even just 50, the results stop lining up.  Very, very perplexing.    </p>

<p>Next, let’s look at the features used for the first and second iteration with different settings (60 and 50) of the minimum number of features.  If they differ in any way, then the code for removing features must be incorrect and we can delve into that.<br />
   - So for the second iteration, the fourth frequency, the RFE results have different features they’ve removed.  WTF, mate?   A reasonable next step would be to look at the values passed into the <code>remove_unused_features</code> for both parameter settings and see if they are the same.</p>

<p>Looking at the classifier weights for the first iteration of the fourth frequency for both parameter settings, we see that they have different values, so naturally removing features would produce different removed features for the second iteration.  Additionally, this implies there is something different being fed into the classifier for the first iteration that is producing different feature weights.</p>

<p><strong>HERE IT IS:</strong>  Currently, we’ve been doing an RFE iteration (going from 64 features down to minimum num_features) for each frequency bin (2 Hz all the way up to 32 Hz).  To maintain the correct level of regularization, we take the max number of features (64) and look at the ratio of regularization (usually 10) to the number of features and keep this proportional as we remove more and more features.  The bug was that between RFE iterations (i.e. starting anew at 4 Hz, after doing 2 Hz), the regularization ratio was calculating using the previous iteration’s last number of features (not the max number of features) - so as we moved from 2 Hz to 32 Hz, our results got more and more skewed from the regular logistic regression results because we were using much smaller regularization.  This is fixed now!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG: James Visit Recap and Brainstorm]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/09/18/sleep-eeg-james-visit-recap-and-brainstorm/"/>
    <updated>2014-09-18T14:49:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/09/18/sleep-eeg-james-visit-recap-and-brainstorm</id>
    <content type="html"><![CDATA[<h4 id="regarding-this-our-best-result-during-sleep-is-058---but-were-messed-up-by-multiple-comparisons">Regarding this, our best result, during sleep, is 0.58 - but we’re messed up by multiple comparisons</h4>
<p>TODO: we can use our a priori best itmebin and frequency to analyze James’ new dataset (timebin 175ms, freq 16)</p>

<p>Cephalapod L1 vs L2</p>

<p>http://elpiloto.github.io/images/research/sleep_eeg_hilbert/sleep_hilb_mean_across_subjects_lambda1.png</p>

<ul>
  <li>
    <p>negative peak of a slow-wave, phase of 0.6 - 1.2 Hz</p>
  </li>
  <li>
    <p>how similar are frequency band patterns to each other?</p>
  </li>
</ul>

<p>N170 is really sharp, maybe at 8 Hz and 225ms, large over Cz sometimes (doesn’t make a lot of sense anatomically - you’d expect it to be over occipital)</p>

<p>once we get the template, we want to inspect them, although it’s hard to know what to expect</p>
]]></content>
  </entry>
  
</feed>
