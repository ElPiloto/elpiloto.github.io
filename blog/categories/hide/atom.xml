<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: hide | Something Witty]]></title>
  <link href="http://ElPiloto.github.io/blog/categories/hide/atom.xml" rel="self"/>
  <link href="http://ElPiloto.github.io/"/>
  <updated>2014-10-30T10:52:03-04:00</updated>
  <id>http://ElPiloto.github.io/</id>
  <author>
    <name><![CDATA[Luis R. Piloto]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[ZNN: Cpu-based Conv Nets]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/10/20/znn-cpu-based-conv-nets/"/>
    <updated>2014-10-20T12:08:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/10/20/znn-cpu-based-conv-nets</id>
    <content type="html"><![CDATA[<h1 id="znn">ZNN</h1>

<h3 id="motivation">Motivation</h3>
<ul>
  <li>not that many neural network implementations utilize multi-cpu</li>
  <li>o</li>
</ul>

<h2 id="convoluational-neural-net-training">Convoluational Neural Net Training</h2>

<h4 id="forward-pass">Forward Pass:</h4>
<p><em>Direct:</em>
- each edge is a filter
- convolve f &amp; w
<em>FFT Method:</em>
- FFT of image
- FFT of filter
- Multiple
- inverse FFT</p>

<h4 id="backward-pass">Backward Pass:</h4>
<p><em>Direct:</em>   <br />
calculate G   <br />
convolve g and w   <br />
<em>FFT Method:</em>   <br />
- calculate FFT of G   <br />
- FFT of W (already calculcated during forward pass)   <br />
- inverse FFT     </p>

<h4 id="updating-weights">Updating Weights</h4>
<p><em>Direct:</em>   <br />
- calculate $\frac{dE}{dB}$ and update the biases  <br />
- convolve F(ilter) &amp; G’ and update W   <br />
<em>FFT:</em>   <br />
- calculate $\frac{dE}{dB}$ and update the biases   <br />
- IFFT(FFT(F),FFT(G)), update W     </p>

<h2 id="parallel-models--data-vs-thread-vs-task-model">Parallel Models:  Data vs. Thread vs. Task Model</h2>

<p><strong>Data Model:</strong>
- parallelize a single operation (convolution)  <br />
- example: parallel loops   <br />
<strong>Thread Model:</strong>  <br />
- each thread has its own duties    <br />
- thread communicate with signals/messages   <br />
<strong>Task Model:</strong>  <br />
- taks is an abstract concept - a function <br />
- CPUs pick up tasks that are ready to be executed  <br />
- global queue - can be prioritized  <br />
- tasks can be stolen   </p>

<h2 id="znns-task-model">ZNN’s Task Model</h2>
<p><strong>Prioritized Task Model</strong>
<strong>Scheduling strategy</strong>
- priority based on the numbder of tasks depending on the current task’s completion
<strong>Stealing tasks</strong>     </p>

<h2 id="general-idea-for-parallelizing-forward-pass">General Idea for parallelizing forward pass:</h2>
<p>Let’s look at dependencies (for instance, need to load a training example before you’ll need to FFT it, maybe even before you FFT individual filters, etc) and to grab job’s as needed.  <br />
- Ultimately this leads to the only bottleneck being waiting for the output peceptron    </p>

<h2 id="general-idea-for-parallelizing-backward-pass">General Idea for parallelizing backward pass:</h2>
<ul>
  <li>can update weights as you do backward pass   </li>
</ul>

<h1 id="how-to-znn">How to Znn?</h1>

<p>Three input files:  <br />
1.  Network spec  <br />
2. Data spec  <br />
3. Training Options     </p>

<h4 id="network-spec">Network Spec:</h4>
<p>Can be specified as a graph:  <br />
	- node groups and edge groups  <br />
[C1]    % unique name of node group
size=5   % number of nodes
activation=tanh
act_params=1.712,0.666
% mass pooling operation(only support max pooling, atm)
filter=max
filter_size=2,2,1
filter_stride=2,2,1
% weight/bias initialization
init_type=Uniform
init_params=0.05
bias=0
% learning
eta=0.01
mom=0.9
wc=0
% fft switch - unnecessary
fft=0</p>

<p>Edgegroup spec:
[SOURCENODE_TARGETNODEGROUP] 
size=4,4,1
% more initialization
init_type=Uniform
init_params=0.05</p>

<h4 id="data-spec">Data Spec:</h4>
<p>[INPUT1]
path=
ext=image
size=512,512,30
pptype=standard2D %preprocessing -</p>

<p>[LABEL1]</p>

<h4 id="training-options">Training Options:</h4>
<p>[PATH]
config=./networks/N3.spec
load= (can load previously saved network instances)
data= DATA_SPEC_FILE
save = SAVE_FILE
[OPTIMIZE]
n_threads=64
force_fft=0 % force all edge groups to use FFT
optimize_fft=1
[TRAIN]
train_range=1
test_range=2
outsz=100,100,1
dp_type=volume % data provider types - only allows volume currently   <br />
cost_fn=cross_entropy   <br />
cost_fn_param=0    <br />
data_aug=1 % randomly transform input data (random rotation, random flip - each data provider is responsible for implementing its own data augmentation logic)   <br />
clas_thresh=0.5 % classification threshold   <br />
softmax=1 % apply softmax at output layer  <br />
[UPDATE]
force_eta=0.01
momentum=0.9
wc=0
anneal_factor=0.997
anneal_freq
[MONITOR] % saves current network instance
n_iters=100000
check_freq=10
test_freq=100
test_samples=10</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG: Pre-SFN Push]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/10/13/sleep-eeg-pre-sfn-push/"/>
    <updated>2014-10-13T17:17:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/10/13/sleep-eeg-pre-sfn-push</id>
    <content type="html"><![CDATA[<p>{% img /images/research/LOSO_POSTBUG/WAKE_ERP_TRANSFORM_SLEEP_ERP_AVG.jpg 700 350 %}    </p>

<h3 id="with-rescaling">WITH rescaling</h3>
<p>{% img /images/research/LOSO_POSTBUG/RESCALING_WAKE_LOSO_ERP_ZSCORE<em>175ms.png 700 350 %}  <br />
{% img /images/research/LOSO_POSTBUG/ERP_WAKE_LOSO_RESCALING_SPATIAL_TEMPLATES</em>175ms.jpg 700 350 %}    </p>

<h3 id="without-rescaling">WITHOUT rescaling</h3>
<p>{% img /images/research/LOSO_POSTBUG/WITHOUT_RESCALING_WAKE_LOSO_ERP_ZSCORE<em>175ms.png 700 350 %}  <br />
{% img /images/research/LOSO_POSTBUG/ERP_WAKE_LOSO_NORESCALING_SPATIAL_TEMPLATES</em>175ms.jpg 700 350 %}    </p>

<h1 id="second-batch-of-subjects">Second Batch Of Subjects:</h1>

<h2 id="things-that-are-the-same">Things that are the same:</h2>

<h4 id="wake-loso-timebin-freq-sweep">Wake LOSO Timebin-Freq Sweep</h4>

<p>These two guys seem pretty similar (the top is the 2nd batch of subjects) </p>

<p>{% img /images/research/LOSO_POSTBUG/LOSO_WAKE_LOGREG_POST_BUG_2nd_subjects_face_sceneAUC_lambda10.png 700 350 %}    </p>

<p>{% img /images/research/LOSO_POSTBUG/LOSO_WAKE_LOGREG_POST_BUG_face_sceneAUC_lambda10.png 700 350 %}    </p>

<h4 id="wake-loso-erp-classification">Wake LOSO ERP Classification</h4>

<p><strong>2nd Batch Subjects</strong>
{% img /images/research/LOSO_POSTBUG/2nd_batch_LOSO_WAKE_LOGREG_ERP25_face_sceneAUC_lambda10.png 700 350 %}     <br />
<strong>1st Batch Subjects</strong>   <br />
{% img /images/research/LOSO_POSTBUG/LOSO_WAKE_LOGREG_ERP25_face_sceneAUC_lambda10.png 700 350 %}       </p>

<h2 id="things-that-are-not-the-same">Things that are NOT the same:</h2>

<h4 id="sleep-loso-timebin-freq-sweep">Sleep LOSO Timebin-Freq Sweep</h4>
<p><strong>2nd Batch Subjects</strong>
{% img /images/research/LOSO_POSTBUG/SLEEP_LOSO_LOGREG_2nd_batch_subjects_face_sceneAUC_lambda10.png 700 350 %}     <br />
<strong>1st Batch Subjects</strong>   <br />
{% img /images/research/LOSO_POSTBUG/SLEEP_LOSO_LOGREG_face_sceneAUC_lambda10.png 700 350 %}       </p>

<h4 id="sleep-loso-erp-classification">Sleep LOSO ERP Classification</h4>
<p><strong>2nd Batch Subjects</strong>
{% img /images/research/LOSO_POSTBUG/2nd_batch_LOSO_SLEEP_LOGREG_ERP25_face_sceneAUC_lambda10.png 700 350 %}     <br />
<strong>1st Batch Subjects</strong>   <br />
{% img /images/research/LOSO_POSTBUG/1st_batch_LOSO_SLEEP_LOGREG_ERP25_face_sceneAUC_lambda10.png 700 350 %}       </p>

<h2 id="sleep-classify-spindleboost-trials-only">Sleep Classify SpindleBoost Trials Only</h2>
<p><strong>2nd Batch Subjects</strong>
{% img /images/research/LOSO_POSTBUG/SPINDLE_TRIALS_SLEEP_LOSO_LOGREG_2nd_batch_subjects_face_sceneAUC_lambda10.png 700 350 %}       </p>

<h2 id="classify-erp-4-ms">Classify ERP 4 ms</h2>
<p>#### WAKE   <br />
<strong>2nd Batch Subjects</strong>
{% img /images/research/LOSO_POSTBUG/2nd_batch_LOSO_WAKE_LOGREG_ERP4<em>2seconds_face_sceneAUC_lambda10.png 700 350 %}     <br />
<strong>1st Batch Subjects</strong>   <br />
{% img /images/research/LOSO_POSTBUG/1st_batch_LOSO_WAKE_LOGREG_ERP4</em>2seconds_face_sceneAUC_lambda10.png 700 350 %}       </p>

<h4 id="sleep">SLEEP</h4>
<p><strong>2nd Batch Subjects</strong>
{% img /images/research/LOSO_POSTBUG/2nd_batch_LOSO_SLEEP_LOGREG_ERP4<em>2seconds_face_sceneAUC_lambda10.png 700 350 %}     <br />
<strong>1st Batch Subjects</strong>   <br />
{% img /images/research/LOSO_POSTBUG/1st_batch_LOSO_SLEEP_LOGREG_ERP4</em>2seconds_face_sceneAUC_lambda10.png 700 350 %}       </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Howard Eichenbaum]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/10/09/howard-eichenbaum/"/>
    <updated>2014-10-09T16:34:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/10/09/howard-eichenbaum</id>
    <content type="html"><![CDATA[<h1 id="the-hippocampus-in-space-and-time">The hippocampus in space and time</h1>

<h2 id="lets-start-with-space">Let’s start with space</h2>
<p>How the crap do we reconcile memory and location functions of hippocampus?   </p>

<p>Two relevant pieces of evidence:  <br />
1. hippocampus essential for episodic memory (Vargha-Khadem , Science 1997)    </p>

<ol>
  <li>episodic memories are organized in space and time  (Tulving, Organization of Memory, 1972)    </li>
</ol>

<p><strong>Hypothesis</strong> hippocampus organizes memories in space and time   </p>

<p>context-guided object association:
key point: forces animal to organize memories spatially   </p>

<p>two connected rooms: <br />
- same pair of compounds “A and B” in each room; one compound is positive in one room, negative in the other and vice versa      </p>

<p>hippocampus neurons highly dimensional:   <br />
- have firing selective to item (compound) AND location     </p>

<h2 id="how-are-these-dimensions-organized-within-hippocampus">How are these dimensions organized within hippocampus?</h2>

<p>to do so, upscaled task:
some trials have same rooms, but different compounds “C and D” (with same positive/negative versions in each room)</p>

<p>can look at:
item identity (A v B v C v D) vs. valence (AC v BD) vs. pair (AB v CD)</p>

<p>Also, recording from ensembles of neurons (instead of looking for individual cells that are responsive) and perform multivariate representational similarity analysis    </p>

<p>position: same item, different item, change valence, change pair</p>

<h4 id="punchline">Punchline</h4>
<p>Look at RSA across different manipulations and find that representation is most sensitive in decreasing order to :context, position, valence, item     </p>

<p>Also looked at this hierarchy in orbitofrontal cortex and found that the hierarchy is swapped (see that it cares more about valence which is consistent with what we expect from OFC)   </p>

<h2 id="how-is-memory-space-managed">How is “memory space” managed?</h2>
<p>Must be some activity between PFC and (dorsal) hippocampus:</p>

<p><strong>Idea:</strong> record from hippocampus, inactivate PFC with muscimol, re-record from hippocampus  </p>

<p><strong>Results:</strong> found that cell that fire to item A and context 2, would subsequently fire to both item A and context 2     – although it seems like maybe they’re talking baout recording from mPFC?   </p>

<h3 id="what-is-the-origin-of-contextual-information-to-prefrontal-cortex">What is the origin of contextual information to prefrontal cortex?</h3>

<p>Perhaps this is the flow of communication
1. details –&gt; context generated from dorsal hippocampus –&gt; ventral
2. ventral hippocampus –&gt; PFC carrying contextual information
3. PFC –&gt; dorsal hippocampus to select relevant information</p>

<p>look at J. Neuro 2010 phase lag at theta between areas to see if we can determine flow of information <br />
found -103ms DHipp, -27ms VHipp, and mPFC 64ms    </p>

<h2 id="what-about-time">What about time?</h2>

<p>Do you need a hippocampus for memory for temporal order? (Fortin et al. Nature Neuro 2002)  <br />
Experiment: learn sequence of 5 stimuli, test: which of two stimuli came earlier? <br />
Look at behavior as a function of normal vs. hippocampal lesions
Also tested for recognition and found lesion animals not deficient - seems like hippocampus must play an important role in memory for temporal order    </p>

<h4 id="how-does-the-hippocampus-tell-time">How does the hippocampus tell time</h4>
<p>Kraus et al. Neuron, 2013 - figure 8 task, but confounded with position, so how do we isolate time?   <br />
Keep rat on treadmill, then find “time cells” which fire at a particular time point     </p>

<p>but could potentially confound time elapsed and distance, so vary treadmill rate and found in fact both time cells and distance cells   <br />
but actually found that there’s a distribution of cells which actually respond to some combination of time and distance, but the time-only and distance-only cells are at the tails of that distribution    </p>

<h2 id="general-knowledge">General Knowledge:</h2>
<p>ventral hippocampus: bigger place fields, generally take up whole context; develop a bit slower than dorsal hippocampus  <br />
dorsal hippocampus: more specific, smaller place fields; develop place field selectivity perfectly correlated with learning    <br />
monosynapse from ventral hippocampus to PFC   </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG: Sleep Transform Details And McDuff Trouble]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/10/08/sleep-eeg-sleep-transform-details-and-mcduff-trouble/"/>
    <updated>2014-10-08T17:28:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/10/08/sleep-eeg-sleep-transform-details-and-mcduff-trouble</id>
    <content type="html"><![CDATA[<h2 id="importance-maps">Importance Maps:</h2>

<p>It turns out that we have been re-scaling (the eeg_ana_toolbox traintest function) features when performing classification.  This is arguably an okay thing to do in general, BUT DEFINITELY NOT OKAY to do when calculating McDuff importance maps which rely on looking at the average pattern activation signs and the signs of the weights because the weights were generated for a rescaled (always between 0 and 1) pattern, so that it doesn’t make sense to look at the sign agreement between the two for calculating the McDuff importance maps.  </p>

<h2 id="generating-sleep-transformed-plots">Generating Sleep Transformed Plots:</h2>
<p>Since it has been working, for the purposes of generating a wake template, we’re going to use the RFE results generated by performing feature selection on the raw feature weights (instead of McDuff importance)</p>

<h4 id="additionally-there-was-some-concern-that-we-might-be-z-scoring-across-electrodes-after-each-rfe-iteration-but-really-thats-not-what-we-think-we-should-be-doing-and-luckily-not-what-were-doing-in-the-code">Additionally, there was some concern that we might be z-scoring across electrodes after each RFE iteration, but really that’s not what we think we should be doing and <em>luckily</em> <strong>not</strong> what we’re doing in the code!</h4>

<h4 id="we-have-a-preference-for-more-textured-templates-and-using-the-correlation-instead-of-the-dot-product-because-the-correlation-is-scale-invariant">We have a preference for more “textured” templates and using the correlation instead of the dot product because the correlation is scale-invariant</h4>

<h4 id="stucture">Stucture:</h4>
<p>We want to generate a “flipbook” for every pair of wake time-freq bin we choose (roughly 6) and each corresponding sleep frequency.  Given 5 frequency bands and 6 wake templates, we’ll end up with roughly 30 flipbooks.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG: LOSO (post-bug) and ERP]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/10/06/sleep-eeg-loso-post-bug-and-erp/"/>
    <updated>2014-10-06T10:29:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/10/06/sleep-eeg-loso-post-bug-and-erp</id>
    <content type="html"><![CDATA[<h3 id="as-described-in-a-hrefhttpelpilotogithubioblog20140925sleep-eeg-replicating-best-p-val-resultsthis-posta-there-was-a-bug-that-messed-things-up-for-the-results-posted-a-hrefhttpelpilotogithubioblog20140904sleep-eeg-finding-optimal-wake-pattern-for-sleep-transformherea">As described in <a href="http://ElPiloto.github.io/blog/2014/09/25/sleep-eeg-replicating-best-p-val-results/">this post</a>, there was a bug that messed things up for the results posted <a href="http://ElPiloto.github.io/blog/2014/09/04/sleep-eeg-finding-optimal-wake-pattern-for-sleep-transform/">here</a></h3>

<p>In general, we were producing a non-random, though erroneous, transformation to our data so now that that bug is fixed we should be getting pretty similar results just along different times and frequencies.   </p>

<h4 id="nothing-has-changed-for-the-optimal-lambda-choice">Nothing has changed for the optimal lambda choice:</h4>
<p>Still seems pretty close between the two, but let’s stick with <a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/LOSO_WAKE_LOGREG_POST_BUG_face_sceneAUC_lambda10.png">$\lambda = 10$</a> over <a href="http://ElPiloto.github.io/images/research/LOSO_POSTBUG/LOSO_WAKE_LOGREG_POST_BUG_face_sceneAUC_lambda50.png">$\lambda = 50$
</a></p>

<h4 id="looking-for-best-timebin-and-frequency">Looking for best timebin and frequency:</h4>
<p>previously we liked <strong>300 ms</strong> time bin and were slightly frequency agnostic based on <a href="http://ElPiloto.github.io/images/research/LOSO_sleep_transform/LOSO_WAKE_LOGREG_SWEEP_PVAL_face_sceneAUC_lambda10.png">this</a>, but now we get totally different looking results:</p>

<p>{% img /images/research/LOSO_POSTBUG/LOSO_WAKE_LOGREG_POST_BUGPVAL_face_sceneAUC_lambda10.png 700 350 %}</p>

<p><strong>Thoughts:</strong> Cool things: we’re getting something around when you’d expect to see the N170 for faces!  We were getting hints of this when we were doing individual subject classification, but it seems that this is the only thing from the <a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/wake_hilb_mean_across_subjects.png">subject-specific classification</a> that comes through in the LOSO analysis (although we also get other strongly classifiable time bins in the LOSO analsis, it’s just that those didn’t pop out in the subject-specific classification results).  <strong>SO</strong>, I say let’s generate transformed sleep data plotaccording to four or five different patterns: 
	- something in the 4 Hz, 125-275 ms timebin range
	- something in the 4 Hz, 450-600 ms timebin range
	- something in the 4 Hz, 725-925 ms timebin range
	- something in the 8 Hz, 100-175 ms timebin range 
	- something in the 8 Hz, 275-325 ms timebin range   </p>

<p>This will be A LOT of plots to look at, but if we see something we can always check on James’ second batch of spindle-cued subjects.</p>

<h4 id="lets-look-at-the-recursive-feature-analysis-results-for-lambda--10">Let’s look at the recursive feature analysis results for $\lambda = 10$</h4>
<p>And see what’s the best we get across the different time bins for the 4 Hz and 8 Hz frequencies at some points that we cared about: <del>- I’m working on presenting these in a better fashion but for now these are the plots that were most easily generated:</del>   </p>

<p>{% img /images/research/LOSO_POSTBUG/LOSO_RFE_BEST.png 700 350 %}   </p>

<h4 id="additionally-lets-look-at-wake-loso-classification-using-just-erps">Additionally, let’s look at wake LOSO classification using just ERPs:</h4>
<p>Well, this is awkward: our best classification results are for using the ERPs: 68% accuracy.</p>

<p>{% img /images/research/LOSO_POSTBUG/LOSO_WAKE_LOGREG_ERP25_face_sceneAUC_lambda10.png 700 350 %}</p>

<h4 id="lastly-lets-not-forget-that-we-wanted-to-look-at-16-hz-175-ms-in-the-sleep-data-because-we-did-well-with-it-on-an-individual-subject-level-as-seen-here">Lastly, let’s not forget that we wanted to look at 16 Hz, 175 ms in the sleep data because we did well with it on an individual subject level as seen here:</h4>

<p>{% img /images/research/sleep_eeg_hilbert/sleep_hilb_mean_across_subjects_lambda1.png 700 350 %}</p>

<p><strong>TODO:</strong></p>

<ul>
  <li>when picking number of electrodes to keep, err on the side of too many so that we get a varied distribution of importance weights (so that we can do correlation)</li>
  <li>generate sleep similarity plots BY wednesday!</li>
  <li>let’s look at RFE and ERP results, too and generate a template based on that as well!</li>
  <li>let’s do whatever we can to help James preprocess the validation subjects</li>
</ul>
]]></content>
  </entry>
  
</feed>
