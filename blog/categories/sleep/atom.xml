<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: sleep | Something Witty]]></title>
  <link href="http://ElPiloto.github.io/blog/categories/sleep/atom.xml" rel="self"/>
  <link href="http://ElPiloto.github.io/"/>
  <updated>2014-07-04T13:17:12-04:00</updated>
  <id>http://ElPiloto.github.io/</id>
  <author>
    <name><![CDATA[Luis R. Piloto]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Sleep EEG 9 Subjects]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/06/23/sleep-eeg-9-subjects/"/>
    <updated>2014-06-23T12:14:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/06/23/sleep-eeg-9-subjects</id>
    <content type="html"><![CDATA[<h2 id="all-the-interpretations">All the interpretations</h2>

<p>Hi Ken,  </p>

<p>Here’s the scoop.  We can definitely classify wake data (see Figure 1a below) The best time bin for the wake data isn’t consistent across all subjects, but it does seem to peak at 230ms, which is consistent with Ehren’s results ( compare figure 1b to figure 4: http://compmem.princeton.edu/publications/NewmanNorman10.pdf)  Our classification results aren’t as good as Ehren’s, he had a peak AUC of 0.7, but I’m not too worried about it - what do you think?  There might be some utility to looking at classification for slightly later timebins (although Ehren’s results do peak at 200ms so we might already be looking at the best time bin)   </p>

<p><strong>So can we classify sleep?</strong> 	</p>

<p>We tried three different sleep classification methods:<br />
1. Feature selection on all the frequencies within a particular time bin of sleep data.
2. Transform the sleep data using the average pattern of activity across all electrodes at a particular time-bin and frequency of the wake data.
3. Transform the sleep data using the importance map generated by the wake classification instead of the average pattern of activity - this should weight the different electrodes according to how informative they are for wake classification.  </p>

<p>For method 1, we’re not getting any consistently classifiable timebins across subjects (figure 3a and 3b) which would have been the nicest result.</p>

<p>The good news is that methods 2 and 3 show comparable classification AUC to method 1 and sometimes do even better.  Moreover, there isn’t a link between good classification for a time bin using method 1 compared to method 2 or 3 (e.g. method 1 classification for subject 2 doesn’t do so hot for timebins 152-230, but if you look at [this plot] ( /images/research/sleep_eeg<em>9_subjects</em>06<em>23</em>2014/sleep_xform_avg_pattern/sleep_transform_cv_auc_scene_subj2.png ) for method 2 classification for subject 2 - it’s pretty clear that that is a helpful time bin ).  These two observations give me confidence that the two sleep transform methods are genuinely helpful preprocessing steps.</p>

<p>Moreover, I’ve been pretty excited about these sleep transform methods because they frequently, though not always, produce strong bands of good classification for particular timebins.  Note that these timebins vary across subjects.  </p>

<table>
<td>

</td>
</table>

<p>So a logical next question is to determine which method of the two sleep transform methods are better.  Unfortunately, I don’t get the impression that one method is better than the other.  Sometimes a subject will look okay (not particularly salient bands of good classification) for one method, but</p>

<h3 id="wake-classification-results">Wake Classification Results</h3>
<p>These are important because it’s likely we’ll want to exclude subjects with poor wake cross-validation AUC from subsequent analyses.</p>

<p><img src="/images/research/sleep_eeg<em>9_subjects</em>06<em>23</em>2014/wake_AUC_most_subjects.png" width="700" height="350"></p>

<p><strong>Figure 1a:</strong> This shows the AUC for leave-one-out cross-validation on wake EEG data training on all the z-scored frequencies corresponding to a particular time bin as indicated on the y-axis.  The AUC was calculating by training a classifier that distinguishes between all classes, but only testing on patterns that corresponded to either a celebrity or a landmark.  The AUC was calculated by feeding in the difference between the output for the celebrity one-vs-all classifier and the landmark one-vs-all classifier for each cross-validation fold and the plotted values show the mean AUC across all cross-validation folds.  We had previously looked at the cross-validation accuracy for the first two subjects across a wider range of times, but narrowed it down to these four windows.</p>

<p><code>plot_loopify_time_sweep_results_AUC.m </code>  <br />
<code>classify_piloy_log_reg_time_sweep_driver.m </code>      </p>

<p><img src="/images/research/sleep_eeg<em>9_subjects</em>06<em>23</em>2014/wake_auc_avgd.png" width="700" height="350"></p>

<p><strong>Figure 1b:</strong> Average AUC per time bin</p>

<h3 id="sleep-cross-validation-classification-all-frequencies-per-time-bin">Sleep Cross-Validation Classification: All Frequencies Per Time Bin</h3>
<p><strong>NOTE:</strong> These results were supposed to be for performing feature selection, but I accidentally set the feature selection statistical threshold to 1, which is the same as not using feature selection.  <del>Re-running these results with feature selection.</del>  The results actually containing feature selection are just below.</p>

<p><img src="/images/research/sleep_eeg<em>9_subjects</em>06<em>23</em>2014/all<em>9subjects_sleep_CV_AUC_feature_select.png" width="700" height="350">
<strong>Figure 2a)</strong> We plot the AUC for classification of celebrity vs. landmarks during sleep across all our subjects (y-axis).  This was calculated by performing cross-validation (somewhere between 15-fold and 40-fold depending on the number of sleep patterns available) using all frequencies z-scored across electrodes for a particular timebin (x-axis). <br />
<img src="/images/research/sleep_eeg</em>9<em>subjects</em>06<em>23</em>2014/all<em>9subjects_sleep_CV_AUC_feature_select_PVAL.png" width="700" height="350">
<strong>Figure 2b)</strong> We plot the p-values for combinations of subjects and time bins with sleep cross-validation AUC values that are calculated via a shuffled permutation test whereby the labels of the classes are shuffled, we train a classifier with the shuffled patterns, and look at the cross-validation AUC for this shuffled dataset. Areas in red have p-values &gt; 0.05.
<img src="/images/research/sleep_eeg</em>9<em>subjects</em>06<em>23</em>2014/all<em>9subjects_sleep_CV_acc_feature_select.png" width="700" height="350">
<strong>Figure 2c)</strong> Instead of calculating the AUC, we plot the classification accuracy - this should be pretty similar to Figure 2b.
<img src="/images/research/sleep_eeg</em>9<em>subjects</em>06<em>23</em>2014/all_9subjects_sleep_CV_acc_feature_select_PVAL.png" width="700" height="350">
<strong>Figure 2d)</strong> Here we plot the p-values for the classification accuracies.</p>

<h3 id="sleep-cross-validation-classification-feature-selection-all-frequencies-per-time-bin">Sleep Cross-Validation Classification Feature Selection: All Frequencies Per Time Bin</h3>

<p><img src="/images/research/sleep_eeg<em>9_subjects</em>06<em>23</em>2014/9<em>subjects_CV_ACTUAL_AUC_feature_select.png" width="700" height="350">
<strong>Figure 3a)</strong> We plot the AUC for classification of celebrity vs. landmarks during sleep across all our subjects (y-axis).  This was calculated by performing cross-validation (somewhere between 15-fold and 40-fold depending on the number of sleep patterns available) using all frequencies z-scored across electrodes for a particular timebin (x-axis). <br />
<img src="/images/research/sleep_eeg</em>9<em>subjects</em>06<em>23</em>2014/9<em>subjects_CV_ACTUAL_AUC_feature_select_PVAL.png" width="700" height="350">
<strong>Figure 3b)</strong> We plot the p-values for combinations of subjects and time bins with sleep cross-validation AUC values that are calculated via a shuffled permutation test whereby the labels of the classes are shuffled, we train a classifier with the shuffled patterns, and look at the cross-validation AUC for this shuffled dataset. Areas in red have p-values &gt; 0.05.
<img src="/images/research/sleep_eeg</em>9<em>subjects</em>06<em>23</em>2014/9<em>subjects_CV_ACTUAL_acc_feature_select.png" width="700" height="350">
<strong>Figure 3c)</strong> Instead of calculating the AUC, we plot the classification accuracy - this should be pretty similar to Figure 2b.
<img src="/images/research/sleep_eeg</em>9<em>subjects</em>06<em>23</em>2014/9_subjects_CV_ACTUAL_acc_feature_select_PVAL.png" width="700" height="350">
<strong>Figure 3d)</strong> Here we plot the p-values for the classification accuracies.</p>

<h3 id="sleep-cross-validation-transform-sleep-pattern-by-average-wake-pattern">Sleep Cross-Validation: Transform Sleep Pattern By Average Wake Pattern</h3>
<p><a href="/blog/2014/07/01/sleep-eeg-transform-avg-pattern/"> dump of plots </a>   </p>

<p>We have 4 x 3 plots per subject (4 plots (AUC, accuracy, and p-val map for both of those) for each result, 3 different results: one for transforming the sleep data with the average face pattern, one for transforming with the average scene pattern, and one for transforming with the difference between the average face and average scene)</p>

<h3 id="sleep-cross-validation-transform-sleep-pattern-by-mcduff-importance-map">Sleep Cross-Validation: Transform Sleep Pattern By McDuff Importance Map</h3>
<p><a href="/blog/2014/07/01/sleep-eeg-mcduff-transform/"> dump of plots </a>   </p>

<p>We have 4 x 3 plots per subject (4 plots (AUC, accuracy, and p-val map for both of those) for each result, 3 different results: one for transforming the sleep data with the face mcduff importance map, one for transforming with the scene mcduff importance map, and one for transforming with the difference between the face and scene importance maps)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG Ken Meeting]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/06/02/sleep-eeg-ken-meeting/"/>
    <updated>2014-06-02T11:27:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/06/02/sleep-eeg-ken-meeting</id>
    <content type="html"><![CDATA[<h3 id="context">Context</h3>

<p>After ending the first-year rotation, I’m now back on the hook for working on this sleep EEG project full-time.  This meeting was simply to re-orient ourselves and plan the next immediate steps.</p>

<h3 id="next-steps">Next Steps</h3>
<ol>
  <li>Area under ROC instead of classification accuracy   </li>
  <li>Represent goodness in terms of where it falls on the empirical null distribution (as opposed to previously where we were showing raw-classification accuracy which we really have no clue how good that should be)  </li>
  <li>Train sleep classifier using wake WEIGHTS instead of wake average pattern as template.  The rationale behind this is that the average pattern templates treat all electrodes as equally important, but this just isn’t true.  The importance maps will take that into account   </li>
  <li>Create existing plots for new subjects [n = 8] (!)</li>
</ol>

<h3 id="next-next-steps">Next Next Steps</h3>
<ol>
  <li>We need to think about how we can collapse down each sleep trial (playing of a sleep sound) into a single reactivation metric.  Currently, we have classifier output for different timebins of the sleep trial and it seems like the maximally-classifiable time bin varies across subjects. <br />
    <ul>
      <li>Train an “uberclassifier” that classifies across all timebins - this would give us a single reactivation score per trial   </li>
      <li>Is there a smart way of reducing the number of features we have?<br />
        <ul>
          <li>we might have some <em>a priori</em> ideas about which features to include.  </li>
          <li>we might want to look at the cross-validation accuracy along certain time-bins to narrow things down.   </li>
        </ul>
      </li>
      <li>This is where I thought a recurrent neural network might be helpful for us.  </li>
      <li>Alternative idea: train a higher-level classifier on the outputs of the individual time bin classifiers    </li>
    </ul>
  </li>
  <li>How do we relate reactivations to subsequent memory?  Recall that there will be multiple classifier readouts per item, perhaps something a la [P-CIT] (https://code.google.com/p/p-cit-toolbox/) would work for us.</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG Update 2]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/05/04/sleep-eeg-update-2/"/>
    <updated>2014-05-04T20:53:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/05/04/sleep-eeg-update-2</id>
    <content type="html"><![CDATA[<p>Here are some questions/conclusions regarding the new data:</p>

<h3 id="are-the-trained-classifier-weights-highest-at-the-frequencies-used-to-generate-the-wake-template--for-instance-given-that-sleep-data-was-transformed-using-the-faces-template-at-230-ms-and-10-hz-does-the-trained-sleep-classifier-have-high-weights-at-10-hz">Are the trained classifier weights highest at the frequencies used to generate the wake template?  For instance, given that sleep data was transformed using the faces template at 230 ms and 10 Hz, does the trained sleep classifier have high weights at 10 Hz?</h3>
<p>This does not seem to be the case.  If we look at the averaged (across time bins) classifier weights at each frequency, we see that there are hot spots of increased feature importance at frequencies that do not depend on the parameters used for the wake template generation.  This is illustrated in the figure below: we see increased feature importance at 3 Hz regardless of the  wake time bin and frequency used for generating the wake template.  We might be tempted to say there is a relationship between the encoding frequency and the feature importance at 10 Hz, but I don’t buy it given that we only see increased feature importance along the y-axis (indicating that there seems to be something informative about the sleep data at 10 Hz) but not the x-axis.</p>

<p><img src="/images/research/collaped_weights_by_freq_faceMINUSscene_wake_template_subj1.png" width="700" height="350"></p>

<h3 id="are-there-any-trends-in-increased-weights-across-any-of-the-following-parameters">Are there any trends in increased weights across any of the following parameters?</h3>

<h4 id="frequencies">Frequencies?</h4>
<p>Across both subjects, we see increased feature importance around 3 Hz.  Additionally, we can see, in both subjects, something around the 10 Hz range (10 Hz for subject 1,  7-12 Hz for subject 2) and a faint </p>

<h4 id="time-bins">Time bins?</h4>

<h4 id="face-vs-scene-vs-face-minus-scene">Face vs. Scene Vs. Face minus Scene</h4>

<h4 id="is-there-a-link-between-the-magnitude-of-the-trained-sleep-classifier-weights-and-the-maximum-cv-accuracy-achieved-at-that-particular-time-bin--this-will-indicate-whether-or-not-there-is-a-benefit-to-reducing-the-classification-process-even-further-using-a-subset-of-the-features">Is there a link between the magnitude of the trained sleep classifier weights and the maximum CV accuracy achieved at that particular time bin?  This will indicate whether or not there is a benefit to reducing the classification process even further using a subset of the features.</h4>

<p>Future To-do: Is there a link between the trained sleep classifier weights and the average cross-validation accuracy for the shuffled sleep?  What would this tell us about the underlying space?</p>

<h4 id="what-is-the-relationship-between-the-weights-for-the-sleep-transformed-classifier-compared-to-the-feature-selected-weights">What is the relationship between the weights for the sleep transformed classifier compared to the feature selected weights?</h4>
<p>I don’t have this data yet.</p>

<h4 id="shuffling-results-given-that-we">Shuffling results: given that we</h4>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG Update]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/05/04/sleep-eeg-update/"/>
    <updated>2014-05-04T18:45:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/05/04/sleep-eeg-update</id>
    <content type="html"><![CDATA[<h3 id="results">Results</h3>

<h4 id="shuffled-cross-validation-accuracy-using-transformed-sleep-data-item--5-on-to-do-list">Shuffled Cross-validation Accuracy Using Transformed Sleep Data (Item # 5 on To-Do List)</h4>
<p>The plots below were using the generate-template-on-wake-then-transform-sleep-data-then-train-a-sleep-classifier approach.  The x-axis corresponds to training on different sleep time bins (although we use all the frequencies) and the y-axis corresponds to using different time-freq pairs to generate the wake template.  The key part of this plot is that we <strong>randomly permuted the sleep labels</strong> - so we would hope to see 50% classification accuracy here (which unfortunately doesn’t seem to be the case).  Ideally, I would have multiple iterations of this shuffling, but so far I’ve only run a single shuffle - <strong>how serious is it that I haven’t run multiple iterations?</strong></p>

<p><img src="/images/research/subject<em>1sleep_transform_CV_shuffled.png" width="700" height="350">
<img src="/images/research/subject</em>2sleep_transform_CV_shuffled.png" width="700" height="350"></p>

<h4 id="sleep-cross-validation-accuracy-using-different-sleep-transformations-item-2-on-to-do-list">Sleep Cross-validation Accuracy using different sleep transformations (Item #2 on To-Do List)</h4>
<p>Previously, we had only tried the generate-template-on-wake-then-transform-sleep-data-then-train-a-sleep-classifier using the FACE template, the results below show cross-validation accuracy using the face template (same results as presented previously), the scene template, and face-minus-scene.</p>

<p><img src="/images/research/subject<em>1sleep_transform_CV.png" width="700" height="350">
<img src="/images/research/subject</em>2sleep_transform_CV.png" width="700" height="350">
<img src="/images/research/subject<em>3sleep_transform_CV.png" width="700" height="350">
<img src="/images/research/subject</em>4sleep_transform_CV.png" width="700" height="350">
<img src="/images/research/subject<em>5sleep_transform_CV.png" width="700" height="350">
<img src="/images/research/subject</em>6sleep_transform_CV.png" width="700" height="350"></p>

<h4 id="sleep-cross-validation-accuracy-using-all-frequencies-feature-select-item-4-on-to-do-list">Sleep Cross-validation Accuracy Using All Frequencies Feature Select (Item #4 on To-Do List)</h4>
<p>This is pretty straightforward cross-validation accuracy performed on the labelled sleep data.  For each time bin of the sleep data, we z-scored the power spectrum at all frequencies across electrodes and trained a classifier using 10-fold cross-validation using standard p-value based feature selection.</p>

<p><img src="/images/research/all_2_subjects_sleep_CV_feature_select.png" width="700" height="350"></p>

<h4 id="classifier-weights-for-sleep-transformed-data-item-1-on-to-do-list">Classifier Weights for sleep transformed data (Item #1 on To-Do List)</h4>
<p>Recall that we have a classifier for each combination of wake time-freq pair and sleep time.  Each classifier sweeps the template across each frequency of the time bin, yielding a classifier with as many features as there are frequencies.  Thus, visualizing the weights would require displaying the magnitude of the weight across three dimensions: wake time-freq pair, sleep time bin, and sleep frequency.  Instead of trying to plot some whacky surface, I’ve created plots that average the magnitude of the weights either across the sleep time bins OR across sleep frequencies - if a particular time bin or frequency looks promising from these plots, we can look at the uncollapsed results as needed.</p>

<p>The value displayed below is taken as follows:<br />
<code>
for each wake time-freq bin x sleep time bin pair:<br />
 &nbsp; &nbsp; &nbsp; &nbsp;for each cross-validation iteration:<br />
		 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;z-score weights<br />
	&nbsp; &nbsp; &nbsp; &nbsp;Avg weights over cross-validation iterations and take absolute value<br />
</code></p>

<p>This 3d matrix was then collapsed either across sleep time bins or sleep frequencies.  This was done for the various wake templates.  </p>

<p><strong>KEN:</strong> I know this is pretty different than the McDuff method for interpreting feature weights - but the requirement to average over the patterns made things difficult so I did things this way as a much easier to accomplish first pass.  It’s worth noting that the EEG analysis code rescales all features to be within the range of 0 and 1, so I think this crude way of looking at the features should still be pretty informative.</p>

<h4 id="face-minus-scene-wake-template">Face Minus Scene Wake Template</h4>
<p><img src="/images/research/collaped_weights_by_freq_faceMINUSscene_wake_template_subj1.png" width="700" height="350">
<img src="/images/research/collaped_weights_by_freq_faceMINUSscene_wake_template_subj2.png" width="700" height="350"></p>

<h4 id="face-wake-template">Face Wake Template</h4>
<p><img src="/images/research/collaped_weights_by_freq_face_wake_template_subj1.png" width="700" height="350">
<img src="/images/research/collaped_weights_by_freq_face_wake_template_subj2.png" width="700" height="350"></p>

<h4 id="scene-wake-template">Scene Wake Template</h4>
<p><img src="/images/research/collaped_weights_by_freq_scene_wake_template_subj1.png" width="700" height="350">
<img src="/images/research/collaped_weights_by_freq_scene_wake_template_subj2.png" width="700" height="350"></p>

<h4 id="face-minus-scene-wake-template-1">Face Minus Scene Wake Template</h4>
<p><img src="/images/research/collapsed_weights_by_sleep_time_faceMINUSscene_wake_template_subj1.png" width="700" height="350">
<img src="/images/research/collapsed_weights_by_sleep_time_faceMINUSscene_wake_template_subj2.png" width="700" height="350"></p>

<h4 id="face-wake-template-1">Face Wake Template</h4>
<p><img src="/images/research/collapsed_weights_by_sleep_time_face_wake_template_subj1.png" width="700" height="350">
<img src="/images/research/collapsed_weights_by_sleep_time_face_wake_template_subj2.png" width="700" height="350"></p>

<h4 id="scene-wake-template-1">Scene Wake Template</h4>
<p><img src="/images/research/collapsed_weights_by_sleep_time_scene_wake_template_subj1.png" width="700" height="350">
<img src="/images/research/collapsed_weights_by_sleep_time_scene_wake_template_subj2.png" width="700" height="350">
<!--### Questions-->
<!--##### Classifier Weights-->
<!--#. When we look at the magnitude of the trained classifier weights for various wake-template-frequencies, do we see that the weights are generally bigger at the frequencies that --></p>

<!--#### -->
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG Update]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/04/20/sleep-eeg-update/"/>
    <updated>2014-04-20T14:41:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/04/20/sleep-eeg-update</id>
    <content type="html"><![CDATA[<h3 id="classification-results">Classification results</h3>
<p>Last time I showed a plot for CV accuracy of wake data generated by looking at individual time bins, but using all possible frequencies.  This plot is the logical next step: it considers individual frequencies - which I did for only the most promising time bins (again independently).  I didn’t use various regularization penalties this time ($\lambda = 1$ for current plot).  Additionally, this data was z-scored across electrodes for each frequency.  </p>

<p><img src="/images/research/sweep_time_freq.png" width="700" height="350"></p>

<p>It’s harder to pick out the “best” frequencies from this plot (compared to picking the “best” times previously).  The ~alpha band seems to pop out for both subjects, but it might be worthwhile to investigate some of the gamma stuff going on in subject 2.   Also, note that the colorbars are on different scales for both subjects.  Also we can actually get higher classification accuracy for both subjects than we did using all frequencies like last time.</p>

<p>The next analysis step we discussed was to pick a vector consisting of the power across electrodes at a best time and frequency and use this to transform the sleep data and perform cross-validation on the sleep data using this transformed dataset. With the idea being that after we get good sleep cv-accuracy, we could look at the relationship between classifier output and memory.</p>

<p>Sleep Transformation refresher:
- for each time bin of the sleep data, compute dot-product between the z-scored power across electrodes at a given frequency and the “best vector” described above and do this for each frequency</p>

<h3 id="next-steps">Next steps</h3>
<p>I see two approaches:<br />
1. glob across subjects and generate transformed-sleep CV results across a range of times and frequencies that seemed promising for both subjects (150ms - 230ms, 7-13Hz)<br />
2. treat subjects independently, rank (time x freq) pairs by highest CV accuracy and generate transformed-sleep CV results for top 10 (time x freq) pairs   </p>

]]></content>
  </entry>
  
</feed>
