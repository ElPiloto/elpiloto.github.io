<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: sleep | Something Witty]]></title>
  <link href="http://ElPiloto.github.io/blog/categories/sleep/atom.xml" rel="self"/>
  <link href="http://ElPiloto.github.io/"/>
  <updated>2014-09-02T15:17:12-04:00</updated>
  <id>http://ElPiloto.github.io/</id>
  <author>
    <name><![CDATA[Luis R. Piloto]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Sleep EEG: Past TODO items and next directions]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/09/02/sleep-eeg-past-to-do-items-and-next-directions/"/>
    <updated>2014-09-02T14:15:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/09/02/sleep-eeg-past-to-do-items-and-next-directions</id>
    <content type="html"><![CDATA[<p>Before determining where we should go next now that we have a little bit of breathing room, I did a big review of the notes (and also some emails) I left myself from the last two months of updates in order to make sure we didn’t leave any ideas behind.  <strong>Bold asterisks indicate the items I intend to do next</strong> either because I think they’re important, are easy to do, or are a requisite for something important.   </p>

<h4 id="below-is-a-consolidated-list-of-to-do-items-random-things-weve-bookmarked-and-some-of-my-own-ideas">Below is a consolidated list of to-do items, random things we’ve bookmarked, and some of my own ideas</h4>

<ol>
  <li>
    <p>L1-regularization (instead of univariate feature selection + feature regularization via L2)<strong>******(important)</strong>    </p>
  </li>
  <li>
    <p>why was subject 9’s classification accuracy so low? (didn’t do poorly with other classification methods, so maybe it’s not so important to look at)   </p>
  </li>
  <li>coming up with a good wake template <strong>******(important)</strong> 
    <ul>
      <li>look at mcduff importance maps for wake LOSO logistic regression: does something pop out as the timebin/frequency to use for wake transformation?      </li>
    </ul>
  </li>
  <li>
    <p>different AUC metrics (diffAUC pooled over trials, meanAUC, etc)   </p>
  </li>
  <li>
    <p>boosting untransformed wake LOSO did poorly (about chance ) - was this a bug/not enough regularization or was it due to using the relatively quicker, unfamiliar totalboost algorithm? we can test this by running logitboost or gentleboost (which worked previously) now that we don’t have the pressure of the kenP deadline <strong>******(easy to do)</strong>   </p>
  </li>
  <li>
    <p>when looking at classification scores that are significantly better than chance, let’s also look at scores below chance   </p>
  </li>
  <li>
    <p>do we want to look out longer than 1 second after a tone is played? if first 500ms of sleep trial data happens to fall on down phase, we wouldn’t expect classification to work for that trial   </p>
  </li>
  <li>sleep transform: <br />
    <ul>
      <li>LOSO version   </li>
      <li>use face AND scene transformed data   </li>
      <li>exclude electrodes individually after finding good timebin/freq for wake   </li>
      <li>explicitly disentangle effects of dot-product vs. correlation on sleep transform   </li>
    </ul>
  </li>
  <li>
    <p>separate face vs. scene classifiers (face vs. objects/scrambled faces/scrambled scenes AND scene vs. objects/scrambled faces/ scrambled scenes): are we doing better at one of these than the other?  do they have different optimal classification times? (ties into previous thoughts about AUC: looking at face and scene classifier difference, diffAUC, vs. meanAUC)   <strong>******(important)</strong>   </p>
  </li>
  <li>
    <p>classification of data using ERPs   </p>
  </li>
  <li>visualize data: <br />
    <ul>
      <li>interface with EEGLAB topoplots   </li>
      <li>compare face/scene templates per subject to those generated on an individual-subject level   </li>
      <li>similarity of sleep data across time bins to wake pattern via simple correlation: are these different for the different classes?   <br />
        <ul>
          <li>we can look at the average similarity to a face or scene pattern against the average similarity for all other classes across sleep time  bins for each frequency<strong>******(important)</strong>     </li>
        </ul>
      </li>
      <li>autocorrelation of sleep data for various electrodes, could we use this as an indicator of down phases for the purpose of excluding time bins or trials?  is there another proxy for determining a down phase?      </li>
    </ul>
  </li>
  <li>
    <p>LOSO using some form of non-linear classification? previously we didn’t think this was a viable option given the ratio of training examples to features, but the combination of Hilbert-transformed data and LOSO makes me think this is now possible   </p>
  </li>
  <li>
    <p>exclude forgotten items from the sleep analysis, does this improve our sleep cross-validation?   </p>
  </li>
  <li>
    <p>test wake reactivation during learning of associated locations (this is different than the current wake data we use to train on which we take from the wake localizer)   </p>
  </li>
  <li>
    <p>generate simulated data to test idea that we could even find a wake pattern embedded randomly in a sleep data timebin-freq bin. also, would be good for general code validation (importance map transformations, mvpa code, etc)   </p>
  </li>
  <li>
    <p>look at classifier output for scrambled conditions and use to sleep transform: do these results look different than what we get when we transform via face and scene conditions?   </p>
  </li>
  <li>
    <p>“feature shuffle noise” wake classification analysis: do we get better generalization when we extend training data with patterns that are hybrids of two patterns from the same class? analogous to dropout techniques for neural networks    </p>
  </li>
  <li>
    <p>What if we transform the sleep data according to the timebin-freq pair that performs the best for each subject?  This is obviously less parsimonious than we’d like, but a signature of reactivation is a signature of reactivation.    </p>
  </li>
  <li>For results with <em>really</em> low wake classification accuracy, what do we get if we sleep transform by flipping the labels on the importance maps generated?    </li>
</ol>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG: Leave-one-subject-out]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/08/26/sleep-eeg-leave-one-subject-out/"/>
    <updated>2014-08-26T14:45:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/08/26/sleep-eeg-leave-one-subject-out</id>
    <content type="html"><![CDATA[<h2 id="wake-leave-one-subject-out-loso-logistic-regression-use-all-freqs-all-times">Wake Leave-One-Subject-Out (LOSO) Logistic Regression, Use All freqs, All Times</h2>

<p><a href='/images/research/sleep_eeg_hilbert/log_reg_LOSO_all_features_class_AUC_vary_lambda.jpg' target='_blank'><img src="/images/research/sleep_eeg_hilbert/log_reg_LOSO_all_features_class_AUC_vary_lambda.jpg" width="700" height="350"></a></p>

<h4 id="this-should-contain-the-mcduff-importance-maps">This should contain the McDuff Importance Maps</h4>

<h2 id="wake-loso-logistic-regression-all-freqs-restrict-time-range-to-150ms---550ms">Wake LOSO Logistic Regression, All Freqs, Restrict Time Range to 150ms - 550ms</h2>

<p><a href='/images/research/sleep_eeg_hilbert/log_reg_LOSO_all_features_btwn_150ms550ms_lambda100_persubj.jpg' target='_blank'><img src="/images/research/sleep_eeg_hilbert/log_reg_LOSO_all_features_btwn_150ms550ms_lambda100_persubj.jpg" width="700" height="350"></a></p>

<h2 id="wake-loso-logistic-regression-time-freq-sweeps">Wake LOSO Logistic Regression, Time-Freq Sweeps</h2>

<h2 id="wake-loso-boosting-totalboost-all-freqs-all-times">Wake LOSO Boosting (TotalBoost), All Freqs, All Times</h2>

<h2 id="loso-sleep-logistic-regression-all-freqs-all-times">LOSO Sleep Logistic Regression, All Freqs, All Times</h2>

<h2 id="loso-sleep-boosting-all-freqs-all-times">LOSO Sleep Boosting, All Freqs, All Times</h2>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG: Hilbert Transformed Data]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/08/06/sleep-eeg-hilbert-transformed-data/"/>
    <updated>2014-08-06T10:36:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/08/06/sleep-eeg-hilbert-transformed-data</id>
    <content type="html"><![CDATA[<h2 id="wake-hilbert-transformed-classification">Wake Hilbert Transformed Classification</h2>

<p><a href='/images/research/sleep_eeg_hilbert/wake_hilb_mean_across_subjects.png' target='_blank'><img src="/images/research/sleep_eeg_hilbert/wake_hilb_mean_across_subjects.png" width="700" height="350"></a></p>

<p><strong>Figure 1:</strong> Average AUC on Hilbert-transformed wake data across all subjects for each time bin and frequency band combination.</p>

<p><strong>Thoughts:</strong> We see “hotspots” where we were hoping to see them: at the theta frequency around 200-230ms. Moreover, we see that the 200-230ms theta bin forms a local peak: as you move earlier or later in time, classification accuracy decreases.  Importantly, the accuracy using these bands (as opposed to individual frequencies like we had before) yields comparable average wake classification across subjects if we look at this <a href="http://ElPiloto.github.io/images/research/sleep_eeg_9_subjects_06_23_2014/wake_auc_avgd.png">old plot</a>.  If this result holds for all subjects, then we would feel pretty good about using the 200ms (or 230ms) theta time-freq bin to transform the sleep data.  Therefore, let’s look at the individual subjects to see if these results are driven by some subjects or if they seem to be pretty evenly distributed (spoiler: doesn’t seem to be a good time-freq bin, or even classifiable, for all subjects, but it does pretty well)</p>

<p><a href='/images/research/sleep_eeg_hilbert/wake_hilb_all_subjects_all_freqs_all_times.png' target='_blank'><img src="/images/research/sleep_eeg_hilbert/wake_hilb_all_subjects_all_freqs_all_times.png" width="700" height="350"></a></p>

<p><strong>Figure 2:</strong> AUC heatmaps of classification AUC per subject.  Subject IDs are along the top of each subplot, timebins run along the y-axis and frequencies along the x-axis (only explicitly marked for the last subject, but the axes are exactly the same as Figure 1).</p>

<p><strong>Thoughts:</strong> These plots, though informative, suck for the purpose of evaluating how good the 200-250ms, theta bins do across all subjects.  Let’s zoom in on the areas of interest below.  One additional point, is that we’re getting <strong>much</strong> better max AUC across timebins than previously before (using individual frequencies) suggesting that the Hilbert transform is a better preprocessing technique for classification.</p>

<p><a href='/images/research/sleep_eeg_hilbert/wake_across_subjs_times7<strong>8</strong>9_freq2.png' target='_blank'><img src="/images/research/sleep_eeg_hilbert/wake_across_subjs_times7<strong>8</strong>9_freq2.png" width="700" height="350"></a></p>

<p><strong>Figure 3:</strong>  Above we show the AUC for each subject for 3 different times (200, 225, and 250ms respectively) for the 4 Hz (theta) frequency band.</p>

<p><strong>Thoughts:</strong> This isn’t the greatest, especially considering some subjects have time-freq bins with much higher AUC.  “Irregardless,” these results are sufficiently positive to justify transforming sleep data.  Everything that we said about the non-band wake analyses holds here (e.g. we can look at other bins that look good for subjects, we can take each subject’s best bin to transform the data, etc).</p>

<h2 id="sleep-hilbert-untransformed-classification---sweep-across-time-and-frequency-combinations">Sleep Hilbert Untransformed Classification - Sweep Across Time And Frequency Combinations</h2>

<p>Below we look at how discriminable the sleep data is at each particular timebin and frequency combination of the sleep data using logistic regression.  Additionally, we test the regularization parameter for two separate values $ \lambda = 1, 50 $</p>

<p><a href='/images/research/sleep_eeg_hilbert/sleep_hilb_mean_across_subjects_lambda1.png' target='_blank'><img src="/images/research/sleep_eeg_hilbert/sleep_hilb_mean_across_subjects_lambda1.png" width="700" height="350"></a></p>

<p><strong>Figure 4:</strong> Above we show the across-subjects average AUC for performing logistic regression on the sleep data at each time and frequency band combination with regularization parameter $ \lambda = 1 $.</p>

<p><a href='/images/research/sleep_eeg_hilbert/sleep_hilb_lambda50_mean_across_subjects.png' target='_blank'><img src="/images/research/sleep_eeg_hilbert/sleep_hilb_lambda50_mean_across_subjects.png" width="700" height="350"></a></p>

<p><strong>Figure 5:</strong> Same as Figure 4, except this time we set the regularization parameter $ \lambda = 50 $.</p>

<p><strong>Thoughts:</strong> The band around 175ms, 16 Hz looks promising - we had mentioned that we’d be pretty happy if we could get something like 7-8 points above chance average classification accuracy across subjects.  This is much higher than the average classification accuracy ($ mean AUC = 0.52 $) we were getting for our “best 8” untransformed, logistic regression average AUC using all times and frequencies (recall that those results were not using frequency bands like we are now, but instead look at the power spectrum at individual frequencies).  Obviously, we can’t directly compare the individual time and frequency band results to using all time and frequency features, but it’s the closest benchmark we have.  Those old results can be found <a href="http://ElPiloto.github.io/blog/2014/07/27/sleep-eeg-post-boosting-results/"> here </a> under the section “To-Do Item #2”.</p>

<p>These results are encouraging for two reasons.  First, the best time and frequency combination falls within a plausible time range.  We could talk to James to see if he has any a priori evidence to support getting results in the BETA frequency range, but at least the time isn’t too early.  Second, the AUC is pretty good on the individual subject level as we see below: most subjects are either at or above chance.</p>

<p><a href='/images/research/sleep_eeg_hilbert/best_time_freq_untransformed_sleep_lambda1.jpg' target='_blank'><img src="/images/research/sleep_eeg_hilbert/best_time_freq_untransformed_sleep_lambda1.jpg" width="700" height="350"></a></p>

<p><strong>Figure 6:</strong> Above we show the AUC for each subjects generated by performing logistic regression on the sleep data at the best time and frequency band combination with regularization parameter $ \lambda = 1 $.</p>

<p><a href='/images/research/sleep_eeg_hilbert/best_time_freq_untransformed_sleep_lambda50.jpg' target='_blank'><img src="/images/research/sleep_eeg_hilbert/best_time_freq_untransformed_sleep_lambda50.jpg" width="700" height="350"></a></p>

<p><strong>Figure 7:</strong> Same as Figure 6, except this time we set the regularization parameter $ \lambda = 50 $.</p>

<p><strong>Thoughts:</strong> Okay, so we have <strong>a</strong> time and frequency combination that gives us good classification accuracy, but this could just arise from multiple comparisons.  Sure, we can take comfort that the classification accuracy seems to peak at 175ms and degrades smoothly as we move away, but it’s also the case that the EEG power spectrum is pretty similar at those times.  <strong>SO</strong>, what would it take for me to believe these results?  Running classification on <strong>ALL</strong> of the <em>untransformed</em> sleep data features with heavy regularization and yielding good classification accuracy with weights that consistently preferred that time and frequency combination.  This is next on my to-do list, it shouldn’t take too long to get up and running.  I’d also like to add a reminder to myself that it may be worth subsampling the time dimension.</p>

<h2 id="sleep-hilbert-untransformed-log-reg-classification---use-all-time-and-frequencies">Sleep Hilbert Untransformed Log Reg Classification - Use All Time and Frequencies</h2>

<p><a href='/images/research/sleep_eeg_hilbert/untransformed_logreg_all_features.png' target='_blank'><img src="/images/research/sleep_eeg_hilbert/untransformed_logreg_all_features.png" width="700" height="350"></a></p>

<p><strong>Figure 8:</strong> Above we show the AUC per subject (along the y-axis) for various logistic regression regularization penalty values (along the x-axis) for classification using ALL of the features for the untransformed (i.e. not transformed according to wake pattern) sleep data using frequency bands (using Hilbert transform).  The <em>NaN</em> value is because that particular job failed and I didn’t think it was that important to justify putting off these results.</p>

<p><a href='/images/research/sleep_eeg_hilbert/untransformed_logreg_all_features_avg_across_subjects.png' target='_blank'><img src="/images/research/sleep_eeg_hilbert/untransformed_logreg_all_features_avg_across_subjects.png" width="700" height="350"></a></p>

<p><strong>Figure 9:</strong> Same as Figure 8, except this time we average across subjects for each regularization setting.</p>

<p><strong>Thoughts:</strong> These results definitely aren’t what we were hoping for, and subject09’s  results are worrisome.  We’ve seen pretty consistently classifiable results out of subject 15 which is a good sanity check that there isn’t something wrong with the code or with the range of regularization values we’ve tried.  The best case scenario is that subject 15’s data is really easily classifiable, whereas the other subjects require more careful feature selection.  Towards this end, I’m going to try three things.  First, I’m running this same exact analysis except I’m including feature selection as a preprocessing step (note: I’ll have to run it for just a single regularization value since the feature selection process is pretty time-consuming).  Second, I’m going to look at the importance maps generated by this current analysis to see which features are being loaded upon.  Lastly, I’m going to try boosting using all the features.  This would also be a great place to have simulated data, but that’ll have to take a back seat.</p>

<h2 id="sleep-hilbert-untransformed-gentleboost-ensemble-classification---use-all-time-and-frequencies">Sleep Hilbert Untransformed GentleBoost Ensemble Classification - Use All Time And Frequencies</h2>

<p><a href='/images/research/sleep_eeg_hilbert/untransformed_gentleboost_all_features_ACC.png' target='_blank'><img src="/images/research/sleep_eeg_hilbert/untransformed_gentleboost_all_features_ACC.png" width="700" height="350"></a></p>

<p><strong>Figure 10:</strong> Above we show the AUC per subject (along the y-axis) for various logistic regression regularization penalty values (along the x-axis) for classification using ALL of the features for the untransformed (i.e. not transformed according to wake pattern) sleep data using frequency bands (using Hilbert transform).  The <em>NaN</em> value is because that particular job failed and I didn’t think it was that important to justify putting off these results.</p>

<p><a href='/images/research/sleep_eeg_hilbert/untransformed_gentleboost_all_features_avg_across_subjects_ACC.png' target='_blank'><img src="/images/research/sleep_eeg_hilbert/untransformed_gentleboost_all_features_avg_across_subjects_ACC.png" width="700" height="350"></a></p>

<p><strong>Figure 11:</strong> Same as Figure 10, except this time we average across subjects for each value of the <code>num_learners</code> parameter.</p>

<h2 id="the-continuing-story-of-delbungalow-billdel-subject-09">The Continuing Story of <del>Bungalow Bill</del> Subject 09</h2>

<p><strong>Sleep Statistics:</strong> 35 faces, 35 scenes.  Cross-validation was performed using leave-one-out-cross-validation (a.k.a. 35-fold).</p>

<p>Achieves reasonable classification accuracy using boosting.  Additionally, classification accuracy (as opposed to AUC) isn’t SO horrendous for logistic regression (0.31 for feature selection using $\lambda = 100$ and setting <code>stat_thresh</code> = 0.1)  When we look at the AUC per cross-validation iteration, we find that the AUC is always 0 or 1 essentially (see figure 11 below).  It seems like we’re severly overfitting.  This is supported by the fact that the boosting method actually does well for subject 09 ( boosting much less susceptible to overfitting).  However, if this really was the case, you’d expect that doing feature selection would help this subject’s results.  Currently, that is not the case, so I’m gunna try even stricter feature selection to see if that’s what’s really at the bottom of this.</p>

<p><a href='/images/research/sleep_eeg_hilbert/auc_per_iteration_feature_select_subj9.jpg' target='_blank'><img src="/images/research/sleep_eeg_hilbert/auc_per_iteration_feature_select_subj9.jpg" width="700" height="350"></a></p>

<p><strong>Figure 12:</strong> Above we show the AUC per cross-validation iteration for subject 09 using logistic regression on all of the untransformed sleep features</p>

<p><a href='/images/research/sleep_eeg_hilbert/per_fold_correctminusincorrect_class_outputs.jpg' target='_blank'><img src="/images/research/sleep_eeg_hilbert/per_fold_correctminusincorrect_class_outputs.jpg" width="700" height="350"></a></p>

<p><strong>Figure 13:</strong> Above we show the difference between the classifier output for the correct class minus the classifier output for the incorrect class for each test item over all cross-validation iterations for various parametrizations of wake logistic regression classification for subject 9.  Note that for p = 0.01, we get something like 30-50 features selected.</p>

<p><strong>Random thought:</strong> What would we get if we tried to use this for sleep transforming data by flipping the classes?</p>

<p><strong>Extra thoughts:</strong> I was feeling uneasy about the fact that AUC and classification accuracy give completely different answers for subject 09.  However, I went through and verified by hand that the code for calculating the AUC is in fact correct (given the type of AUC metric we want to use).  Currently, we’re looking at the distribution of (face - scene) classifier outputs to calculate the AUC, but we get vastly different results for this subject (and I assume other subjects as well), if we calculate a separate AUC for the face classifier output, a separate AUC for the scene classifier output, and then take the average (possibly weighted by number of examples from each class) of these two AUCs to yield a single number (let’s call this method <em>meanAUC</em>).  Although we’ve been evaluating with respect to the former definition of AUC ( let’s dub this <em>diffAUC</em>), the best metric to use actually depends on how we intend to relate the sleep classifier output to the behavioral data.  One method for relating classifier output uses a binary measure to predict subsequent performance: each reactivation event is labelled as a 0 or a 1 indicating whether or not the classifier predicted the correct class.  Subsequent memory for an item should increase if the classifier correctly classified reactivation events.  In this case, the appropriate AUC method to use is the <em>diffAUC</em> because we’re interested in the difference between the classifier outputs for each class.    An alternative method for relating classifier output to behavioral data would be to only look at, for each reactivation event, the classifier output from the classifier corresponding to the item’s category (i.e. if we’re looking at a face item, we only look at the face classifier output; if we’re looking at a scene item, we take the scene classifier output).  We would predict that items with a high classifier output, despite their category, would yield better subsequent memory.  This method lends itself to evaluation via <em>meanAUC</em>.   <strong>They are fundamentally different metrics</strong> and so I shouldn’t take this as a sign that my code base is messed up.  To further illustrate how different these metrics can be, below I provide a sample of the different metrics for a particular subject:   </p>
<pre>
                 diffAUC: .1714
                 meanAUC: 0.5286
              % correct: 0.2857
</pre>

<p>Here’s a case that’s also a bit counterintuitive, but checks out.  I’m leaving this here just as a note for future Luis.</p>
<pre>
true classes = {face, scene}
---------------------------
face = 0.435, 0.4635
scene = 0.5650, 0.5365
diff = -0.13, -0.07

Max Class Output Prediction: scene (incorrect), scene (correct)
% correct = 0.5
diffAUC = 0
</pre>

<h2 id="ken-meeting-points">Ken Meeting Points</h2>

<h4 id="auc">AUC</h4>
<p>The space of <em>diffAUC</em>, <em>meanAUC</em>, and individual AUCs (e.g. <em>faceAUC</em> or <em>sceneAUC</em>) are all justifiable classification metrics.  Let’s not open this can of worms just yet, <em>however</em>, let’s <strong>definitely</strong> switch to pooling the classification outputs from all cross-validation folds and <em>then</em> calculating AUC, instead of calculating AUC for each fold, and averaging across folds.  That being said, subject 09 per-fold diffAUC was 0.1714 (see above), and their pooled diffAUC = 0.18 - so it doesn’t seem to have too much of an impact as I suspected.</p>

<h4 id="feature-selection">Feature Selection</h4>
<p>Ken thinks it’s unconventional to use feature selection and L2 regularization.  Let’s try L1 regularization instead, which has one less parameter and performs multivariate feature selection.</p>

<h4 id="subject-9-crappiness">Subject 9 Crappiness</h4>
<p>I haven’t justified why overfitting should lead to systematically incorrect predictions, it should lead to chance performance.   It may be the case that feature selection and L2 regularization is just susceptible to pathological results given that our classifier does okay when we try boosting.  If we try L1-regularization and that works okay, then we’ll know the problem is feature selection + L2 regularization.</p>

<h4 id="next-steps">Next Steps:</h4>
<p>Let’s try our leave-one-subject-out analysis for individual time-frequency pairs.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG: Varying regularization and simulated results]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/08/01/sleep-eeg-varying-regularization-and-simulated-results/"/>
    <updated>2014-08-01T10:27:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/08/01/sleep-eeg-varying-regularization-and-simulated-results</id>
    <content type="html"><![CDATA[<h3 id="logistic-regression-untransformed-sleep-data-varying-regularization-term">Logistic Regression Untransformed Sleep Data: Varying regularization term</h3>
<p>The motivation for this line of inquiry is contained in <a href="/blog/2014/07/27/sleep-eeg-post-boosting-results/">this post</a> under the discussion section for figure 4.</p>

<p><a href='/images/research/untransformed_log_reg_lambda250and750.jpg' target='_blank'><img src="/images/research/untransformed_log_reg_lambda250and750.jpg" width="700" height="350"></a></p>

<p><strong>Figure 1:</strong> This shows the sleep classification accuracy for logistic regression using untransformed sleep data with <code>statmap_anova</code> feature selection (stat_thresh = 0.1).</p>

<p><strong>Thoughts</strong> Well this is interesting.  I had previously been using a regularization term of 1 for all the logistic regression analyses and to see this level of classification accuracy using a regularization penalty two orders of magnitude higher than previously suggests that I didn’t have much of a grasp on the role of the regularization term in this analysis.  The $\lambda = 750$ case is encouraging because it’s up there in terms of the highest average classification accuracy we’ve seen AND it includes that largest regularization term we’ve used thus far.  It’s worth investigating how far we can push regularization before it collapses, thus I’ve launched an additional analysis trying results with regularization = 1250 and 1750.  Will update this post when those results are in.  ALSO, this begs the question: should we re-run our sleep transformed analyses using stronger regularization for the wake classes?  <strong>YES</strong></p>

<p><a href='/images/research/untransformed_log_reg_lambda1250and1750.jpg' target='_blank'><img src="/images/research/untransformed_log_reg_lambda1250and1750.jpg" width="700" height="350"></a></p>

<p><strong>Figure 2:</strong> This shows the sleep classification accuracy for logistic regression using untransformed sleep data with <code>statmap_anova</code> feature selection (stat_thresh = 0.1).</p>

<p><strong>Thoughts:</strong> This still doesn’t make it seem like we’ve hit the upper bound on the regularization penalty. I’m not sure what to make of the fact that 750 = good regularzation, 1750 = good regularization, but 1250 gives pretty crappy results.  I guess I don’t have any strong theoretical justification to think classification accuracy would vary smoothly as we modify the regularization parameter.  I’m now launching sleep untransformed classification on <em>all</em> the subjects with a high regularization term.</p>

<p><a href='/images/research/untransformed_log_reg_lambda12000_ALLSUBJS.jpg' target='_blank'><img src="/images/research/untransformed_log_reg_lambda12000_ALLSUBJS.jpg" width="700" height="350"></a></p>

<p><strong>Figure 3:</strong> This shows the sleep classification accuracy for logistic regression using untransformed sleep data with <code>statmap_anova</code> feature selection (stat_thresh = 0.1).</p>

<p><strong>Thoughts:</strong> This is clearly too high for most subjects. Although <strong>some</strong> of the subjects that have been doing well in our “best 8” analysis get pretty good classification with $\lambda = 2000$, which is unsurprising given their results for $\lambda = 1750$, some other subjects (e.g. subject 7) that did well are now getting below-chance accuracy.  The flop of some of the good “best 8” subjects from above to below chance accuracy makes a good case for reducing the regularization penalty.  Thus, I’m gunna try running all subjects with a smaller value for $\lambda$ because the infrastructure is in place to do that.</p>

<h3 id="simulated-results">Simulated Results</h3>

<p><del><strong>STILL CODING</strong></del> <strong>PUT ON HOLD, GOING TO TRY HILBERT TRANSFORMED DATA ANALYSIS FIRST</strong></p>

<!--Ken is most interested in the hilbert-transformed data in the hopes that the Hilbert stuff will clean up (induce consistency across subjects) for wake.  Hopefully see good classification across subjects theta, 200-ish, also which electrodes are informative.-->
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG: Group Meeting]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/07/28/sleep-eeg-group-meeting/"/>
    <updated>2014-07-28T17:10:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/07/28/sleep-eeg-group-meeting</id>
    <content type="html"><![CDATA[<h3 id="ken-succinctly-described-two-levels-of-complexity-in-our-sleep-analysis">Ken succinctly described two levels of complexity in our sleep analysis:</h3>
<ol>
  <li>How do we transform the sleep?  Recall this can be face or scene transformed, we can use correlation or the dot product, we can use data from different frequencies and different time bins in the wake data, etc.<br />
    <ul>
      <li>Perhaps we don’t need to sleep transform?</li>
      <li>Perhaps we need to exclude electrodes explicitly? We thought the McDuff importance maps method implemented this, but correlation doesn’t care about the magnitude so we need to re-evaluate this (either use dot product OR filter out electrodes by some other measure)  <strong>(TODO ITEM: dot product)</strong></li>
    </ul>
  </li>
  <li>When does reinstatement happen?  How should we score reinstatement?  If we do classification, do we just look at one time bin for the sleep?  Do we sum up classification across all time bins, etc.<br />
    <ul>
      <li>Maybe we want to use a simpler method than classification a la Staresina paper: sum correlation between template and sleep pattern and threshold to indicate replay event.   </li>
      <li>We could and should look at trial-by-trial plots of correlation across time for both the incorrect and correct pattern - eyeball the crap out of this.  <strong>(TODO ITEM)</strong></li>
    </ul>
  </li>
  <li>James also mentioned trial-by-trial variability: if cue during down phase, won’t expect reactivation in next 500 ms when all neurons are turned off.</li>
</ol>

<h3 id="behavioral-results-look-great">Behavioral Results Look great</h3>
<ol>
  <li>Spindles too rare to use to limit the sleep data   </li>
</ol>

<h3 id="additional-thoughts">Additional thoughts:</h3>
<ol>
  <li>We should leave open the option to look at ERPs for classification, although Ehren’s thoughts were that ERP winds up showing in theta band of power spectrum, plus see things in power spectrum that you don’t see in the ERPs.</li>
  <li>We can theoretically get more classification juice if we exclude forgotten items from the sleep analysis.  </li>
  <li>Waiting on James to get power spectrum for bands using Hilbert transform  </li>
  <li>We have the data to look at wake classification during wake reactivation i.e. can we classify when they’re learning the associated locations?   </li>
</ol>

<h3 id="todo-items">TODO ITEMS:</h3>
<p><strong>dot product:</strong><br />
<del>currently running sleep log reg mcduff using both faces and scenes, results will be in <code>sleep_xform_mcduff_scene_face_8subjs_dotprod.mat</code></del>  Results in post: sleep-eeg-boosting-results
<del>currently running boosting gentle log reg using both faces and scenes, results will be in <code>boosting_mcduff_best8_gentle_scene_face_dotprod.mat</code></del>Results in post: sleep-eeg-boosting-results, <strong>EXCEPT</strong> for some reason</p>

]]></content>
  </entry>
  
</feed>
