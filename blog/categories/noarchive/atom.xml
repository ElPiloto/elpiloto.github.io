<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: noarchive | Something Witty]]></title>
  <link href="http://ElPiloto.github.io/blog/categories/noarchive/atom.xml" rel="self"/>
  <link href="http://ElPiloto.github.io/"/>
  <updated>2014-07-28T17:46:53-04:00</updated>
  <id>http://ElPiloto.github.io/</id>
  <author>
    <name><![CDATA[Luis R. Piloto]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Sleep EEG: Group Meeting]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/07/28/sleep-eeg-group-meeting/"/>
    <updated>2014-07-28T17:10:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/07/28/sleep-eeg-group-meeting</id>
    <content type="html"><![CDATA[<h3 id="ken-succinctly-described-two-levels-of-complexity-in-our-sleep-analysis">Ken succinctly described two levels of complexity in our sleep analysis:</h3>
<ol>
  <li>How do we transform the sleep?  Recall this can be face or scene transformed, we can use correlation or the dot product, we can use data from different frequencies and different time bins in the wake data, etc.<br />
    <ul>
      <li>Perhaps we don’t need to sleep transform?</li>
      <li>Perhaps we need to exclude electrodes explicitly? We thought the McDuff importance maps method implemented this, but correlation doesn’t care about the magnitude so we need to re-evaluate this (either use dot product OR filter out electrodes by some other measure)  <strong>(TODO ITEM: dot product)</strong></li>
    </ul>
  </li>
  <li>When does reinstatement happen?  How should we score reinstatement?  If we do classification, do we just look at one time bin for the sleep?  Do we sum up classification across all time bins, etc.<br />
    <ul>
      <li>Maybe we want to use a simpler method than classification a la Staresina paper: sum correlation between template and sleep pattern and threshold to indicate replay event.   </li>
      <li>We could and should look at trial-by-trial plots of correlation across time for both the incorrect and correct pattern - eyeball the shit out of this.  <strong>(TODO ITEM)</strong></li>
    </ul>
  </li>
  <li>James also mentioned trial-by-trial variability: if cue during down phase, won’t expect reactivation in next 500 ms when all neurons are turned off.</li>
</ol>

<h3 id="behavioral-results-look-great">Behavioral Results Look great</h3>
<ol>
  <li>Spindles too rare to use to limit the sleep data   </li>
</ol>

<h3 id="additional-thoughts">Additional thoughts:</h3>
<ol>
  <li>We should leave open the option to look at ERPs for classification, although Ehren’s thoughts were that ERP winds up showing in theta band of power spectrum, plus see things in power spectrum that you don’t see in the ERPs.</li>
  <li>We can theoretically get more classification juice if we exclude forgotten items from the sleep analysis.  </li>
  <li>Waiting on James to get power spectrum for bands using Hilbert transform  </li>
  <li>We have the data to look at wake classification during wake reactivation i.e. can we classify when they’re learning the associated locations?   </li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG: Post Boosting Results]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/07/27/sleep-eeg-post-boosting-results/"/>
    <updated>2014-07-27T14:46:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/07/27/sleep-eeg-post-boosting-results</id>
    <content type="html"><![CDATA[<h3 id="our-to-do-list-consists-of-the-following">Our to-do list consists of the following:</h3>
<ol>
  <li>logistic regression on sleep data using data from all time bins of the sleep data and compare this to existing results from boosting  </li>
  <li>try boosting and logistic regression using <strong>untransformed</strong> sleep data (large volume of data)</li>
  <li>look at various other parameterizations of boosting classification result (just because it’s easy to do)</li>
</ol>

<h3 id="as-a-refresher-here-is-the-last-boosting-result">As a refresher, here is the last boosting result:</h3>

<p><img src="/images/research/ensemble_best<em>8_wake_subj</em>202ms_7hz_gentle.jpg" width="700" height="350"></p>

<p><strong>Figure 1:</strong> The y-axis shows classification accuracy.  Subjects and parameters are shown along the x-axis (g = gentleboost algorithm, f = face mcduff importance sleep transform).</p>

<h2 id="logistic-regression-sleep-importance-map-transformed-results-to-do-item-1">Logistic Regression Sleep Importance Map Transformed Results (To-Do Item #1)</h2>
<p><img src="/images/research/log_reg_best_8_subjects_mcduff_xform.jpg" width="700" height="350"></p>

<p><strong>Figure 2a:</strong> Displayed above is the classifier accuracy for all subjects(x-axis).  The classification accuracy was generated for various sleep importance map transformations (face, scene, or face minus scene), which is indicated along the y-axis.  The classifier features consisted of transforming the sleep data at each timebin and concatenating the sleep transformed data at each timebin into a single data matrix.  The entries along the x and y axis labelled “Mean” simply show the mean along the x and y axis respectively.</p>

<p><strong>Thoughts</strong>: These results lend further credence to the notion that it may be beneficial to sleep transform data according to both face and scene classifiers and feed both of those into a single classifier.  This idea that perhaps there is information in the scene pattern that isn’t in the face pattern (and vice versa) can be further scrutinized by looking at MDS plots of the untransformed data, the face transformed data, and scene transformed data - this may be worth doing depending on how much free time I have.  If I don’t end up looking at that, I’m not too upset since item #2 on the to-do list will also give us information about the role of the sleep transformation.  Additionally: note that the average classification accuracy, across both subjects and sleep transformation types, is pretty similar to that achieved with boosting, but none of the individual sleep transformations really give the same classification accuracy profile across subjects as the boosting results (this is easier to see in the plot below where I plot the rows of Figure 2 as separate horizontal bar graphs to match the format of the boosting results.  Why does it matter that accuracy across subjects in the logistic regression case doesn’t look like the accuracy achieved with boosting?  If our sleep transformation method yielded enough signal, then it should yield similar classification accuracy across subjects for the different classifier types.</p>

<p><img src="/images/research/log_reg_best_8_subjects_mcduff_xform_supplement.jpg" width="700" height="350">
<strong>Figure 2b:</strong> A different visualization of the logistic regression plots useful for direct visual comparison against the ensemble results (Figure 1).</p>

<h2 id="to-do-item-3-boosting-classification-results-using-different-sleep-transforms-best-8-subjects">To-Do Item #3: Boosting Classification Results Using Different Sleep Transforms, Best 8 Subjects</h2>
<p><img src="/images/research/ensemble_best<em>8_wake_subj</em>202ms_7hz_gentle_ALL_TRANSFORMS.jpg" width="700" height="350"></p>

<p><strong>Figure 3:</strong> This shows boosting classification accuracy using different sleep transforms.  The x-axis indicates which boosting algorithm (g for gentleboost) was used, the subject id (e.g. ‘15’ or ‘07’), and the wake pattern that was used for the sleep transformation (f = face, s = scene, M = face minus scene).</p>

<p><strong>Thoughts:</strong> This is a clear example where the classification accuracy results would look totally different based on which analysis parameters we use.  I suspect that if we get around to doing boosting classification using both the scene and face transformation, that will look entirely different, too.</p>

<h2 id="to-do-item-2-logistic-regression-classification-results-sleep---untransformed-best-8-subjects">To-Do Item #2: Logistic Regression Classification Results Sleep - UNTRANSFORMED, Best 8 Subjects</h2>
<p><strong>3/23 JOBS STILL RUNNING ON CLUSTER</strong>  This takes a relatively long time because the feature selection has to operate over ~100,000 features and that needs to run for each cross-validation fold for each subject.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG: Boosting Classification Results]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/07/15/sleep-eeg-boosting-classification-results/"/>
    <updated>2014-07-15T09:37:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/07/15/sleep-eeg-boosting-classification-results</id>
    <content type="html"><![CDATA[<h2 id="the-results-below-ensemble-classification-results-round-1-are-all-useless">The results below “Ensemble Classification Results, Round 1” are all useless</h2>

<p>I messed up the calculation of average pattern in wake classification (which is used in creating the mcduff importance map in the preprocessing below).</p>

<h3 id="ensemble-classification-results-round-6---cross-validation-subject-1">Ensemble Classification Results, Round 6 - Cross-validation subject 1</h3>

<p><img src="/images/research/ensemble_results_subj1_round6.jpg" width="700" height="350"></p>

<p><img src="/images/research/ensemble_results_subj1_round6pt2.jpg" width="700" height="350"></p>

<p><img src="/images/research/ensemble_results_subj1_round6pt3.jpg" width="700" height="350"></p>

<h3 id="ensemble-classification-results-round-3">Ensemble Classification Results, Round 3</h3>

<p>I used the following values to produce the plot below:
10-fold cross validation, GentleBoost, 200 Learners, Wake Time = 152ms, Freq = 7 Hz, Importance Map for Face</p>

<p><img src="/images/research/ensemble_results_subj1thru8_round3.jpg" width="700" height="350"></p>

<h3 id="ensemble-classification-results-round-1-incorrect">Ensemble Classification Results, Round 1 (INCORRECT)</h3>
<p><strong>these results are wrong - leaving here for historical purposes only</strong></p>

<p>Previously, I went through and tried various combinations of boosting algorithms (LogitBoost, GentleBoost, Adaboost.M1), number of learners, and number of folds for a single subject (subject 01) to try to determine what to run across all subjects to get the best tradeoff between accuracy and running time.  Below is a summary of those results.</p>

<p><img src="/images/research/subj1_ensemble_params_testing.png" width="700" height="300"></p>

<p>Ultimately, I chose the following: <br />
  <strong>GentleBoost:</strong> just as good accuracy as other algorithms <em>and</em> better running time than AdaBoost.M1 or LogitBoost for some parametrizations   <br />
  <strong>50-fold cross-validation:</strong> it seemed to me like accuracy would increase with the number of folds   <br />
  <strong>400 Learners:</strong> I was skeptical of AdaBoostM1’s results with 700 learners, that begin to overfit, so I aimed for something slightly higher than what I had tried (300 learners) that wouldn’t go too far in terms of overfitting.</p>

<p>Below are the results for running <strong>GentleBoost</strong> using the aforementioned parameters, transforming the sleep data with a face minus scene McDuff importance map using wake data from time-bin = 230ms and freq = 11 Hz. <br />
j  </p>

<p>This is below what we’d like to get and is lower than I would have expected for the first subject.  Possible reasons this could be the case:<br />
  - not enough data per fold<br />
  - I (accidentally )probed boosting results on subject 1 with different parameters than used for the current results: <br />
     - time bin = 152 ms<br />
     - freq = 7 Hz <br />
     - mcduff pattern = face   </p>

<p><img src="/images/research/ensemble_results_subj1thru8_round1.jpg" width="700" height="350"></p>

<h3 id="next-steps">Next Steps</h3>

<p>The easiest problem to check for is to see if we get better results with more data in each fold, so I’ve launched a batch of results that use 20 folds.  This should give new results relatively quickly and we can move from there.  If that doesn’t ameliorate the poor accuracy, I’ll move to using the wake parameters used to probe subject 1 to make sure that the results for subject 1 are consistent with the results I got during the probing phase.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Towers of Hanoi SFA and 8-ball SFA results: Robustness and Potential Neural Data to Model]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/07/06/towers-of-hanoi-sfa-and-8-ball-sfa-results-robustness-and-potential-neural-data-to-model/"/>
    <updated>2014-07-06T15:29:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/07/06/towers-of-hanoi-sfa-and-8-ball-sfa-results-robustness-and-potential-neural-data-to-model</id>
    <content type="html"><![CDATA[<h4 id="towers-of-hanoi-to-do">Towers of Hanoi To-Do:</h4>
<ol>
  <li><strong>Run SFA on random walk using multiple different random number seeds</strong> </li>
  <li>I am using the same starting location, this might affect the solutions found by SFA.  However, looking at the average input values (<code>SFA_STRUCTS.avg0</code>) shows that each feature has an average value of approximately 0.33, indicating that we do traverse all configurations equally frequently.</li>
  <li><code>sfa_tk</code> toolbox does not give features with unit variance when performing linear SFA. This might be a problem in the general case, but shouldn’t be a problem for towers of hanoi because the point of getting unit variance is so that the weights you find that minimize the derivative are directly comparable to each other.  But in our case, the derivatives should already be directly comparable because the input features all have exactly the same distribution for a random walk on towers of hanoi. I’m not entirely sure about this last bit, so I’ll have to verify this if Ari doesn’t have any useful input on the matter - easy way forward: look at MDP (python SFA implementation).</li>
</ol>

<h4 id="general-tips-for-sfatk-toolbox">General Tips for sfa_tk toolbox</h4>
<ol>
  <li><code>SFA_STRUCTS.SF</code> - contains slow feature functions along the rows</li>
  <li>Can get degenerate eigendecompositions, where an eigenvalue has greater algebraic multiplicity than geometric multiplicity (e.g. $ (2 - \lambda)^2(3-\lambda)^2 $ – alg. mult = 2, geometric = 1 for both eigenvalues).  In this case, extracted slow features are redundant (though they’ll have opposite signs).</li>
</ol>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SFA Nonmarkov: Modify network, maze size and type, visualize gates]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/06/24/sfa-nonmarkov-modify-network/"/>
    <updated>2014-06-24T13:38:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/06/24/sfa-nonmarkov-modify-network</id>
    <content type="html"><![CDATA[<h3 id="overview-of-simulations">Overview of simulations</h3>
<ol>
  <li>Look at the gate values along the maze trajectories (previously we only looked at the hidden state values). Recall that a long-short term memory network has three gates: read, write, and forget   </li>
  <li>We also need to look at our old MDS plots of the hidden layers and label the trajectory (previously we only showed whether a representation belonged to the center hallway or right or left outer maze locations)</li>
  <li>We want to see if we get any qualitative differences as we change the width of the maze.  </li>
  <li>We want to see if changing the maze type to be a ring will still yield the segmenting behavior we see from SFA  </li>
  <li>We want to see if we get any qualitative differences as we change the number of hidden units</li>
</ol>

<h3 id="different-num-hidden-units">Different Num Hidden Units</h3>

<h4 id="hidden-unit-mds-plots">Hidden Unit MDS Plots</h4>
<p><img src="/images/research/sfa_lstm/hidden_layer_width3_hidden5_loop.png" width="750" height="350"> 
<img src="/images/research/sfa_lstm/hidden_layer_width3_hidden3_loop.png" width="750" height="350"> </p>

<h5 id="observations">Observations:</h5>
<ol>
  <li>Much cleaner separation of states with 5 hidden units compared to 3 hidden units based on the loop to which they belong.</li>
  <li>Also get cleaner separation of states based on center versus non-center.</li>
  <li><strong>These results suggest that the hidden layer representation generated using 5 hidden units might be a better representation for looking at the benefits of SFA.</strong></li>
</ol>

<h4 id="hidden-unit-similarity-plots-according-to-location-on-maze">Hidden Unit Similarity Plots According to Location on Maze</h4>
<p><img src="/images/research/sfa_lstm/Linear_corr_states_hidden_representation_width3_hidden5.png" width="750" height="350"> 
<strong>Figure 2a) 5 Hidden Units</strong>
<img src="/images/research/sfa_lstm/Linear_corr_states_hidden_representation_width3_hidden3.png" width="750" height="350"> 
<strong>Figure 2b) 3 Hidden Units</strong>  </p>

<h5 id="observations-1">Observations:</h5>
<p>This is just another way of seeing how much cleaner the 5 hidden units representation is.</p>

<h4 id="linear-sfa-top-2-slowest-features">Linear SFA Top 2 Slowest Features</h4>
<p><img src="/images/research/sfa_lstm/sfa<em>1st</em>2<em>features_rm</em>1st_visit_width3<em>hidden5.png" width="750" height="350"> 
<strong>Figure 3a) 5 Hidden Units</strong>
<img src="/images/research/sfa_lstm/sfa</em>1st<em>2_features_rm</em>1st_visit_width3_hidden3.png" width="750" height="350"> 
<strong>Figure 3b) 3 Hidden Units</strong>   </p>

<h5 id="observations-2">Observations:</h5>
<ol>
  <li>Here we see exactly what we hoped for in the 5 hidden units case: the first slow-feature indicates which loop we’re on!</li>
  <li>The question is: how much of this is the neural network and how much of this is SFA? Well let’s look at the plots below of the raw hidden layer values across the maze.  If the neural network is doing all the heavy lifting, then we would see a single, slowly varying feature that encodes the loops direction but this doesn’t seem to be the case!</li>
</ol>

<p><img src="/images/research/sfa_lstm/hidden_layer_on_maze_raw_left_width3_hidden5.png" width="750" height="350"> 
<strong>Figure 4a) 5 Hidden Units</strong>
<img src="/images/research/sfa_lstm/hidden_layer_on_maze_raw_right_width3_hidden5.png" width="750" height="350"> 
<strong>Figure 4b) 5 Hidden Units</strong>   </p>

<p><strong>NOTE:</strong> These colors look exactly the same along the central corridor (which would be troubling), but their values are actually different if you look at the numbers.</p>

<h3 id="different-maze-size----width-7-5-hidden-units">Different Maze Size  - Width 7, 5 Hidden Units</h3>

<h4 id="hidden-unit-mds-plots-1">Hidden Unit MDS Plots</h4>
<p><img src="/images/research/sfa_lstm/hidden_layer_mds_width7_hidden5.png" width="750" height="350"> </p>

<h4 id="linear-sfa-top-2-slowest-features-1">Linear SFA Top 2 Slowest Features</h4>

<p><img src="/images/research/sfa_lstm/sfa_linear_1st2features_width7_hidden5.png" width="750" height="350"> </p>

<h3 id="different-maze-structure">Different Maze Structure</h3>

<h4 id="hidden-unit-mds-plots-2">Hidden Unit MDS Plots</h4>
<p><img src="/images/research/sfa_lstm/hidden_layer_mds_width7_hidden5.png" width="750" height="350"> </p>

<h4 id="linear-sfa-top-2-slowest-features-2">Linear SFA Top 2 Slowest Features</h4>
<p><img src="/images/research/sfa_lstm/sfa_1wayloop_width7_hidden5.png" width="750" height="350"> </p>

<h3 id="meeting-with-mattbot">Meeting with Mattbot:</h3>

<h4 id="observations-3">Observations:</h4>
<ol>
  <li>It does seem like the first slow feature encodes which outer loop we’re on.   </li>
  <li>Cooler than that, our second slow feature for the width 7, 5 hidden units case, encodes which step of the outer loop we’re on, regardless of which direction the loop is.  This is motherfucking abstraction! We’re going to try to connect this to to-do item #4.</li>
</ol>

<h4 id="to-do">To-Do:</h4>
<ol>
  <li>Why are the slow-features on such a small scale?    </li>
  <li>Why are the hidden layer representations (think MDS width3, hidden5) so similar at the fork point - Matt thinks I might be off by one because the point just after the fork diverges which is what you would expect from the fork point - since they make vastly different predictions.  </li>
  <li>Will PCA give us anything useful?    </li>
  <li>I should read a paper on rat neurophysiology, an older paper - they showed that in an 8-arm radial maze, certain PFC cells encoded specific parts of an arm, without being specific to the particular arm.  This is similar to our second slow feature in the </li>
  <li>Check that these results are invariant o the number of hidden units and size of the maze.</li>
</ol>

<h4 id="other-to-do">Other to-do:</h4>
<p>This isn’t related to the non-markov, but to the factored state towers of hanoi: I was tasked with performing slow feature analysis on a random walk on a factored state representation of the towers of hanoi task.  It seems like I’m getting a result that Ari wasn’t able to: The first slow feature seems to represent the location of the largest ring.
* I should check with Ari to make sure I did things the way he did, too <br />
* I should show Matt the MDS of the slow features <br />
* I should make sure all of theslow features load on all dimensions of the original input space.<br />
* We’re not sure what the second slowest feature should look like, but it seems like it might be the configuration of the second largest ring conditioned on the location of the largest ring.  That’s definitely what the scatter plot of the first two features looks like.</p>
]]></content>
  </entry>
  
</feed>
