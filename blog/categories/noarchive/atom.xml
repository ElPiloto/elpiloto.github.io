<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: noarchive | Something Witty]]></title>
  <link href="http://ElPiloto.github.io/blog/categories/noarchive/atom.xml" rel="self"/>
  <link href="http://ElPiloto.github.io/"/>
  <updated>2014-09-10T13:04:49-04:00</updated>
  <id>http://ElPiloto.github.io/</id>
  <author>
    <name><![CDATA[Luis R. Piloto]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Sleep Transform ALL the Subjects]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/09/10/sleep-transform-all-the-subjects/"/>
    <updated>2014-09-10T11:35:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/09/10/sleep-transform-all-the-subjects</id>
    <content type="html"><![CDATA[<h2 id="transformation-outline">Transformation Outline</h2>

<p>In this <a href="http://ElPiloto.github.io/blog/2014/09/04/sleep-eeg-finding-optimal-wake-pattern-for-sleep-transform/">post</a> we decided we’re going to try to generate a wake template for all of the different frequency bands at the <strong>300ms time bin</strong>.  Recall that those results were for L2-regularization of LOSO data ( with $\lambda = 10$ giving us the best results )  Below we outline how we plan to transform the sleep data.</p>

<pre>
# we're going to do a sleep transformation for each frequency
foreach frequency band F:
	foreach electrode E:
		if pval( mean_power_faces[E] - mean_power_scenes[E] ) &lt; 0.05
			informative_electrodes.add_electrode(E)
	
	data = append_subject_data(freq = F, timebin = 300ms, electrodes = informative_electrodes,
				classes_to_use = [faces scenes objects scrambled_faces scrambled_scenes])
	
	wake_logreg = train_logistic_regression_classifier(data, lambda = 10)

	wake_template = get_importance_map(wake_logreg, class = faces)
	
	# Option 1: old-school sleep transform analysis
	foreach subject S:
		foreach pattern P in sleep_data[S]:

			filtered_pattern = remove_noninformative_electrodes(P, informative_electrodes)

			foreach timebin T:
				foreach frequency F:

					transformed_sleep_data[S,P,T,F] = dot_product( wake_template, filtered_pattern[T,F] )
				
				# note below we are only grabbing data for THIS subject and THIS timebin - meaning a classifier
				# score for each timebin
				sleep_logreg[T] = train_logistic_regression_classifier(transformed_sleep_data[S,all_patterns,T, all_frequencies],
											lambda = ?, cross_validation = leave-one-trial-out)
	
	# Option 2: LOSO sleep transformed
	LOSO_sleep_logreg = train_logistic_regression_classifier(transformed_sleep_data, lambda = ?, cross_validation = per_subject)

</pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG Finding Optimal Wake Pattern For Sleep Transform]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/09/04/sleep-eeg-finding-optimal-wake-pattern-for-sleep-transform/"/>
    <updated>2014-09-04T10:06:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/09/04/sleep-eeg-finding-optimal-wake-pattern-for-sleep-transform</id>
    <content type="html"><![CDATA[<h2 id="wake-loso-log-reg-150ms---550ms-time-freq-sweep-on-hilbert-transformed-data">WAKE LOSO LOG REG 150ms - 550ms Time Freq. Sweep On Hilbert Transformed data</h2>

<p>Find those results below.  It seems that across the board, looking at diffAUC, we get that the 300ms timebin is the best.  However, one of the goals of this analysis was to see if there was any variability in when faces are classifiable compared to when scenes are classifiable.  Towards this end, we plot the AUC on a per class basis.  That is, we look at the AUC calculating the face classifier output for faces as positive instances, and the face classifier output for scenes as negative instances.  Similarly, we do this for scenes.  When we look at this across all subjects, we find that the 300ms timebin strong diffAUC is actually driven by strong faceAUC(~0.58) for that timebin and not at all by the sceneAUC(~0.43).  Thus, if we do decide to proceed with sleep transforming, we should only face transform at 300ms.   When we look at the faceAUC per subject, we find that 300ms at the 30Hz band is the good for all but two subjects (08 and 22).  This is encouraging!  Feel encouraged.  Did it work? Are you encouraged?  Fine.  Nevermind.  Whatever.  </p>

<p>I ran this analysis with a shortened time window, I’m re-running the analysis with the full time window to make sure that scenes aren’t doing well during the later periods.  Additionally, we have always been z-scoring across electrodes, I’m not doing that for this new analysis just to make sure our z-scoring decision (supported via our first pilot) are still valid.  I can look at the difference between the new results and the results below for the timebins they overlap to see the effect of z-scoring.   </p>

<p><strong>TLDR;</strong><br />
- diffAUC points to timebin 300ms as best time across all subjects with average diffAUC = 0.58 <br />
- looking into faceAUC vs. sceneAUC, it’s clear that the diffAUC is entirely driven by faceAUC (i.e. scenes aren’t above chance at 300ms), we should sleep transform sleep data using the 300ms timebin importance maps.   </p>

<p><strong>QUESTIONS TO ANSWER</strong> <br />
1. does one-sample, one-tailed t-test reveal better bins? <br />
2. are there better times? <br />
3. is there a difference between z-scoring and not z-scoring across channels? <br />
4. do we get better results for lower lambdas?   </p>

<p><strong>ANSWERS TO QUESTIONS</strong> <br />
<strong>ONE.</strong> We still get the best time bin to be 300 ms <a href="http://ElPiloto.github.io/images/research/LOSO_sleep_transform/LOSO_WAKE_LOGREG_SWEEP_PVAL_face_sceneAUC_lambda100.png">if you look at the p-values</a> instead of the <a href="http://ElPiloto.github.io/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__face_sceneAUC_lambda100.png">mean across subjects</a>  <br /></p>

<p><strong>TWO.</strong> When we do the same analysis looking at time bins 550ms - 1000ms, we don’t get anything better.  There’s a random blip around 950ms for scenes    <a href="http://ElPiloto.github.io/images/research/LOSO_sleep_transform/LOSO_WAKE_LOGREG_SWEEP_Q2_ZSCOREPVAL_face_sceneAUC_lambda50.png">here</a>, but it’s not any better than what we saw for the 150ms - 550ms case.     <br /></p>

<p><strong>THREE.</strong> YES IT DOES.  Compare these two plots WITHOUT z-scoring to their respective plots under question 4.  <br /></p>
<center>
<a href="http://ElPiloto.github.io/images/research/LOSO_sleep_transform/LOSO_WAKE_LOGREG_SWEEP_Q3_ZSCOREPVAL_face_sceneAUC_lambda50.png">$\lambda = 50$</a><br />
<a href="http://ElPiloto.github.io/images/research/LOSO_sleep_transform/LOSO_WAKE_LOGREG_SWEEP_Q3_ZSCOREPVAL_face_sceneAUC_lambda100.png">$\lambda = 100$</a><br />
</center>

<p><strong>FOUR.</strong>  We get better results with lambda = 10 for the p-val AUC plots, but not by too much if we just look at the mean AUC across subjects.  <strong>Let’s go with 10 for a wake transform</strong>    </p>
<center>
<a href="http://ElPiloto.github.io/images/research/LOSO_sleep_transform/LOSO_WAKE_LOGREG_SWEEP_PVAL_face_sceneAUC_lambda1.png">$ \lambda = 1$</a><br />   
<a href="http://ElPiloto.github.io/images/research/LOSO_sleep_transform/LOSO_WAKE_LOGREG_SWEEP_PVAL_face_sceneAUC_lambda10.png">$ \lambda = 10$</a><br />   
<a href="http://ElPiloto.github.io/images/research/LOSO_sleep_transform/LOSO_WAKE_LOGREG_SWEEP_PVAL_face_sceneAUC_lambda50.png">$ \lambda = 50$</a><br />   
<a href="http://ElPiloto.github.io/images/research/LOSO_sleep_transform/LOSO_WAKE_LOGREG_SWEEP_PVAL_face_sceneAUC_lambda100.png">$ \lambda = 100$</a><br />   
</center>

<p><strong>Ken Reply</strong></p>

<p>i like the idea of finding the best time/freq for faces and then using
that for a sleep transform. i agree that 300ms / 30hz looks like the
best candidate for the “face champion”.    </p>

<p>before you do the sleep transform, it would be great if you could run
the same analysis that you just ran, but for lower lambda vals (say 1,
10, 50). it looks like things were definitely getting worse as you
increased lambda in the analyses that you just ran, so maybe the best
lambda vals for this analysis are &lt; 100.    </p>

<p>if 300ms / 30hz still looks good with other lambda vals, then let’s
sleep transform using that.    </p>

<p>if we go ahead with 300ms / 30hz for the transformation, then i
recommend first filtering out uninformative electrodes, and then
re-running the classifier on the leftover electrodes and either 1)
using the importance map values for faces as the template, or 2) using
the mean pattern values (for faces) as the template. i also recommend
sticking with our original plan of grabbing the spatial pattern from
the BEST frequency band (e.g., 30hz) at the best time point (e.g.,
300ms) as opposed to grabbing ALL of the frequency-specific spatial
patterns from the best time point. we can use this pattern to
transform all of the frequences in the sleep data.     </p>

<p>also, after you do the sleep transform, i am interested in looking at
timecourses for reactivation as opposed to jumping right into crossval
accuracy.    </p>

<h2 id="lambda--100">Lambda = 100</h2>

<h4 id="diffauc-face-minus-scene">diffAUC (face minus scene)</h4>

<p><a href='/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__diffAUC_lambda100.png' target='_blank'><img src="/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__diffAUC_lambda100.png" width="700" height="350"></a>
<a href='/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__face_sceneAUC_lambda100.png' target='_blank'><img src="/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__face_sceneAUC_lambda100.png" width="700" height="350"></a>
<a href='/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__faceAUC_per_subj_lambda100.png' target='_blank'><img src="/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__faceAUC_per_subj_lambda100.png" width="700" height="350"></a>
<a href='/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__sceneAUC_per_subj_lambda100.png' target='_blank'><img src="/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__sceneAUC_per_subj_lambda100.png" width="700" height="350"></a></p>

<h2 id="lambda--500">Lambda = 500</h2>

<h4 id="diffauc-face-minus-scene-1">diffAUC (face minus scene)</h4>

<p><a href='/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__diffAUC_lambda500.png' target='_blank'><img src="/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__diffAUC_lambda500.png" width="700" height="350"></a>
<a href='/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__face_sceneAUC_lambda500.png' target='_blank'><img src="/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__face_sceneAUC_lambda500.png" width="700" height="350"></a>
<a href='/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__faceAUC_per_subj_lambda500.png' target='_blank'><img src="/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__faceAUC_per_subj_lambda500.png" width="700" height="350"></a>
<a href='/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__sceneAUC_per_subj_lambda500.png' target='_blank'><img src="/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__sceneAUC_per_subj_lambda500.png" width="700" height="350"></a></p>

<h2 id="lambda--1000">Lambda = 1000</h2>

<h4 id="diffauc-face-minus-scene-2">diffAUC (face minus scene)</h4>

<p><a href='/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__diffAUC_lambda1000.png' target='_blank'><img src="/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__diffAUC_lambda1000.png" width="700" height="350"></a>
<a href='/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__face_sceneAUC_lambda1000.png' target='_blank'><img src="/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__face_sceneAUC_lambda1000.png" width="700" height="350"></a>
<a href='/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__faceAUC_per_subj_lambda1000.png' target='_blank'><img src="/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__faceAUC_per_subj_lambda1000.png" width="700" height="350"></a>
<a href='/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__sceneAUC_per_subj_lambda1000.png' target='_blank'><img src="/images/research/sleep_eeg_hilbert/WAKE_LOSO_logreg__sceneAUC_per_subj_lambda1000.png" width="700" height="350"></a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG: Past TODO items and next directions]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/09/02/sleep-eeg-past-to-do-items-and-next-directions/"/>
    <updated>2014-09-02T14:15:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/09/02/sleep-eeg-past-to-do-items-and-next-directions</id>
    <content type="html"><![CDATA[<p>Before determining where we should go next now that we have a little bit of breathing room, I did a big review of the notes (and also some emails) I left myself from the last two months of updates in order to make sure we didn’t leave any ideas behind.  <strong>Bold asterisks indicate the items I intend to do next</strong> either because I think they’re important, are easy to do, or are a requisite for something important.   </p>

<h4 id="below-is-a-consolidated-list-of-to-do-items-random-things-weve-bookmarked-and-some-of-my-own-ideas">Below is a consolidated list of to-do items, random things we’ve bookmarked, and some of my own ideas</h4>

<ol>
  <li>
    <p>L1-regularization (instead of univariate feature selection + feature regularization via L2)<strong>******(important)</strong>    </p>
  </li>
  <li>
    <p>why was subject 9’s classification accuracy so low? (didn’t do poorly with other classification methods, so maybe it’s not so important to look at)   </p>
  </li>
  <li>coming up with a good wake template <strong>******(important)</strong> 
    <ul>
      <li>look at mcduff importance maps for wake LOSO logistic regression: does something pop out as the timebin/frequency to use for wake transformation?      </li>
    </ul>
  </li>
  <li>
    <p>different AUC metrics (diffAUC pooled over trials, meanAUC, etc)   </p>
  </li>
  <li>
    <p>boosting untransformed wake LOSO did poorly (about chance ) - was this a bug/not enough regularization or was it due to using the relatively quicker, unfamiliar totalboost algorithm? we can test this by running logitboost or gentleboost (which worked previously) now that we don’t have the pressure of the kenP deadline <strong>******(easy to do)</strong>   </p>
  </li>
  <li>
    <p>when looking at classification scores that are significantly better than chance, let’s also look at scores below chance   </p>
  </li>
  <li>
    <p>do we want to look out longer than 1 second after a tone is played? if first 500ms of sleep trial data happens to fall on down phase, we wouldn’t expect classification to work for that trial   </p>
  </li>
  <li>sleep transform: <br />
    <ul>
      <li>LOSO version   </li>
      <li>use face AND scene transformed data   </li>
      <li>exclude electrodes individually after finding good timebin/freq for wake   </li>
      <li>explicitly disentangle effects of dot-product vs. correlation on sleep transform   </li>
    </ul>
  </li>
  <li>
    <p>separate face vs. scene classifiers (face vs. objects/scrambled faces/scrambled scenes AND scene vs. objects/scrambled faces/ scrambled scenes): are we doing better at one of these than the other?  do they have different optimal classification times? (ties into previous thoughts about AUC: looking at face and scene classifier difference, diffAUC, vs. meanAUC)   <strong>******(important)</strong>   </p>
  </li>
  <li>
    <p>classification of data using ERPs   </p>
  </li>
  <li>visualize data: <br />
    <ul>
      <li>interface with EEGLAB topoplots   </li>
      <li>compare face/scene templates per subject to those generated on an individual-subject level   </li>
      <li>similarity of sleep data across time bins to wake pattern via simple correlation: are these different for the different classes?   <br />
        <ul>
          <li>we can look at the average similarity to a face or scene pattern against the average similarity for all other classes across sleep time  bins for each frequency<strong>******(important)</strong>     </li>
        </ul>
      </li>
      <li>autocorrelation of sleep data for various electrodes, could we use this as an indicator of down phases for the purpose of excluding time bins or trials?  is there another proxy for determining a down phase?      </li>
    </ul>
  </li>
  <li>
    <p>LOSO using some form of non-linear classification? previously we didn’t think this was a viable option given the ratio of training examples to features, but the combination of Hilbert-transformed data and LOSO makes me think this is now possible   </p>
  </li>
  <li>
    <p>exclude forgotten items from the sleep analysis, does this improve our sleep cross-validation?   </p>
  </li>
  <li>
    <p>test wake reactivation during learning of associated locations (this is different than the current wake data we use to train on which we take from the wake localizer)   </p>
  </li>
  <li>
    <p>generate simulated data to test idea that we could even find a wake pattern embedded randomly in a sleep data timebin-freq bin. also, would be good for general code validation (importance map transformations, mvpa code, etc)   </p>
  </li>
  <li>
    <p>look at classifier output for scrambled conditions and use to sleep transform: do these results look different than what we get when we transform via face and scene conditions?   </p>
  </li>
  <li>
    <p>“feature shuffle noise” wake classification analysis: do we get better generalization when we extend training data with patterns that are hybrids of two patterns from the same class? analogous to dropout techniques for neural networks    </p>
  </li>
  <li>
    <p>What if we transform the sleep data according to the timebin-freq pair that performs the best for each subject?  This is obviously less parsimonious than we’d like, but a signature of reactivation is a signature of reactivation.    </p>
  </li>
  <li>For results with <em>really</em> low wake classification accuracy, what do we get if we sleep transform by flipping the labels on the importance maps generated?    </li>
</ol>

<h4 id="below-is-a-working-list-identical-to-the-above-list-showing-whats-been-completed-so-far">Below is a working list identical to the above list showing what’s been completed so far.</h4>

<ol>
 <li> <del>L1-regularization (instead of univariate feature selection + feature regularization via L2)</del> Results: <br /> <br />

	NOTE: For lambda too high, the classifier simply assigns 0 to all the weights which makes sense given the number of training examples i.e. the $\lambda\ = 100$ case was just to test that this actually worked.  Totally intentional, I swear.   

<a href='/images/research/sleep_eeg_hilbert/LOSO_wake_L1.jpg' target='_blank'><img src="/images/research/sleep_eeg_hilbert/LOSO_wake_L1.jpg" width="700" height="350"></a>

<a href='/images/research/sleep_eeg_hilbert/LOSO_wake_L1_per_subject.jpg' target='_blank'><img src="/images/research/sleep_eeg_hilbert/LOSO_wake_L1_per_subject.jpg" width="700" height="350"></a>

   
 <li> <del>why was subject 9's classification accuracy so low? (didn't do poorly with other classification methods, so maybe it's not so important to look at)</del> Look at <a href="http://localhost:4000/images/research/sleep_eeg_hilbert/LOSO_wake_L1_per_subject.jpg">this L1-regularized plot</a>, clearly it was just a pathological case for feature selection plus L2-regularization.  Officially, NOT gunna worry about this.  <br />  <br />
    
<li> coming up with a good wake template __******(important)__<br /> 
    - look at mcduff importance maps for wake LOSO logistic regression: does something pop out as the timebin/frequency to use for wake transformation?     <br />  <br />
   
<li> different AUC metrics (diffAUC pooled over trials, meanAUC, etc)  <br />  <br />
   
<li> <del>boosting untransformed wake LOSO did poorly (about chance ) - was this a bug/not enough regularization or was it due to using the relatively quicker, unfamiliar totalboost algorithm? we can test this by running logitboost or gentleboost (which worked previously) now that we don't have the pressure of the kenP deadline </del> Results:    <br /><br /> 
   
	Additional: Running logitboost, also looking at methods for generating feature importance from boosting ensembles (using matlab toolbox so it's a bit of a blackbox to get information from) <br /> <br />
<a href='/images/research/sleep_eeg_hilbert/LOSO_wake_gentleboost_2_classes.jpg' target='_blank'><img src="/images/research/sleep_eeg_hilbert/LOSO_wake_gentleboost_2_classes.jpg" width="700" height="350"></a>


<li> when looking at classification scores that are significantly better than chance, let's also look at scores below chance    <br /> <br />
   
<li> do we want to look out longer than 1 second after a tone is played? if first 500ms of sleep trial data happens to fall on down phase, we wouldn't expect classification to work for that trial    <br /> <br /> <br />
   
<li> sleep transform:    <br />
    - LOSO version    <br />
    - use face AND scene transformed data    <br />
    - exclude electrodes individually after finding good timebin/freq for wake    <br />
    - explicitly disentangle effects of dot-product vs. correlation on sleep transform    <br /> <br />
   
<li> separate face vs. scene classifiers (face vs. objects/scrambled faces/scrambled scenes AND scene vs. objects/scrambled faces/ scrambled scenes): are we doing better at one of these than the other?  do they have different optimal classification times? (ties into previous thoughts about AUC: looking at face and scene classifier difference, diffAUC, vs. meanAUC)   br&gt;<br />
   
<li> classification of data using ERPs   <br /><br />
   
<li> visualize data:   <br />
    - interface with EEGLAB topoplots   <br />
    - compare face/scene templates per subject to those generated on an individual-subject level  <br /> 
    - similarity of sleep data across time bins to wake pattern via simple correlation: are these different for the different classes?    <br /> 
		- we can look at the average similarity to a face or scene pattern against the average similarity for all other classes across sleep time  bins for each frequency__******(important)__    <br /> 
    - autocorrelation of sleep data for various electrodes, could we use this as an indicator of down phases for the purpose of excluding time bins or trials?  is there another proxy for determining a down phase?      <br /><br />
      
    
<li> LOSO using some form of non-linear classification? previously we didn't think this was a viable option given the ratio of training examples to features, but the combination of Hilbert-transformed data and LOSO makes me think this is now possible   <br /><br />
   
<li> exclude forgotten items from the sleep analysis, does this improve our sleep cross-validation?   <br /><br />
   
<li> test wake reactivation during learning of associated locations (this is different than the current wake data we use to train on which we take from the wake localizer)   <br /><br />
   

<li> generate simulated data to test idea that we could even find a wake pattern embedded randomly in a sleep data timebin-freq bin. also, would be good for general code validation (importance map transformations, mvpa code, etc)  <br /><br /> 


<li> look at classifier output for scrambled conditions and use to sleep transform: do these results look different than what we get when we transform via face and scene conditions?  <br /><br /> 
   
<li> "feature shuffle noise" wake classification analysis: do we get better generalization when we extend training data with patterns that are hybrids of two patterns from the same class? analogous to dropout techniques for neural networks   <br /><br /> 

<li> What if we transform the sleep data according to the timebin-freq pair that performs the best for each subject?  This is obviously less parsimonious than we'd like, but a signature of reactivation is a signature of reactivation.    

<li> For results with _really_ low wake classification accuracy, what do we get if we sleep transform by flipping the labels on the importance maps generated?    

</li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></ol>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG: Leave-one-subject-out]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/08/26/sleep-eeg-leave-one-subject-out/"/>
    <updated>2014-08-26T14:45:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/08/26/sleep-eeg-leave-one-subject-out</id>
    <content type="html"><![CDATA[<h2 id="wake-leave-one-subject-out-loso-logistic-regression-use-all-freqs-all-times">Wake Leave-One-Subject-Out (LOSO) Logistic Regression, Use All freqs, All Times</h2>

<p><a href='/images/research/sleep_eeg_hilbert/log_reg_LOSO_all_features_class_AUC_vary_lambda.jpg' target='_blank'><img src="/images/research/sleep_eeg_hilbert/log_reg_LOSO_all_features_class_AUC_vary_lambda.jpg" width="700" height="350"></a></p>

<h4 id="this-should-contain-the-mcduff-importance-maps">This should contain the McDuff Importance Maps</h4>

<h2 id="wake-loso-logistic-regression-all-freqs-restrict-time-range-to-150ms---550ms">Wake LOSO Logistic Regression, All Freqs, Restrict Time Range to 150ms - 550ms</h2>

<p><a href='/images/research/sleep_eeg_hilbert/log_reg_LOSO_all_features_btwn_150ms550ms_lambda100_persubj.jpg' target='_blank'><img src="/images/research/sleep_eeg_hilbert/log_reg_LOSO_all_features_btwn_150ms550ms_lambda100_persubj.jpg" width="700" height="350"></a></p>

<h2 id="wake-loso-logistic-regression-time-freq-sweeps">Wake LOSO Logistic Regression, Time-Freq Sweeps</h2>

<h2 id="wake-loso-boosting-totalboost-all-freqs-all-times">Wake LOSO Boosting (TotalBoost), All Freqs, All Times</h2>

<h2 id="loso-sleep-logistic-regression-all-freqs-all-times">LOSO Sleep Logistic Regression, All Freqs, All Times</h2>

<h2 id="loso-sleep-boosting-all-freqs-all-times">LOSO Sleep Boosting, All Freqs, All Times</h2>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Relational RL]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/08/10/relational-rl/"/>
    <updated>2014-08-10T16:12:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/08/10/relational-rl</id>
    <content type="html"><![CDATA[<h2 id="motivation">Motivation</h2>

<p>Why do we even care about relational reinformcent learning?  </p>

<ol>
  <li>
    <p><em>Rule Learning by Seven-Month-Old Infants.</em> Marcus, Vijayan, Rao, &amp; Vishton, 1999:<br />
<strong>Methods:</strong>  <br />
7-month old infants were exposed to sentences of the form: ABB or ABA during a training phase (e.g. “ga ti ti” or “ga ti ga”), testing phase entailed consistent or inconsistent conditions where presented with entirely new words either in ABB or ABA format (e.g. “wo fe fe” or “wo fe wo”). <br />
<strong>Results:</strong> Infants looked longer at flashing light during inconsistent sentences in test period ( e.g. train “ABB”, test “ABA”) compared to consistent.<br />
<strong>Discussion:</strong> This indicates rule learning in infants, cannot be explained by statistical learning of transition probabilities because test set consisted entirely of new words (no estimate of transition probability).  Requires extraction of relationships: are these two entities the same?  </p>
  </li>
  <li>
    <p>Can generalize relationships over identities of objects in the world - variable abstraction.</p>
  </li>
  <li>
    <p>Introspection: our own capacities for abstraction and transfer suggest the importance of symbolic processing (Buchheit, 1999).</p>
  </li>
  <li>
    <p>Allows rich, intuitive specification of background knowledge: facilitating learning to new tasks (via bootstrapping from previously learned experiences): using a key on a lock is not dependent on the particular task at hand: if we learn through trial and error the abstract transition model $ T(isLocked(lock) = true, unlocks(lock,key) = true, useKeyOnLock(lock,key)) \rightarrow  isLocked(lock) = false $, then we can utilize that information in any domain that uses keys and locks.  While this allows for humans to specify background knowledge to an artificial agent by inserting that definite clause into the initial transition structure for a new RL problem, the more satisying use case is one where an agent builds up its own set of abstract relations and takes a library of relations into each new task.   </p>
  </li>
  <li>
    <p>Once rules have been learned, acting with them is a well-studied research problem (Pasula, Zettlemoyer, and Kaelbling, 2004) allowing for one-shot policy adjustment given novel domain configurations (e.g. a previously blocked path is now open).</p>
  </li>
  <li>
    <p>Propositional logic isn’t sufficient, need first-order a.k.a. relational representations: chess can be propositionalized as follows via propositions as follows: <br />
<code>NumberOfBlackPawnsIsNotFive</code> and <code>whiteKingOnSameLineAsBlackKing</code>, but requires fixed number of objects, proposition to be constructed for all possible relations between all objects.  Compare to the relational representation: <code>sameLine(blackKing,whiteKing)</code>.  The effect of this being that we cannot easily generalize over objects or similar situations (Van Otterlo 2005). See Bongard problem for another example where relational representation is much more natural than propositional.  <strong>Take home:</strong> the allure of first-order logic is not simply that there exist theorem provers and goal regression (which is also a property of propositional logic), but the fact that it explicitly provides a mechanism for relationships between entities.   </p>
  </li>
  <li>
    <p>Lookup tables or propositional representations aren’t able to represent structural aspects of states and actions in relational domains such as Block World.   </p>
  </li>
  <li>
    <p>A representation should enable representing and reasoning about <em>objects</em> (Kaelbling et al., 2001); one has to be be able to represent objects and relations in our language if an intensional stance is taken (Dennett, 1987).</p>
  </li>
</ol>

<h2 id="relational-learning">Relational Learning</h2>

<p>Learning in first-order logic can be broken into two categories: <br />
1. <strong>parameter learning:</strong> assuming that we’re given a set of definite clauses, let’s learn the parameter values (i.e. probabilities).<br />
2. __ structure learning:__ let’s learn both the definite clauses and the associated parameters (using refinement and generalization operators to maximize).   </p>

<h2 id="what-is-relational-reinforcement-learning">What is relational reinforcement learning?</h2>

<p>Relational reinforcement learning hinges on upgrading representations of the individual components of an MDP e.g. states, transition function, etc.  There are various relational representations with which the components of an MDP can be upgraded: first-order logic (instantiated as stochastic context-free grammars or probabilistic relational models), graph-based relational representations.  Here we’ll stick with a probabilistic first-order logic:</p>

<p>Formally, given a first-order probabilistic logic $ \Lambda $ (e.g. <em>inductive logic programming</em>), a hypothesis $ \Upsilon \in \Lambda $, predicates $p$, constants $c$, a special set of predicates $A$, then we can define the MDP as:<br />
  $ S: {s \in HB^{P \cap C} | s \models \Upsilon } $ <br />
  $ A: {a \in HB^{A \cap C} | a \models \Upsilon } $<br />
  $ T: S x A \longmapsto S $ <br />
  $ R: S x A \longmapsto R $ <br />
  $ HB = \text{Herbrand Base} $ <br />
  $ \Upsilon $ defines which states are possible in the current domain<br />
  $ \models $ means is an interpretation of   </p>

<h4 id="example-relational-mdp-definition-of-states-and-actions">Example relational MDP definition of states and actions</h4>
<p>$ P = {on/2, clear/1} $<br />
$ C = a,b,c,d,e, floor $<br />
$ A = {move/2} $</p>

<p>This defines the set of all possible states by filling in each possible constant into each possible slot of the predicates such that $ \mid S\mid = 501 $ legal states</p>

<p>Holy Moley Batman, that’s a lot of states!  If $\mid C\mid = k$ and the arity (number of arguments accepted by a predicate) is $ \alpha_p $, then the total number of possible interpretations is $ \prod_p 2^{k^{\alpha_p}} \implies $ <strong>intractable</strong>.  </p>

<p>Clearly, like many other state representations, we are in dire need of abstraction.  This is done by using <em>ungrounded</em> predicates for learning a transition structure or policy, that is, predicates where the arguments are populated entirely or partially by variables.  </p>

<h4 id="example-abstract-policy">Example abstract policy</h4>
<p>Assuming the current goal is to place object a on object b, here is an abstract policy utilizing the decision list construction where we follow the rules sequentially and apply the first rule that matches: <br />
  $ r<em>0: on(a,b) $ $\rightarrow \text{no move} $  <br />
  $ r</em>1: onTop(X,b) \rightarrow move(X,floor) $  <br />
  $ r<em>2: onTop(X,a) \rightarrow move(X, floor) $  <br />
  $ r</em>3: clear(a), clear(b) \rightarrow move(a,b) $   </p>

<p><strong>Note:</strong> It is important that any ungrounded terms (i.e. <em>variables</em>) that appear on the lefthand side of a rule, also appear on the righthand side.   </p>

<h2 id="model-free-relational-rl">Model-free relational RL:</h2>

<h4 id="q-value">Q-Value</h4>
<p>The general idea is to use relational regression in order to approximate q-value function.  This has yielded things like:<br />
    k-nearest neighbor: store examples, predict new q-values using distance-weighted average (requires defining distance over relational states) - also called relational instance-based regression (I think)   <br />
    kernel-based regression using Gaussian processes: pretty similar to above, requires defining kernel over graphs or using convolution kernel, two benefits: can control generalization via covariance function smoothing parameter and since it’s embedded in a statistical framework, can give confidence of q-values which can be used to guide exploration using any of a bunch of algorithms such as upper-confidence bound, etc. <br />
    learning decision list: use relational regression to learn <em>grounded</em> Q-value function, calculate optimal <em>grounded</em> policy, feed optimal ground policy to decision list learner to produce <em>abstract</em> policy, keep both grounded and abstract policy around, use abstract policy to guide learning via generalization <br />
    TG relational regression:  incrementally build first-order trees; relational MDP specification defines which predicates are posssible, TG algorithm incrementally adds them to a regression tree as needed  </p>

<h4 id="approximate-policy-iteration">Approximate Policy Iteration</h4>

<h4 id="relational-naive-bayes">Relational Naive Bayes</h4>

<h4 id="q-larc">Q-LARC:</h4>
<p>feature construction based on predictive features   </p>

<h2 id="model-based-relational-rl">Model-based relational RL:</h2>

<p>Idea: Translate Bellman Backup Operator into relational domain.  Requires finding “preconditions” i.e. what states could lead to this state (this is called <em>regression), then a _combination</em> operator, and a <em>maximization</em> operator to define relational Bellman operator.   Defines new abstract policy from old abstract policy.  This is called <strong>decision-theoretic regression</strong>.  See explanation-based reinforcement learning.</p>

<h2 id="applications-of-relational-reinforcement-learning">Applications of relational reinforcement learning</h2>

<h4 id="learning-grounded-relational-symbols-from-continuous-data-for-abstract-reasoning">Learning Grounded Relational Symbols from Continuous Data for Abstract Reasoning</h4>

<h4 id="learning-probabilistic-relational-planning-rules">Learning Probabilistic Relational Planning Rules</h4>

<p><strong>learnRules:</strong> performs a search through a rule set using ILP operators (i.e. refinement and generalization operators)<br />
<strong>induceOutcomes:</strong>  finds a best set of outcomes given a context and an action <br />
<strong>learnParameters:</strong> learns a probability distribution over a set of outcomes  </p>

]]></content>
  </entry>
  
</feed>
