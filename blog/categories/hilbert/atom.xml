<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: hilbert | Something Witty]]></title>
  <link href="http://ElPiloto.github.io/blog/categories/hilbert/atom.xml" rel="self"/>
  <link href="http://ElPiloto.github.io/"/>
  <updated>2014-08-26T16:01:47-04:00</updated>
  <id>http://ElPiloto.github.io/</id>
  <author>
    <name><![CDATA[Luis R. Piloto]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Sleep EEG: Leave-one-subject-out]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/08/26/sleep-eeg-leave-one-subject-out/"/>
    <updated>2014-08-26T14:45:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/08/26/sleep-eeg-leave-one-subject-out</id>
    <content type="html"><![CDATA[<h2 id="wake-leave-one-subject-out-loso-logistic-regression-use-all-freqs-all-times">Wake Leave-One-Subject-Out (LOSO) Logistic Regression, Use All freqs, All Times</h2>

<p><a href='/images/research/sleep_eeg_hilbert/log_reg_LOSO_all_features_class_AUC_vary_lambda.jpg' target='_blank'><img src="/images/research/sleep_eeg_hilbert/log_reg_LOSO_all_features_class_AUC_vary_lambda.jpg" width="700" height="350"></a></p>

<h4 id="this-should-contain-the-mcduff-importance-maps">This should contain the McDuff Importance Maps</h4>

<h2 id="wake-loso-logistic-regression-all-freqs-restrict-time-range-to-150ms---550ms">Wake LOSO Logistic Regression, All Freqs, Restrict Time Range to 150ms - 550ms</h2>

<p><a href='/images/research/sleep_eeg_hilbert/log_reg_LOSO_all_features_btwn_150ms550ms_lambda100_persubj.jpg' target='_blank'><img src="/images/research/sleep_eeg_hilbert/log_reg_LOSO_all_features_btwn_150ms550ms_lambda100_persubj.jpg" width="700" height="350"></a></p>

<h2 id="wake-loso-logistic-regression-time-freq-sweeps">Wake LOSO Logistic Regression, Time-Freq Sweeps</h2>

<h2 id="wake-loso-boosting-totalboost-all-freqs-all-times">Wake LOSO Boosting (TotalBoost), All Freqs, All Times</h2>

<h2 id="loso-sleep-logistic-regression-all-freqs-all-times">LOSO Sleep Logistic Regression, All Freqs, All Times</h2>

<h2 id="loso-sleep-boosting-all-freqs-all-times">LOSO Sleep Boosting, All Freqs, All Times</h2>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sleep EEG: Hilbert Transformed Data]]></title>
    <link href="http://ElPiloto.github.io/blog/2014/08/06/sleep-eeg-hilbert-transformed-data/"/>
    <updated>2014-08-06T10:36:00-04:00</updated>
    <id>http://ElPiloto.github.io/blog/2014/08/06/sleep-eeg-hilbert-transformed-data</id>
    <content type="html"><![CDATA[<h2 id="wake-hilbert-transformed-classification">Wake Hilbert Transformed Classification</h2>

<p><a href='/images/research/sleep_eeg_hilbert/wake_hilb_mean_across_subjects.png' target='_blank'><img src="/images/research/sleep_eeg_hilbert/wake_hilb_mean_across_subjects.png" width="700" height="350"></a></p>

<p><strong>Figure 1:</strong> Average AUC on Hilbert-transformed wake data across all subjects for each time bin and frequency band combination.</p>

<p><strong>Thoughts:</strong> We see “hotspots” where we were hoping to see them: at the theta frequency around 200-230ms. Moreover, we see that the 200-230ms theta bin forms a local peak: as you move earlier or later in time, classification accuracy decreases.  Importantly, the accuracy using these bands (as opposed to individual frequencies like we had before) yields comparable average wake classification across subjects if we look at this <a href="http://ElPiloto.github.io/images/research/sleep_eeg_9_subjects_06_23_2014/wake_auc_avgd.png">old plot</a>.  If this result holds for all subjects, then we would feel pretty good about using the 200ms (or 230ms) theta time-freq bin to transform the sleep data.  Therefore, let’s look at the individual subjects to see if these results are driven by some subjects or if they seem to be pretty evenly distributed (spoiler: doesn’t seem to be a good time-freq bin, or even classifiable, for all subjects, but it does pretty well)</p>

<p><a href='/images/research/sleep_eeg_hilbert/wake_hilb_all_subjects_all_freqs_all_times.png' target='_blank'><img src="/images/research/sleep_eeg_hilbert/wake_hilb_all_subjects_all_freqs_all_times.png" width="700" height="350"></a></p>

<p><strong>Figure 2:</strong> AUC heatmaps of classification AUC per subject.  Subject IDs are along the top of each subplot, timebins run along the y-axis and frequencies along the x-axis (only explicitly marked for the last subject, but the axes are exactly the same as Figure 1).</p>

<p><strong>Thoughts:</strong> These plots, though informative, suck for the purpose of evaluating how good the 200-250ms, theta bins do across all subjects.  Let’s zoom in on the areas of interest below.  One additional point, is that we’re getting <strong>much</strong> better max AUC across timebins than previously before (using individual frequencies) suggesting that the Hilbert transform is a better preprocessing technique for classification.</p>

<p><a href='/images/research/sleep_eeg_hilbert/wake_across_subjs_times7<strong>8</strong>9_freq2.png' target='_blank'><img src="/images/research/sleep_eeg_hilbert/wake_across_subjs_times7<strong>8</strong>9_freq2.png" width="700" height="350"></a></p>

<p><strong>Figure 3:</strong>  Above we show the AUC for each subject for 3 different times (200, 225, and 250ms respectively) for the 4 Hz (theta) frequency band.</p>

<p><strong>Thoughts:</strong> This isn’t the greatest, especially considering some subjects have time-freq bins with much higher AUC.  “Irregardless,” these results are sufficiently positive to justify transforming sleep data.  Everything that we said about the non-band wake analyses holds here (e.g. we can look at other bins that look good for subjects, we can take each subject’s best bin to transform the data, etc).</p>

<h2 id="sleep-hilbert-untransformed-classification---sweep-across-time-and-frequency-combinations">Sleep Hilbert Untransformed Classification - Sweep Across Time And Frequency Combinations</h2>

<p>Below we look at how discriminable the sleep data is at each particular timebin and frequency combination of the sleep data using logistic regression.  Additionally, we test the regularization parameter for two separate values $ \lambda = 1, 50 $</p>

<p><a href='/images/research/sleep_eeg_hilbert/sleep_hilb_mean_across_subjects_lambda1.png' target='_blank'><img src="/images/research/sleep_eeg_hilbert/sleep_hilb_mean_across_subjects_lambda1.png" width="700" height="350"></a></p>

<p><strong>Figure 4:</strong> Above we show the across-subjects average AUC for performing logistic regression on the sleep data at each time and frequency band combination with regularization parameter $ \lambda = 1 $.</p>

<p><a href='/images/research/sleep_eeg_hilbert/sleep_hilb_lambda50_mean_across_subjects.png' target='_blank'><img src="/images/research/sleep_eeg_hilbert/sleep_hilb_lambda50_mean_across_subjects.png" width="700" height="350"></a></p>

<p><strong>Figure 5:</strong> Same as Figure 4, except this time we set the regularization parameter $ \lambda = 50 $.</p>

<p><strong>Thoughts:</strong> The band around 175ms, 16 Hz looks promising - we had mentioned that we’d be pretty happy if we could get something like 7-8 points above chance average classification accuracy across subjects.  This is much higher than the average classification accuracy ($ mean AUC = 0.52 $) we were getting for our “best 8” untransformed, logistic regression average AUC using all times and frequencies (recall that those results were not using frequency bands like we are now, but instead look at the power spectrum at individual frequencies).  Obviously, we can’t directly compare the individual time and frequency band results to using all time and frequency features, but it’s the closest benchmark we have.  Those old results can be found <a href="http://ElPiloto.github.io/blog/2014/07/27/sleep-eeg-post-boosting-results/"> here </a> under the section “To-Do Item #2”.</p>

<p>These results are encouraging for two reasons.  First, the best time and frequency combination falls within a plausible time range.  We could talk to James to see if he has any a priori evidence to support getting results in the BETA frequency range, but at least the time isn’t too early.  Second, the AUC is pretty good on the individual subject level as we see below: most subjects are either at or above chance.</p>

<p><a href='/images/research/sleep_eeg_hilbert/best_time_freq_untransformed_sleep_lambda1.jpg' target='_blank'><img src="/images/research/sleep_eeg_hilbert/best_time_freq_untransformed_sleep_lambda1.jpg" width="700" height="350"></a></p>

<p><strong>Figure 6:</strong> Above we show the AUC for each subjects generated by performing logistic regression on the sleep data at the best time and frequency band combination with regularization parameter $ \lambda = 1 $.</p>

<p><a href='/images/research/sleep_eeg_hilbert/best_time_freq_untransformed_sleep_lambda50.jpg' target='_blank'><img src="/images/research/sleep_eeg_hilbert/best_time_freq_untransformed_sleep_lambda50.jpg" width="700" height="350"></a></p>

<p><strong>Figure 7:</strong> Same as Figure 6, except this time we set the regularization parameter $ \lambda = 50 $.</p>

<p><strong>Thoughts:</strong> Okay, so we have <strong>a</strong> time and frequency combination that gives us good classification accuracy, but this could just arise from multiple comparisons.  Sure, we can take comfort that the classification accuracy seems to peak at 175ms and degrades smoothly as we move away, but it’s also the case that the EEG power spectrum is pretty similar at those times.  <strong>SO</strong>, what would it take for me to believe these results?  Running classification on <strong>ALL</strong> of the <em>untransformed</em> sleep data features with heavy regularization and yielding good classification accuracy with weights that consistently preferred that time and frequency combination.  This is next on my to-do list, it shouldn’t take too long to get up and running.  I’d also like to add a reminder to myself that it may be worth subsampling the time dimension.</p>

<h2 id="sleep-hilbert-untransformed-log-reg-classification---use-all-time-and-frequencies">Sleep Hilbert Untransformed Log Reg Classification - Use All Time and Frequencies</h2>

<p><a href='/images/research/sleep_eeg_hilbert/untransformed_logreg_all_features.png' target='_blank'><img src="/images/research/sleep_eeg_hilbert/untransformed_logreg_all_features.png" width="700" height="350"></a></p>

<p><strong>Figure 8:</strong> Above we show the AUC per subject (along the y-axis) for various logistic regression regularization penalty values (along the x-axis) for classification using ALL of the features for the untransformed (i.e. not transformed according to wake pattern) sleep data using frequency bands (using Hilbert transform).  The <em>NaN</em> value is because that particular job failed and I didn’t think it was that important to justify putting off these results.</p>

<p><a href='/images/research/sleep_eeg_hilbert/untransformed_logreg_all_features_avg_across_subjects.png' target='_blank'><img src="/images/research/sleep_eeg_hilbert/untransformed_logreg_all_features_avg_across_subjects.png" width="700" height="350"></a></p>

<p><strong>Figure 9:</strong> Same as Figure 8, except this time we average across subjects for each regularization setting.</p>

<p><strong>Thoughts:</strong> These results definitely aren’t what we were hoping for, and subject09’s  results are worrisome.  We’ve seen pretty consistently classifiable results out of subject 15 which is a good sanity check that there isn’t something wrong with the code or with the range of regularization values we’ve tried.  The best case scenario is that subject 15’s data is really easily classifiable, whereas the other subjects require more careful feature selection.  Towards this end, I’m going to try three things.  First, I’m running this same exact analysis except I’m including feature selection as a preprocessing step (note: I’ll have to run it for just a single regularization value since the feature selection process is pretty time-consuming).  Second, I’m going to look at the importance maps generated by this current analysis to see which features are being loaded upon.  Lastly, I’m going to try boosting using all the features.  This would also be a great place to have simulated data, but that’ll have to take a back seat.</p>

<h2 id="sleep-hilbert-untransformed-gentleboost-ensemble-classification---use-all-time-and-frequencies">Sleep Hilbert Untransformed GentleBoost Ensemble Classification - Use All Time And Frequencies</h2>

<p><a href='/images/research/sleep_eeg_hilbert/untransformed_gentleboost_all_features_ACC.png' target='_blank'><img src="/images/research/sleep_eeg_hilbert/untransformed_gentleboost_all_features_ACC.png" width="700" height="350"></a></p>

<p><strong>Figure 10:</strong> Above we show the AUC per subject (along the y-axis) for various logistic regression regularization penalty values (along the x-axis) for classification using ALL of the features for the untransformed (i.e. not transformed according to wake pattern) sleep data using frequency bands (using Hilbert transform).  The <em>NaN</em> value is because that particular job failed and I didn’t think it was that important to justify putting off these results.</p>

<p><a href='/images/research/sleep_eeg_hilbert/untransformed_gentleboost_all_features_avg_across_subjects_ACC.png' target='_blank'><img src="/images/research/sleep_eeg_hilbert/untransformed_gentleboost_all_features_avg_across_subjects_ACC.png" width="700" height="350"></a></p>

<p><strong>Figure 11:</strong> Same as Figure 10, except this time we average across subjects for each value of the <code>num_learners</code> parameter.</p>

<h2 id="the-continuing-story-of-delbungalow-billdel-subject-09">The Continuing Story of <del>Bungalow Bill</del> Subject 09</h2>

<p><strong>Sleep Statistics:</strong> 35 faces, 35 scenes.  Cross-validation was performed using leave-one-out-cross-validation (a.k.a. 35-fold).</p>

<p>Achieves reasonable classification accuracy using boosting.  Additionally, classification accuracy (as opposed to AUC) isn’t SO horrendous for logistic regression (0.31 for feature selection using $\lambda = 100$ and setting <code>stat_thresh</code> = 0.1)  When we look at the AUC per cross-validation iteration, we find that the AUC is always 0 or 1 essentially (see figure 11 below).  It seems like we’re severly overfitting.  This is supported by the fact that the boosting method actually does well for subject 09 ( boosting much less susceptible to overfitting).  However, if this really was the case, you’d expect that doing feature selection would help this subject’s results.  Currently, that is not the case, so I’m gunna try even stricter feature selection to see if that’s what’s really at the bottom of this.</p>

<p><a href='/images/research/sleep_eeg_hilbert/auc_per_iteration_feature_select_subj9.jpg' target='_blank'><img src="/images/research/sleep_eeg_hilbert/auc_per_iteration_feature_select_subj9.jpg" width="700" height="350"></a></p>

<p><strong>Figure 12:</strong> Above we show the AUC per cross-validation iteration for subject 09 using logistic regression on all of the untransformed sleep features</p>

<p><a href='/images/research/sleep_eeg_hilbert/per_fold_correctminusincorrect_class_outputs.jpg' target='_blank'><img src="/images/research/sleep_eeg_hilbert/per_fold_correctminusincorrect_class_outputs.jpg" width="700" height="350"></a></p>

<p><strong>Figure 13:</strong> Above we show the difference between the classifier output for the correct class minus the classifier output for the incorrect class for each test item over all cross-validation iterations for various parametrizations of wake logistic regression classification for subject 9.  Note that for p = 0.01, we get something like 30-50 features selected.</p>

<p><strong>Random thought:</strong> What would we get if we tried to use this for sleep transforming data by flipping the classes?</p>

<p><strong>Extra thoughts:</strong> I was feeling uneasy about the fact that AUC and classification accuracy give completely different answers for subject 09.  However, I went through and verified by hand that the code for calculating the AUC is in fact correct (given the type of AUC metric we want to use).  Currently, we’re looking at the distribution of (face - scene) classifier outputs to calculate the AUC, but we get vastly different results for this subject (and I assume other subjects as well), if we calculate a separate AUC for the face classifier output, a separate AUC for the scene classifier output, and then take the average (possibly weighted by number of examples from each class) of these two AUCs to yield a single number (let’s call this method <em>meanAUC</em>).  Although we’ve been evaluating with respect to the former definition of AUC ( let’s dub this <em>diffAUC</em>), the best metric to use actually depends on how we intend to relate the sleep classifier output to the behavioral data.  One method for relating classifier output uses a binary measure to predict subsequent performance: each reactivation event is labelled as a 0 or a 1 indicating whether or not the classifier predicted the correct class.  Subsequent memory for an item should increase if the classifier correctly classified reactivation events.  In this case, the appropriate AUC method to use is the <em>diffAUC</em> because we’re interested in the difference between the classifier outputs for each class.    An alternative method for relating classifier output to behavioral data would be to only look at, for each reactivation event, the classifier output from the classifier corresponding to the item’s category (i.e. if we’re looking at a face item, we only look at the face classifier output; if we’re looking at a scene item, we take the scene classifier output).  We would predict that items with a high classifier output, despite their category, would yield better subsequent memory.  This method lends itself to evaluation via <em>meanAUC</em>.   <strong>They are fundamentally different metrics</strong> and so I shouldn’t take this as a sign that my code base is messed up.  To further illustrate how different these metrics can be, below I provide a sample of the different metrics for a particular subject:   </p>
<pre>
                 diffAUC: .1714
                 meanAUC: 0.5286
              % correct: 0.2857
</pre>

<p>Here’s a case that’s also a bit counterintuitive, but checks out.  I’m leaving this here just as a note for future Luis.</p>
<pre>
true classes = {face, scene}
---------------------------
face = 0.435, 0.4635
scene = 0.5650, 0.5365
diff = -0.13, -0.07

Max Class Output Prediction: scene (incorrect), scene (correct)
% correct = 0.5
diffAUC = 0
</pre>

<h2 id="ken-meeting-points">Ken Meeting Points</h2>

<h4 id="auc">AUC</h4>
<p>The space of <em>diffAUC</em>, <em>meanAUC</em>, and individual AUCs (e.g. <em>faceAUC</em> or <em>sceneAUC</em>) are all justifiable classification metrics.  Let’s not open this can of worms just yet, <em>however</em>, let’s <strong>definitely</strong> switch to pooling the classification outputs from all cross-validation folds and <em>then</em> calculating AUC, instead of calculating AUC for each fold, and averaging across folds.  That being said, subject 09 per-fold diffAUC was 0.1714 (see above), and their pooled diffAUC = 0.18 - so it doesn’t seem to have too much of an impact as I suspected.</p>

<h4 id="feature-selection">Feature Selection</h4>
<p>Ken thinks it’s unconventional to use feature selection and L2 regularization.  Let’s try L1 regularization instead, which has one less parameter and performs multivariate feature selection.</p>

<h4 id="subject-9-crappiness">Subject 9 Crappiness</h4>
<p>I haven’t justified why overfitting should lead to systematically incorrect predictions, it should lead to chance performance.   It may be the case that feature selection and L2 regularization is just susceptible to pathological results given that our classifier does okay when we try boosting.  If we try L1-regularization and that works okay, then we’ll know the problem is feature selection + L2 regularization.</p>

<h4 id="next-steps">Next Steps:</h4>
<p>Let’s try our leave-one-subject-out analysis for individual time-frequency pairs.</p>
]]></content>
  </entry>
  
</feed>
